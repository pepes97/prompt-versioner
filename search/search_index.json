{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Prompt Versioner","text":"<p>Prompt Versioner is an enterprise-grade prompt management system that provides version control, performance tracking, A/B testing, and collaboration tools for AI applications.</p>"},{"location":"#why-prompt-versioner","title":"\ud83c\udfaf Why Prompt Versioner?","text":"<p>In the rapidly evolving world of AI and Large Language Models, managing prompt versions, tracking performance, and ensuring consistent quality is crucial for production applications. Prompt Versioner provides enterprise-grade prompt management with:</p> <ul> <li>\ud83d\udd04 Version Control: Complete versioning for prompts with full history</li> <li>\ud83d\udcca Performance Tracking: Comprehensive metrics and regression detection</li> <li>\ud83e\uddea A/B Testing: Built-in statistical framework for prompt optimization</li> <li>\u26a1 Real-time Monitoring: Automated alerts and performance dashboards</li> <li>\ud83d\udc65 Team Collaboration: Annotations, reviews, and shared insights</li> <li>\ud83c\udfa8 Modern UI: Beautiful web dashboard with dark/light themes</li> </ul>"},{"location":"#key-features","title":"\u2728 Key Features","text":""},{"location":"#core-functionality","title":"\ud83d\udd27 Core Functionality","text":"<ul> <li>Versioning: Automatic version management with MAJOR/MINOR/PATCH bumps</li> <li>Metrics Tracking: Comprehensive LLM call metrics (tokens, latency, quality, cost)</li> <li>Export/Import: Backup and share prompts with full history</li> <li>Git Integration: Optional Git integration for version control</li> </ul>"},{"location":"#advanced-testing-monitoring","title":"\ud83e\uddea Advanced Testing &amp; Monitoring","text":"<ul> <li>A/B Testing: Built-in statistical framework for comparing prompt versions</li> <li>Performance Monitoring: Automated regression detection and alerting</li> <li>Real-time Analytics: Live metrics and performance dashboards</li> <li>Custom Alerts: Configure thresholds for cost, latency, and quality metrics</li> </ul>"},{"location":"#collaboration-management","title":"\ud83d\udc65 Collaboration &amp; Management","text":"<ul> <li>Team Annotations: Collaborative notes and feedback system</li> <li>Version Comparison: Detailed diff views with change tracking</li> <li>Search &amp; Filtering: Find prompts by metadata, performance, and tags</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install prompt-versioner\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\n# Initialize the versioner\nversioner = PromptVersioner(project_name=\"my-app\")\n\n# Save a prompt version\nversion_id = versioner.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are a helpful AI assistant.\",\n    user_prompt=\"Answer this question: {question}\",\n    bump_type=VersionBump.MAJOR,  # Creates version 1.0.0\n    metadata={\"type\": \"general_assistant\"}\n)\n\n# Create a new version\nversioner.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are an expert AI assistant with deep knowledge.\",\n    user_prompt=\"Please provide a detailed answer to: {question}\",\n    bump_type=VersionBump.MINOR,  # Creates version 1.1.0\n    metadata={\"improvement\": \"enhanced expertise\"}\n)\n\n# Track metrics\nversioner.log_metrics(\n    name=\"assistant\",\n    version=\"1.1.0\",\n    model_name=\"gpt-4o\",\n    input_tokens=15,\n    output_tokens=25,\n    latency_ms=1200,\n    cost_eur=0.001,\n    quality_score=0.95\n)\n</code></pre>"},{"location":"#web-dashboard","title":"Web Dashboard","text":"<p>Launch the interactive web dashboard:</p> <pre><code>python examples/run_dashboard.py\n</code></pre>"},{"location":"#documentation-structure","title":"\ud83d\udcd6 Documentation Structure","text":"<ul> <li>Getting Started: Installation, setup, and configuration</li> <li>User Guide: Comprehensive guides for all features</li> <li>Examples: Practical examples and use cases</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"#quick-links","title":"\ud83d\udd17 Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>A/B Testing Guide</li> <li>Web Dashboard</li> <li>API Reference</li> </ul>"},{"location":"#community-support","title":"\ud83e\udd1d Community &amp; Support","text":"<ul> <li>GitHub Repository: pepes97/prompt-versioner</li> <li>Issues &amp; Bug Reports: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul> <p>Built with \u2764\ufe0f for the AI community</p>"},{"location":"api-reference/cli/commands/","title":"CLI Commands","text":"<p>Command-line interface commands for Prompt Versioner operations.</p>"},{"location":"api-reference/cli/commands/#overview","title":"Overview","text":"<p>The CLI commands module provides a comprehensive command-line interface for managing prompts, versions, and project configuration. All commands are accessible through the <code>prompt-versioner</code> CLI tool.</p>"},{"location":"api-reference/cli/commands/#available-commands","title":"Available Commands","text":""},{"location":"api-reference/cli/commands/#installation-and-usage","title":"Installation and Usage","text":"<pre><code># Install prompt-versioner\npip install git+https://github.com/pepes97/prompt-versioner.git\n\n# Access help\nprompt-versioner --help\n\n# Command-specific help\nprompt-versioner &lt;command&gt; --help\n</code></pre>"},{"location":"api-reference/cli/commands/#command-categories","title":"Command Categories","text":""},{"location":"api-reference/cli/commands/#project-management","title":"Project Management","text":"<ul> <li><code>init</code> - Initialize a new prompt versioning project</li> </ul>"},{"location":"api-reference/cli/commands/#prompt-operations","title":"Prompt Operations","text":"<ul> <li><code>create</code> - Create a new prompt version</li> <li><code>list</code> - List prompts and versions</li> <li><code>show</code> - Display prompt details</li> </ul>"},{"location":"api-reference/cli/commands/#version-management","title":"Version Management","text":"<ul> <li><code>diff</code> - Compare prompt versions</li> <li><code>rollback</code> - Rollback to previous version</li> </ul>"},{"location":"api-reference/cli/commands/#analysis-and-monitoring","title":"Analysis and Monitoring","text":"<ul> <li><code>metrics</code> - View performance metrics</li> <li><code>dashboard</code> - Launch web dashboard</li> <li><code>export</code> - Export prompts and data</li> </ul>"},{"location":"api-reference/cli/commands/#command-reference","title":"Command Reference","text":""},{"location":"api-reference/cli/commands/#init","title":"init","text":"<p>Initialize a new prompt versioning project in the current directory.</p> <pre><code>prompt-versioner init [OPTIONS] PROJECT_NAME\n</code></pre> <p>Arguments: - <code>PROJECT_NAME</code> (required): Name of the project to initialize</p> <p>Options: - <code>--db-path PATH</code>: Custom database path (default: <code>./prompts.db</code>) - <code>--git / --no-git</code>: Enable/disable Git integration (default: enabled) - <code>--template TEXT</code>: Project template to use - <code>--force</code>: Overwrite existing configuration</p> <p>Examples: <pre><code># Basic initialization\nprompt-versioner init my-ai-project\n\n# Custom database path\nprompt-versioner init my-project --db-path /opt/prompts/db.sqlite\n\n# Without Git integration\nprompt-versioner init my-project --no-git\n\n# Force overwrite existing project\nprompt-versioner init my-project --force\n</code></pre></p> <p>Output: <pre><code>\u2705 Initialized prompt versioner project: my-ai-project\n\ud83d\udcc1 Database: ./prompts.db\n\ud83d\udd17 Git integration: enabled\n\ud83d\udcdd Config: .prompt-versioner.yaml\n\nNext steps:\n  prompt-versioner create --help\n  prompt-versioner dashboard\n</code></pre></p>"},{"location":"api-reference/cli/commands/#create","title":"create","text":"<p>Create a new prompt version.</p> <pre><code>prompt-versioner create [OPTIONS] NAME\n</code></pre> <p>Arguments: - <code>NAME</code> (required): Name of the prompt to create/update</p> <p>Options: - <code>--system-prompt TEXT</code>: System prompt content - <code>--user-prompt TEXT</code>: User prompt template - <code>--file PATH</code>: Read prompts from file - <code>--bump {major,minor,patch}</code>: Version bump type (default: patch) - <code>--metadata KEY=VALUE</code>: Add metadata (can be used multiple times) - <code>--interactive / --no-interactive</code>: Interactive prompt editor</p> <p>Examples: <pre><code># Create with inline prompts\nprompt-versioner create code-reviewer \\\n  --system-prompt \"You are an expert code reviewer.\" \\\n  --user-prompt \"Review this code: {code}\" \\\n  --bump major\n\n# Create from file\nprompt-versioner create assistant --file prompts/assistant.yaml --bump minor\n\n# Interactive creation\nprompt-versioner create classifier --interactive\n\n# With metadata\nprompt-versioner create chatbot \\\n  --system-prompt \"You are a helpful chatbot.\" \\\n  --user-prompt \"User: {message}\" \\\n  --metadata author=john.doe \\\n  --metadata purpose=\"customer support\"\n</code></pre></p> <p>File Format (YAML): <pre><code># prompts/assistant.yaml\nsystem_prompt: \"You are a helpful AI assistant.\"\nuser_prompt: \"Help the user with: {request}\"\nmetadata:\n  author: \"team@company.com\"\n  purpose: \"General assistance\"\n</code></pre></p>"},{"location":"api-reference/cli/commands/#list","title":"list","text":"<p>List prompts and their versions.</p> <pre><code>prompt-versioner list [OPTIONS] [NAME]\n</code></pre> <p>Arguments: - <code>NAME</code> (optional): Specific prompt name to list versions for</p> <p>Options: - <code>--format {table,json,yaml}</code>: Output format (default: table) - <code>--sort {name,version,created}</code>: Sort order (default: name) - <code>--limit INTEGER</code>: Limit number of results - <code>--filter TEXT</code>: Filter by name pattern</p> <p>Examples: <pre><code># List all prompts\nprompt-versioner list\n\n# List versions of specific prompt\nprompt-versioner list code-reviewer\n\n# JSON output\nprompt-versioner list --format json\n\n# Filter by pattern\nprompt-versioner list --filter \"chat*\"\n\n# Limited results\nprompt-versioner list --limit 10 --sort created\n</code></pre></p> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name         \u2502 Version \u2502 Created             \u2502 Author         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 code-reviewer\u2502 1.2.0   \u2502 2025-01-15 10:30:00 \u2502 john.doe       \u2502\n\u2502 chatbot      \u2502 2.1.0   \u2502 2025-01-15 09:15:00 \u2502 team@company   \u2502\n\u2502 classifier   \u2502 1.0.1   \u2502 2025-01-14 16:45:00 \u2502 ai-team        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"api-reference/cli/commands/#show","title":"show","text":"<p>Display detailed information about a prompt or version.</p> <pre><code>prompt-versioner show [OPTIONS] NAME [VERSION]\n</code></pre> <p>Arguments: - <code>NAME</code> (required): Prompt name - <code>VERSION</code> (optional): Specific version (default: latest)</p> <p>Options: - <code>--format {text,json,yaml}</code>: Output format (default: text) - <code>--include-metrics</code>: Include performance metrics - <code>--include-annotations</code>: Include team annotations</p> <p>Examples: <pre><code># Show latest version\nprompt-versioner show code-reviewer\n\n# Show specific version\nprompt-versioner show code-reviewer 1.0.0\n\n# Include metrics and annotations\nprompt-versioner show chatbot --include-metrics --include-annotations\n\n# JSON output\nprompt-versioner show classifier --format json\n</code></pre></p> <p>Output: <pre><code>\ud83d\udcdd Prompt: code-reviewer\n\ud83c\udff7\ufe0f  Version: 1.2.0\n\ud83d\udcc5 Created: 2025-01-15 10:30:00\n\ud83d\udc64 Author: john.doe@company.com\n\n\ud83d\udccb System Prompt:\nYou are an expert code reviewer with deep knowledge of best practices.\n\n\ud83d\udcdd User Prompt:\nReview the following code and provide feedback:\n{code}\n\nFocus on: {focus_areas}\n\n\ud83d\udcca Metadata:\n  purpose: code review automation\n  model_target: gpt-4o\n  languages: [\"python\", \"javascript\", \"go\"]\n</code></pre></p>"},{"location":"api-reference/cli/commands/#diff","title":"diff","text":"<p>Compare two versions of a prompt.</p> <pre><code>prompt-versioner diff [OPTIONS] NAME VERSION_A VERSION_B\n</code></pre> <p>Arguments: - <code>NAME</code> (required): Prompt name - <code>VERSION_A</code> (required): First version to compare - <code>VERSION_B</code> (required): Second version to compare</p> <p>Options: - <code>--format {text,unified,side-by-side}</code>: Diff display format - <code>--context INTEGER</code>: Number of context lines (default: 3) - <code>--ignore-whitespace</code>: Ignore whitespace changes - <code>--output PATH</code>: Save diff to file</p> <p>Examples: <pre><code># Basic diff\nprompt-versioner diff code-reviewer 1.0.0 1.1.0\n\n# Side-by-side comparison\nprompt-versioner diff chatbot 1.0.0 2.0.0 --format side-by-side\n\n# Save diff to file\nprompt-versioner diff classifier 1.0.0 1.0.1 --output changes.diff\n\n# Ignore whitespace\nprompt-versioner diff assistant 1.1.0 1.2.0 --ignore-whitespace\n</code></pre></p> <p>Output: <pre><code>\ud83d\udcca Diff: code-reviewer (1.0.0 \u2192 1.1.0)\n\n\ud83d\udccb System Prompt:\n- You are an expert code reviewer.\n+ You are an expert code reviewer with deep knowledge of best practices.\n\n\ud83d\udcdd User Prompt:\n  Review the following code and provide feedback:\n  {code}\n+\n+ Focus on: {focus_areas}\n\n\ud83d\udcc8 Summary:\n  \u2022 Added: 2 lines\n  \u2022 Removed: 0 lines\n  \u2022 Modified: 1 line\n</code></pre></p>"},{"location":"api-reference/cli/commands/#metrics","title":"metrics","text":"<p>View and analyze performance metrics for prompts.</p> <pre><code>prompt-versioner metrics [OPTIONS] [NAME] [VERSION]\n</code></pre> <p>Arguments: - <code>NAME</code> (optional): Prompt name to analyze - <code>VERSION</code> (optional): Specific version (default: latest)</p> <p>Options: - <code>--format {table,chart,json}</code>: Output format - <code>--period {day,week,month}</code>: Time period for analysis - <code>--limit INTEGER</code>: Limit number of metrics records - <code>--export PATH</code>: Export metrics to file</p> <p>Examples: <pre><code># Overview of all prompts\nprompt-versioner metrics\n\n# Specific prompt metrics\nprompt-versioner metrics code-reviewer\n\n# Specific version metrics\nprompt-versioner metrics chatbot 1.2.0\n\n# Weekly analysis with chart\nprompt-versioner metrics classifier --period week --format chart\n\n# Export to CSV\nprompt-versioner metrics --export metrics.csv\n</code></pre></p> <p>Output: <pre><code>\ud83d\udcca Metrics Summary: code-reviewer v1.2.0\n\n\ud83c\udfaf Performance (Last 30 days):\n  Average Quality Score: 0.87 \u00b1 0.05\n  Average Latency: 1,247ms \u00b1 324ms\n  Success Rate: 98.5%\n  Total Requests: 1,247\n\n\ud83d\udcb0 Cost Analysis:\n  Total Cost: \u20ac12.45\n  Cost per Request: \u20ac0.01\n  Token Usage: 245,670 tokens\n\n\ud83d\udcc8 Trends:\n  Quality: \u2197\ufe0f  +3.2% vs previous week\n  Latency: \u2198\ufe0f  -8.1% vs previous week\n  Cost: \u2197\ufe0f  +1.5% vs previous week\n</code></pre></p>"},{"location":"api-reference/cli/commands/#dashboard","title":"dashboard","text":"<p>Launch the interactive web dashboard.</p> <pre><code>prompt-versioner dashboard [OPTIONS]\n</code></pre> <p>Options: - <code>--host TEXT</code>: Host to bind to (default: localhost) - <code>--port INTEGER</code>: Port to bind to (default: 8080) - <code>--debug</code>: Enable debug mode - <code>--open / --no-open</code>: Open browser automatically (default: open)</p> <p>Examples: <pre><code># Launch dashboard\nprompt-versioner dashboard\n\n# Custom host and port\nprompt-versioner dashboard --host 0.0.0.0 --port 3000\n\n# Debug mode without auto-opening browser\nprompt-versioner dashboard --debug --no-open\n</code></pre></p> <p>Output: <pre><code>\ud83d\ude80 Starting Prompt Versioner Dashboard...\n\n\ud83d\udccd Local:   http://localhost:8080\n\ud83c\udf10 Network: http://192.168.1.100:8080\n\n\u2728 Features available:\n  \u2022 Prompt management and editing\n  \u2022 Version comparison and diffs\n  \u2022 Metrics visualization and analysis\n  \u2022 A/B testing management\n  \u2022 Team annotations and collaboration\n\nPress Ctrl+C to stop the server\n</code></pre></p>"},{"location":"api-reference/cli/commands/#export","title":"export","text":"<p>Export prompts and associated data.</p> <pre><code>prompt-versioner export [OPTIONS] [NAME]\n</code></pre> <p>Arguments: - <code>NAME</code> (optional): Specific prompt to export (default: all)</p> <p>Options: - <code>--output PATH</code>: Output file or directory - <code>--format {json,yaml,csv}</code>: Export format (default: json) - <code>--include-metrics</code>: Include performance metrics - <code>--include-annotations</code>: Include team annotations - <code>--version VERSION</code>: Specific version to export</p> <p>Examples: <pre><code># Export all prompts\nprompt-versioner export --output backup.json\n\n# Export specific prompt\nprompt-versioner export code-reviewer --output code-reviewer.yaml --format yaml\n\n# Export with metrics and annotations\nprompt-versioner export chatbot \\\n  --include-metrics \\\n  --include-annotations \\\n  --output chatbot-full.json\n\n# Export specific version\nprompt-versioner export classifier --version 1.0.0 --output classifier-v1.json\n</code></pre></p>"},{"location":"api-reference/cli/commands/#rollback","title":"rollback","text":"<p>Rollback a prompt to a previous version.</p> <pre><code>prompt-versioner rollback [OPTIONS] NAME TO_VERSION\n</code></pre> <p>Arguments: - <code>NAME</code> (required): Prompt name - <code>TO_VERSION</code> (required): Version to rollback to</p> <p>Options: - <code>--bump {major,minor,patch}</code>: Version bump for rollback (default: patch) - <code>--reason TEXT</code>: Reason for rollback - <code>--confirm / --no-confirm</code>: Skip confirmation prompt</p> <p>Examples: <pre><code># Rollback with confirmation\nprompt-versioner rollback code-reviewer 1.0.0\n\n# Rollback with reason\nprompt-versioner rollback chatbot 1.5.0 --reason \"Critical bug in 2.0.0\"\n\n# Force rollback without confirmation\nprompt-versioner rollback classifier 1.0.1 --no-confirm --bump major\n</code></pre></p> <p>Output: <pre><code>\u26a0\ufe0f  Rollback Operation\n\n\ud83d\udcdd Prompt: code-reviewer\n\ud83d\udd04 From: 1.2.0 \u2192 1.0.0\n\ud83c\udff7\ufe0f  New Version: 1.2.1 (rollback)\n\n\u2753 This will create a new version based on 1.0.0. Continue? [y/N]: y\n\n\u2705 Rollback completed successfully!\n\ud83d\udce6 New version: 1.2.1\n\ud83d\udcdd Reason: User requested rollback to stable version\n</code></pre></p>"},{"location":"api-reference/cli/commands/#see-also","title":"See Also","text":"<ul> <li><code>PromptVersioner</code> - Python API that powers these commands</li> <li><code>CLI Utils</code> - Utility functions used by CLI commands</li> <li>Configuration Guide - Detailed configuration options</li> </ul>"},{"location":"api-reference/cli/utils/","title":"CLI Utilities","text":"<p>Utility functions and formatters for the command-line interface.</p>"},{"location":"api-reference/cli/utils/#overview","title":"Overview","text":"<p>The CLI utilities module provides formatting, display, and helper functions used across all CLI commands. It leverages the Rich library for enhanced terminal output with colors, tables, and interactive elements.</p>"},{"location":"api-reference/cli/utils/#formatters-module","title":"Formatters Module","text":""},{"location":"api-reference/cli/utils/#overview_1","title":"Overview","text":"<p>The formatters module (<code>prompt_versioner.cli.utils.formatters</code>) provides Rich-based formatting functions for displaying prompt data in the terminal.</p> <pre><code>from prompt_versioner.cli.utils.formatters import (\n    format_prompts_table,\n    format_versions_table,\n    format_version_detail,\n    format_metrics_table,\n    format_diff_output\n)\n</code></pre>"},{"location":"api-reference/cli/utils/#functions","title":"Functions","text":""},{"location":"api-reference/cli/utils/#format_prompts_table","title":"format_prompts_table()","text":"<pre><code>def format_prompts_table(prompts: List[str], versioner: Any) -&gt; Table\n</code></pre> <p>Format a list of prompts as a Rich table with version counts and latest version info.</p> <p>Parameters: - <code>prompts</code> (List[str]): List of prompt names - <code>versioner</code> (PromptVersioner): PromptVersioner instance for data access</p> <p>Returns: - <code>Table</code>: Rich Table object ready for display</p> <p>Example: <pre><code>from prompt_versioner import PromptVersioner\nfrom prompt_versioner.cli.utils.formatters import format_prompts_table\nfrom rich.console import Console\n\npv = PromptVersioner(\"my-project\")\nprompts = pv.list_prompts()\ntable = format_prompts_table(prompts, pv)\n\nconsole = Console()\nconsole.print(table)\n</code></pre></p> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name          \u2502 Versions  \u2502 Latest \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 code-reviewer \u2502 5         \u2502 1.2.0  \u2502\n\u2502 chatbot       \u2502 12        \u2502 2.1.0  \u2502\n\u2502 classifier    \u2502 3         \u2502 1.0.1  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"api-reference/cli/utils/#format_versions_table","title":"format_versions_table()","text":"<pre><code>def format_versions_table(name: str, versions: List[Dict[str, Any]]) -&gt; Table\n</code></pre> <p>Format a list of versions for a specific prompt as a Rich table.</p> <p>Parameters: - <code>name</code> (str): Prompt name for table title - <code>versions</code> (List[Dict[str, Any]]): List of version dictionaries</p> <p>Returns: - <code>Table</code>: Rich Table object with version details</p> <p>Example: <pre><code>versions = pv.list_versions(\"code-reviewer\")\ntable = format_versions_table(\"code-reviewer\", versions)\nconsole.print(table)\n</code></pre></p> <p>Output: <pre><code>                   Versions of 'code-reviewer'\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Version \u2502 Timestamp           \u2502 Git Commit  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1.2.0   \u2502 2025-01-15 10:30:00 \u2502 a1b2c3d4    \u2502\n\u2502 1.1.0   \u2502 2025-01-14 16:45:00 \u2502 e5f6g7h8    \u2502\n\u2502 1.0.0   \u2502 2025-01-10 09:15:00 \u2502 i9j0k1l2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"api-reference/cli/utils/#format_version_detail","title":"format_version_detail()","text":"<pre><code>def format_version_detail(name: str, version: Dict[str, Any]) -&gt; None\n</code></pre> <p>Display detailed information about a specific version with panels and formatting.</p> <p>Parameters: - <code>name</code> (str): Prompt name - <code>version</code> (Dict[str, Any]): Version data dictionary</p> <p>Side Effects: - Prints formatted output directly to console</p> <p>Example: <pre><code>version = pv.get_version(\"code-reviewer\", \"1.2.0\")\nformat_version_detail(\"code-reviewer\", version)\n</code></pre></p> <p>Output: <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt: code-reviewer \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Version: 1.2.0                                         \u2502\n\u2502 Timestamp: 2025-01-15 10:30:00                         \u2502\n\u2502 Git Commit: a1b2c3d4                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nSystem Prompt:\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 You are an expert code reviewer with deep knowledge    \u2502\n\u2502 of best practices and security considerations.         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nUser Prompt:\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Review the following code and provide feedback:        \u2502\n\u2502 {code}                                                 \u2502\n\u2502                                                        \u2502\n\u2502 Focus areas: {focus_areas}                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p>"},{"location":"api-reference/cli/utils/#format_metrics_table","title":"format_metrics_table()","text":"<pre><code>def format_metrics_table(metrics: Dict[str, List[float]]) -&gt; Table\n</code></pre> <p>Format performance metrics as a statistical summary table.</p> <p>Parameters: - <code>metrics</code> (Dict[str, List[float]]): Dictionary mapping metric names to value lists</p> <p>Returns: - <code>Table</code>: Rich Table with statistical summary</p> <p>Example: <pre><code># Sample metrics data\nmetrics = {\n    \"quality_score\": [0.85, 0.87, 0.86, 0.88, 0.84],\n    \"latency_ms\": [1200, 1150, 1300, 1180, 1220],\n    \"cost_eur\": [0.003, 0.004, 0.003, 0.004, 0.003]\n}\n\ntable = format_metrics_table(metrics)\nconsole.print(table)\n</code></pre></p> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Metric       \u2502 Count \u2502 Mean  \u2502 Min   \u2502 Max    \u2502 Std    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 quality_score\u2502 5     \u2502 0.860 \u2502 0.840 \u2502 0.880  \u2502 0.015  \u2502\n\u2502 latency_ms   \u2502 5     \u2502 1210  \u2502 1150  \u2502 1300   \u2502 58.3   \u2502\n\u2502 cost_eur     \u2502 5     \u2502 0.003 \u2502 0.003 \u2502 0.004  \u2502 0.0005 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"api-reference/cli/utils/#format_diff_output","title":"format_diff_output()","text":"<pre><code>def format_diff_output(\n    name: str,\n    version_a: str,\n    version_b: str,\n    diff_result: DiffResult\n) -&gt; None\n</code></pre> <p>Display a formatted diff comparison between two prompt versions.</p> <p>Parameters: - <code>name</code> (str): Prompt name - <code>version_a</code> (str): First version being compared - <code>version_b</code> (str): Second version being compared - <code>diff_result</code> (DiffResult): Diff result object</p> <p>Side Effects: - Prints formatted diff output directly to console</p> <p>Example: <pre><code>diff = pv.diff(\"code-reviewer\", \"1.0.0\", \"1.1.0\", format_output=False)\nformat_diff_output(\"code-reviewer\", \"1.0.0\", \"1.1.0\", diff)\n</code></pre></p> <p>Output: <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Diff: code-reviewer (1.0.0 \u2192 1.1.0) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Changes: 2 additions, 0 deletions, 1 modification         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nSystem Prompt:\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 - You are an expert code reviewer.                        \u2502\n\u2502 + You are an expert code reviewer with deep knowledge     \u2502\n\u2502 + of best practices and security considerations.          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nUser Prompt:\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   Review the following code and provide feedback:         \u2502\n\u2502   {code}                                                   \u2502\n\u2502 +                                                          \u2502\n\u2502 + Focus areas: {focus_areas}                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p>"},{"location":"api-reference/cli/utils/#helper-functions","title":"Helper Functions","text":""},{"location":"api-reference/cli/utils/#prompt_input","title":"prompt_input()","text":"<pre><code>def prompt_input(message: str, default: str = None) -&gt; str\n</code></pre> <p>Enhanced input prompt with Rich formatting and default value support.</p> <p>Parameters: - <code>message</code> (str): Prompt message to display - <code>default</code> (str, optional): Default value if user presses Enter</p> <p>Returns: - <code>str</code>: User input or default value</p> <p>Example: <pre><code>from prompt_versioner.cli.utils.formatters import prompt_input\n\nname = prompt_input(\"Enter prompt name\", default=\"my-prompt\")\n# \u2192 \"Enter prompt name [my-prompt]: \"\n</code></pre></p>"},{"location":"api-reference/cli/utils/#confirm","title":"confirm()","text":"<pre><code>def confirm(message: str, default: bool = False) -&gt; bool\n</code></pre> <p>Interactive yes/no confirmation prompt.</p> <p>Parameters: - <code>message</code> (str): Confirmation message - <code>default</code> (bool): Default response (default: False)</p> <p>Returns: - <code>bool</code>: True for yes, False for no</p> <p>Example: <pre><code>from prompt_versioner.cli.utils.formatters import confirm\n\nif confirm(\"Delete this prompt?\", default=False):\n    # Proceed with deletion\n    pass\n</code></pre></p>"},{"location":"api-reference/cli/utils/#progress_bar","title":"progress_bar()","text":"<pre><code>def progress_bar(iterable, description: str = \"Processing...\")\n</code></pre> <p>Create a Rich progress bar for long-running operations.</p> <p>Parameters: - <code>iterable</code>: Iterable to process - <code>description</code> (str): Description text for progress bar</p> <p>Returns: - Progress bar context manager</p> <p>Example: <pre><code>from prompt_versioner.cli.utils.formatters import progress_bar\n\nprompts = [\"prompt1\", \"prompt2\", \"prompt3\"]\nwith progress_bar(prompts, \"Exporting prompts...\") as progress:\n    for prompt in progress:\n        # Process each prompt\n        export_prompt(prompt)\n</code></pre></p>"},{"location":"api-reference/cli/utils/#console-configuration","title":"Console Configuration","text":""},{"location":"api-reference/cli/utils/#rich-console-setup","title":"Rich Console Setup","text":"<p>The formatters module uses a pre-configured Rich console:</p> <pre><code>from rich.console import Console\nfrom rich.theme import Theme\n\n# Custom theme for Prompt Versioner CLI\ncustom_theme = Theme({\n    \"info\": \"cyan\",\n    \"warning\": \"yellow\",\n    \"error\": \"bold red\",\n    \"success\": \"bold green\",\n    \"prompt\": \"magenta\",\n    \"version\": \"blue\",\n    \"metric\": \"green\"\n})\n\nconsole = Console(theme=custom_theme)\n</code></pre>"},{"location":"api-reference/cli/utils/#color-scheme","title":"Color Scheme","text":"Element Color Usage Info Cyan General information Warning Yellow Warnings and cautions Error Bold Red Error messages Success Bold Green Success confirmations Prompt Magenta User input prompts Version Blue Version numbers Metric Green Performance metrics"},{"location":"api-reference/cli/utils/#output-formats","title":"Output Formats","text":""},{"location":"api-reference/cli/utils/#json-formatting","title":"JSON Formatting","text":"<pre><code>def format_json_output(data: Any) -&gt; str:\n    \"\"\"Format data as pretty-printed JSON.\"\"\"\n    import json\n    return json.dumps(data, indent=2, ensure_ascii=False)\n</code></pre>"},{"location":"api-reference/cli/utils/#yaml-formatting","title":"YAML Formatting","text":"<pre><code>def format_yaml_output(data: Any) -&gt; str:\n    \"\"\"Format data as YAML.\"\"\"\n    import yaml\n    return yaml.dump(data, default_flow_style=False, allow_unicode=True)\n</code></pre>"},{"location":"api-reference/cli/utils/#csv-formatting","title":"CSV Formatting","text":"<pre><code>def format_csv_output(data: List[Dict[str, Any]]) -&gt; str:\n    \"\"\"Format tabular data as CSV.\"\"\"\n    import csv\n    import io\n\n    if not data:\n        return \"\"\n\n    output = io.StringIO()\n    writer = csv.DictWriter(output, fieldnames=data[0].keys())\n    writer.writeheader()\n    writer.writerows(data)\n    return output.getvalue()\n</code></pre>"},{"location":"api-reference/cli/utils/#interactive-elements","title":"Interactive Elements","text":""},{"location":"api-reference/cli/utils/#menu-selection","title":"Menu Selection","text":"<pre><code>def select_from_menu(options: List[str], title: str = \"Select an option\") -&gt; int:\n    \"\"\"Display interactive menu and return selected index.\"\"\"\n    from rich.prompt import IntPrompt\n\n    console.print(f\"\\n[bold]{title}:[/bold]\")\n    for i, option in enumerate(options, 1):\n        console.print(f\"  {i}. {option}\")\n\n    choice = IntPrompt.ask(\n        \"Enter your choice\",\n        choices=[str(i) for i in range(1, len(options) + 1)]\n    )\n    return choice - 1\n</code></pre>"},{"location":"api-reference/cli/utils/#multi-line-input","title":"Multi-line Input","text":"<pre><code>def multiline_input(prompt: str) -&gt; str:\n    \"\"\"Get multi-line input from user.\"\"\"\n    console.print(f\"[prompt]{prompt}[/prompt]\")\n    console.print(\"[dim]Enter your text (Ctrl+D to finish):[/dim]\")\n\n    lines = []\n    try:\n        while True:\n            line = input()\n            lines.append(line)\n    except EOFError:\n        pass\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api-reference/cli/utils/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/cli/utils/#complete-cli-command-output","title":"Complete CLI Command Output","text":"<pre><code>from prompt_versioner import PromptVersioner\nfrom prompt_versioner.cli.utils.formatters import *\nfrom rich.console import Console\n\ndef list_command():\n    \"\"\"Example implementation of list command.\"\"\"\n    console = Console()\n    pv = PromptVersioner(\"my-project\")\n\n    try:\n        # Get prompts\n        prompts = pv.list_prompts()\n\n        if not prompts:\n            display_warning(\"No prompts found in this project\")\n            return\n\n        # Display table\n        table = format_prompts_table(prompts, pv)\n        console.print(table)\n\n        display_success(f\"Found {len(prompts)} prompts\")\n\n    except Exception as e:\n        display_error(\"Failed to list prompts\", str(e))\n</code></pre>"},{"location":"api-reference/cli/utils/#interactive-prompt-creation","title":"Interactive Prompt Creation","text":"<pre><code>def create_command_interactive():\n    \"\"\"Example interactive prompt creation.\"\"\"\n    console = Console()\n\n    # Get basic info\n    name = prompt_input(\"Prompt name\")\n    system_prompt = multiline_input(\"System prompt\")\n    user_prompt = multiline_input(\"User prompt\")\n\n    # Select bump type\n    bump_options = [\"major\", \"minor\", \"patch\"]\n    bump_index = select_from_menu(bump_options, \"Version bump type\")\n    bump_type = bump_options[bump_index]\n\n    # Confirm creation\n    if confirm(f\"Create prompt '{name}' with {bump_type} version?\"):\n        try:\n            pv = PromptVersioner(\"my-project\")\n            version = pv.save_version(\n                name=name,\n                system_prompt=system_prompt,\n                user_prompt=user_prompt,\n                bump_type=bump_type\n            )\n            display_success(f\"Created prompt '{name}' version {version}\")\n        except Exception as e:\n            display_error(\"Failed to create prompt\", str(e))\n</code></pre>"},{"location":"api-reference/cli/utils/#see-also","title":"See Also","text":"<ul> <li><code>CLI Commands</code> - CLI commands that use these utilities</li> <li>Rich Library - Terminal formatting library</li> <li><code>PromptVersioner</code> - Core functionality accessed by CLI</li> </ul>"},{"location":"api-reference/core/enums/","title":"Enums","text":"<p>Core enumeration classes used throughout the Prompt Versioner library for versioning and configuration.</p>"},{"location":"api-reference/core/enums/#overview","title":"Overview","text":"<p>The enums module defines standard constants and types used for version management, pre-release labeling, and system configuration. All enums follow Python's standard <code>enum.Enum</code> pattern.</p>"},{"location":"api-reference/core/enums/#versionbump","title":"VersionBump","text":"<p>Defines the type of version increment for semantic versioning operations.</p> <pre><code>from prompt_versioner.core.enums import VersionBump\n</code></pre>"},{"location":"api-reference/core/enums/#enum-definition","title":"Enum Definition","text":"<pre><code>class VersionBump(Enum):\n    \"\"\"Type of version bump following SemVer 2.0.0.\"\"\"\n\n    MAJOR = \"major\"  # Breaking changes (non retrocompatibile)\n    MINOR = \"minor\"  # New features (retrocompatibile)\n    PATCH = \"patch\"  # Bug fixes (retrocompatibile)\n</code></pre>"},{"location":"api-reference/core/enums/#values","title":"Values","text":"Value String Description Version Impact <code>MAJOR</code> \"major\" Breaking changes, non-backward compatible 1.0.0 \u2192 2.0.0 <code>MINOR</code> \"minor\" New features, backward compatible 1.0.0 \u2192 1.1.0 <code>PATCH</code> \"patch\" Bug fixes, backward compatible 1.0.0 \u2192 1.0.1"},{"location":"api-reference/core/enums/#usage-examples","title":"Usage Examples","text":"<pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\npv = PromptVersioner(\"my-project\")\n\n# Major version - breaking change\npv.save_version(\n    name=\"classifier\",\n    system_prompt=\"You are a classification expert.\",\n    user_prompt=\"Classify: {text}\",\n    bump_type=VersionBump.MAJOR  # Creates 1.0.0\n)\n\n# Minor version - improvement\npv.save_version(\n    name=\"classifier\",\n    system_prompt=\"You are an expert text classifier with deep understanding.\",\n    user_prompt=\"Classify the following text: {text}\",\n    bump_type=VersionBump.MINOR  # Creates 1.1.0\n)\n\n# Patch version - small fix\npv.save_version(\n    name=\"classifier\",\n    system_prompt=\"You are an expert text classifier with deep understanding.\",\n    user_prompt=\"Classify the following text carefully: {text}\",\n    bump_type=VersionBump.PATCH  # Creates 1.1.1\n)\n</code></pre>"},{"location":"api-reference/core/enums/#when-to-use-each-type","title":"When to Use Each Type","text":""},{"location":"api-reference/core/enums/#major-breaking-changes","title":"MAJOR (Breaking Changes)","text":"<p>Use when changes fundamentally alter the prompt's behavior or interface:</p> <pre><code># Before: Simple classification\nsystem_prompt = \"You are a classifier.\"\nuser_prompt = \"Classify: {text}\"\n\n# After: Structured output format (breaking change)\nsystem_prompt = \"You are a classifier. Return JSON format.\"\nuser_prompt = \"Classify: {text}. Format: {\\\"category\\\": \\\"...\\\", \\\"confidence\\\": 0.9}\"\n\n# This is a MAJOR change\nbump_type = VersionBump.MAJOR\n</code></pre>"},{"location":"api-reference/core/enums/#minor-new-features","title":"MINOR (New Features)","text":"<p>Use when adding new capabilities while maintaining backward compatibility:</p> <pre><code># Before: Basic classification\nsystem_prompt = \"You are a classifier.\"\nuser_prompt = \"Classify: {text}\"\n\n# After: Added context awareness (compatible)\nsystem_prompt = \"You are a classifier with context awareness.\"\nuser_prompt = \"Context: {context}\\nClassify: {text}\"\n\n# This is a MINOR change\nbump_type = VersionBump.MINOR\n</code></pre>"},{"location":"api-reference/core/enums/#patch-bug-fixes","title":"PATCH (Bug Fixes)","text":"<p>Use for small corrections, typos, or minor improvements:</p> <pre><code># Before: Typo in prompt\nsystem_prompt = \"You are a classifer.\"  # Typo!\nuser_prompt = \"Classify: {text}\"\n\n# After: Fixed typo\nsystem_prompt = \"You are a classifier.\"  # Fixed\nuser_prompt = \"Classify: {text}\"\n\n# This is a PATCH change\nbump_type = VersionBump.PATCH\n</code></pre>"},{"location":"api-reference/core/enums/#string-conversion","title":"String Conversion","text":"<p>The VersionBump enum can be parsed from strings:</p> <pre><code>from prompt_versioner.core.version_manager import VersionManager\n\n# Parse from string (case-insensitive)\nbump = VersionManager.parse_bump_type(\"major\")    # \u2192 VersionBump.MAJOR\nbump = VersionManager.parse_bump_type(\"MINOR\")    # \u2192 VersionBump.MINOR\nbump = VersionManager.parse_bump_type(\"patch\")    # \u2192 VersionBump.PATCH\n\n# Invalid strings return None\nbump = VersionManager.parse_bump_type(\"invalid\")  # \u2192 None\n</code></pre>"},{"location":"api-reference/core/enums/#prereleaselabel","title":"PreReleaseLabel","text":"<p>Defines pre-release labels for development and staging versions.</p> <pre><code>from prompt_versioner.core.enums import PreReleaseLabel\n</code></pre>"},{"location":"api-reference/core/enums/#enum-definition_1","title":"Enum Definition","text":"<pre><code>class PreReleaseLabel(Enum):\n    \"\"\"Pre-release labels for versioning.\"\"\"\n\n    SNAPSHOT = \"SNAPSHOT\"  # Development version\n    MILESTONE = \"M\"        # Milestone version\n    RC = \"RC\"             # Release Candidate\n    STABLE = None         # Stable release (no label)\n</code></pre>"},{"location":"api-reference/core/enums/#values_1","title":"Values","text":"Value String Description Example Version <code>SNAPSHOT</code> \"SNAPSHOT\" Development/nightly builds 1.0.0-SNAPSHOT <code>MILESTONE</code> \"M\" Feature milestone releases 1.0.0-M.1 <code>RC</code> \"RC\" Release candidates 1.0.0-RC.1 <code>STABLE</code> None Stable production releases 1.0.0"},{"location":"api-reference/core/enums/#usage-examples_1","title":"Usage Examples","text":"<pre><code>from prompt_versioner.core.version_manager import VersionManager\nfrom prompt_versioner.core.enums import VersionBump, PreReleaseLabel\n\n# Development snapshot\nversion = VersionManager.calculate_next_version(\n    current_version=\"1.0.0\",\n    bump_type=VersionBump.MINOR,\n    pre_label=PreReleaseLabel.SNAPSHOT\n)\n# \u2192 \"1.1.0-SNAPSHOT\"\n\n# Milestone release\nversion = VersionManager.calculate_next_version(\n    current_version=\"1.1.0-SNAPSHOT\",\n    bump_type=VersionBump.PATCH,\n    pre_label=PreReleaseLabel.MILESTONE,\n    pre_number=1\n)\n# \u2192 \"1.1.0-M.1\"\n\n# Release candidate\nversion = VersionManager.calculate_next_version(\n    current_version=\"1.1.0-M.1\",\n    bump_type=VersionBump.PATCH,\n    pre_label=PreReleaseLabel.RC,\n    pre_number=1\n)\n# \u2192 \"1.1.0-RC.1\"\n\n# Final stable release\nversion = VersionManager.calculate_next_version(\n    current_version=\"1.1.0-RC.1\",\n    bump_type=VersionBump.PATCH,\n    pre_label=PreReleaseLabel.STABLE\n)\n# \u2192 \"1.1.0\"\n</code></pre>"},{"location":"api-reference/core/enums/#pre-release-workflow","title":"Pre-Release Workflow","text":"<p>A typical development workflow using pre-release labels:</p> <pre><code># 1. Start development with snapshot\ndev_version = VersionManager.calculate_next_version(\n    \"1.0.0\", VersionBump.MINOR, PreReleaseLabel.SNAPSHOT\n)\n# \u2192 \"1.1.0-SNAPSHOT\"\n\n# 2. Feature complete - create milestone\nmilestone = VersionManager.calculate_next_version(\n    \"1.1.0-SNAPSHOT\", VersionBump.PATCH, PreReleaseLabel.MILESTONE, 1\n)\n# \u2192 \"1.1.0-M.1\"\n\n# 3. Testing phase - release candidate\nrc1 = VersionManager.calculate_next_version(\n    \"1.1.0-M.1\", VersionBump.PATCH, PreReleaseLabel.RC, 1\n)\n# \u2192 \"1.1.0-RC.1\"\n\n# 4. Bug fixes - new release candidate\nrc2 = VersionManager.calculate_next_version(\n    \"1.1.0-RC.1\", VersionBump.PATCH, PreReleaseLabel.RC, 2\n)\n# \u2192 \"1.1.0-RC.2\"\n\n# 5. Production ready - stable release\nstable = VersionManager.calculate_next_version(\n    \"1.1.0-RC.2\", VersionBump.PATCH, PreReleaseLabel.STABLE\n)\n# \u2192 \"1.1.0\"\n</code></pre>"},{"location":"api-reference/core/enums/#string-conversion_1","title":"String Conversion","text":"<p>PreReleaseLabel can be parsed from strings:</p> <pre><code>from prompt_versioner.core.version_manager import VersionManager\n\n# Parse from string (case-insensitive)\nlabel = VersionManager.parse_pre_label(\"snapshot\")    # \u2192 PreReleaseLabel.SNAPSHOT\nlabel = VersionManager.parse_pre_label(\"RC\")          # \u2192 PreReleaseLabel.RC\nlabel = VersionManager.parse_pre_label(\"m\")           # \u2192 PreReleaseLabel.MILESTONE\nlabel = VersionManager.parse_pre_label(\"stable\")      # \u2192 PreReleaseLabel.STABLE\n\n# Alternative string formats\nlabel = VersionManager.parse_pre_label(\"milestone\")           # \u2192 PreReleaseLabel.MILESTONE\nlabel = VersionManager.parse_pre_label(\"release_candidate\")   # \u2192 PreReleaseLabel.RC\nlabel = VersionManager.parse_pre_label(\"\")                    # \u2192 PreReleaseLabel.STABLE\n</code></pre>"},{"location":"api-reference/core/enums/#version-precedence","title":"Version Precedence","text":"<p>Pre-release versions have lower precedence than stable versions:</p> <pre><code>from prompt_versioner.core.version_manager import VersionManager\n\n# Stable &gt; pre-release\nresult = VersionManager.compare_versions(\"1.0.0\", \"1.0.0-RC.1\")  # \u2192 1\n\n# Among pre-releases: SNAPSHOT &lt; M &lt; RC\nresult = VersionManager.compare_versions(\"1.0.0-SNAPSHOT\", \"1.0.0-M.1\")  # \u2192 -1\nresult = VersionManager.compare_versions(\"1.0.0-M.1\", \"1.0.0-RC.1\")      # \u2192 -1\n\n# Same label, compare numbers\nresult = VersionManager.compare_versions(\"1.0.0-RC.1\", \"1.0.0-RC.2\")     # \u2192 -1\n</code></pre>"},{"location":"api-reference/core/enums/#validation-and-error-handling","title":"Validation and Error Handling","text":"<p>Both enums have validation through the VersionManager:</p> <pre><code>from prompt_versioner.core.version_manager import VersionManager\n\n# Valid enum values\nassert VersionManager.parse_bump_type(\"major\") == VersionBump.MAJOR\nassert VersionManager.parse_pre_label(\"RC\") == PreReleaseLabel.RC\n\n# Invalid values return None\nassert VersionManager.parse_bump_type(\"invalid\") is None\nassert VersionManager.parse_pre_label(\"invalid\") is None\n\n# Use in your code with validation\ndef safe_version_bump(bump_string: str) -&gt; VersionBump:\n    bump = VersionManager.parse_bump_type(bump_string)\n    if bump is None:\n        raise ValueError(f\"Invalid bump type: {bump_string}\")\n    return bump\n</code></pre>"},{"location":"api-reference/core/enums/#see-also","title":"See Also","text":"<ul> <li><code>VersionManager</code> - Uses these enums for version calculations</li> <li><code>PromptVersioner</code> - Main class that accepts these enum values</li> <li>Semantic Versioning - Official SemVer specification</li> </ul>"},{"location":"api-reference/core/version-manager/","title":"Version Manager","text":"<p>The <code>VersionManager</code> class provides utilities for semantic version management and calculations following SemVer 2.0.0 standards.</p>"},{"location":"api-reference/core/version-manager/#overview","title":"Overview","text":"<p>The Version Manager is a utility class responsible for: - Parsing and validating semantic version strings - Calculating next versions with different bump types - Comparing versions for ordering - Supporting pre-release labels (SNAPSHOT, M, RC)</p>"},{"location":"api-reference/core/version-manager/#class-reference","title":"Class Reference","text":""},{"location":"api-reference/core/version-manager/#versionmanager","title":"VersionManager","text":"<pre><code>from prompt_versioner.core.version_manager import VersionManager\n</code></pre> <p>A static utility class for semantic versioning operations. All methods are static and don't require instantiation.</p>"},{"location":"api-reference/core/version-manager/#methods","title":"Methods","text":""},{"location":"api-reference/core/version-manager/#parse_bump_type","title":"parse_bump_type()","text":"<pre><code>@staticmethod\ndef parse_bump_type(bump_type: VersionBump | str | None) -&gt; Optional[VersionBump]\n</code></pre> <p>Parse bump type from string or enum value.</p> <p>Parameters: - <code>bump_type</code> (VersionBump | str | None): VersionBump enum or string (\"major\", \"minor\", \"patch\")</p> <p>Returns: - <code>Optional[VersionBump]</code>: Parsed VersionBump enum or None if invalid</p> <p>Example: <pre><code>from prompt_versioner.core.version_manager import VersionManager\nfrom prompt_versioner.core.enums import VersionBump\n\n# Parse from string\nbump = VersionManager.parse_bump_type(\"patch\")  # \u2192 VersionBump.PATCH\nbump = VersionManager.parse_bump_type(\"MAJOR\")  # \u2192 VersionBump.MAJOR\n\n# Pass through enum\nbump = VersionManager.parse_bump_type(VersionBump.MINOR)  # \u2192 VersionBump.MINOR\n</code></pre></p>"},{"location":"api-reference/core/version-manager/#parse_pre_label","title":"parse_pre_label()","text":"<pre><code>@staticmethod\ndef parse_pre_label(pre_label: PreReleaseLabel | str | None) -&gt; Optional[PreReleaseLabel]\n</code></pre> <p>Parse pre-release label from string or enum value.</p> <p>Parameters: - <code>pre_label</code> (PreReleaseLabel | str | None): PreReleaseLabel enum or string</p> <p>Returns: - <code>Optional[PreReleaseLabel]</code>: Parsed PreReleaseLabel enum or None if invalid</p> <p>Example: <pre><code># Parse from string\nlabel = VersionManager.parse_pre_label(\"snapshot\")  # \u2192 PreReleaseLabel.SNAPSHOT\nlabel = VersionManager.parse_pre_label(\"RC\")        # \u2192 PreReleaseLabel.RC\nlabel = VersionManager.parse_pre_label(\"m\")         # \u2192 PreReleaseLabel.MILESTONE\nlabel = VersionManager.parse_pre_label(\"stable\")    # \u2192 PreReleaseLabel.STABLE\n</code></pre></p>"},{"location":"api-reference/core/version-manager/#parse_version","title":"parse_version()","text":"<pre><code>@staticmethod\ndef parse_version(version_string: str) -&gt; Optional[Dict]\n</code></pre> <p>Parse a semantic version string into its components.</p> <p>Parameters: - <code>version_string</code> (str): Version string (e.g., \"1.2.3-RC.1\")</p> <p>Returns: - <code>Optional[Dict]</code>: Dictionary with parsed components or None if invalid</p> <p>Components: - <code>major</code> (int): Major version number - <code>minor</code> (int): Minor version number - <code>patch</code> (int): Patch version number - <code>pre_label</code> (str): Pre-release label (if any) - <code>pre_number</code> (int): Pre-release number (if any)</p> <p>Example: <pre><code># Parse different version formats\nparsed = VersionManager.parse_version(\"1.2.3\")\n# \u2192 {\"major\": 1, \"minor\": 2, \"patch\": 3, \"pre_label\": None, \"pre_number\": None}\n\nparsed = VersionManager.parse_version(\"2.0.0-RC.1\")\n# \u2192 {\"major\": 2, \"minor\": 0, \"patch\": 0, \"pre_label\": \"RC\", \"pre_number\": 1}\n\nparsed = VersionManager.parse_version(\"1.5.0-SNAPSHOT\")\n# \u2192 {\"major\": 1, \"minor\": 5, \"patch\": 0, \"pre_label\": \"SNAPSHOT\", \"pre_number\": None}\n</code></pre></p>"},{"location":"api-reference/core/version-manager/#format_version","title":"format_version()","text":"<pre><code>@staticmethod\ndef format_version(\n    major: int,\n    minor: int,\n    patch: int,\n    pre_label: Optional[PreReleaseLabel] = None,\n    pre_number: Optional[int] = None,\n) -&gt; str\n</code></pre> <p>Format version components into a semantic version string.</p> <p>Parameters: - <code>major</code> (int): Major version number - <code>minor</code> (int): Minor version number - <code>patch</code> (int): Patch version number - <code>pre_label</code> (Optional[PreReleaseLabel]): Pre-release label - <code>pre_number</code> (Optional[int]): Pre-release number</p> <p>Returns: - <code>str</code>: Formatted version string</p> <p>Example: <pre><code>from prompt_versioner.core.enums import PreReleaseLabel\n\n# Basic version\nversion = VersionManager.format_version(1, 2, 3)  # \u2192 \"1.2.3\"\n\n# With pre-release label\nversion = VersionManager.format_version(1, 0, 0, PreReleaseLabel.RC, 1)  # \u2192 \"1.0.0-RC.1\"\n\n# Snapshot version\nversion = VersionManager.format_version(2, 1, 0, PreReleaseLabel.SNAPSHOT)  # \u2192 \"2.1.0-SNAPSHOT\"\n</code></pre></p>"},{"location":"api-reference/core/version-manager/#calculate_next_version","title":"calculate_next_version()","text":"<pre><code>@staticmethod\ndef calculate_next_version(\n    current_version: Optional[str],\n    bump_type: VersionBump,\n    pre_label: Optional[PreReleaseLabel] = None,\n    pre_number: Optional[int] = None,\n) -&gt; str\n</code></pre> <p>Calculate the next semantic version following SemVer 2.0.0 rules.</p> <p>Parameters: - <code>current_version</code> (Optional[str]): Current version string or None for first version - <code>bump_type</code> (VersionBump): Type of version bump (MAJOR, MINOR, PATCH) - <code>pre_label</code> (Optional[PreReleaseLabel]): Pre-release label for new version - <code>pre_number</code> (Optional[int]): Pre-release number</p> <p>Returns: - <code>str</code>: Next version string</p> <p>Example: <pre><code>from prompt_versioner.core.enums import VersionBump, PreReleaseLabel\n\n# First version\nnext_ver = VersionManager.calculate_next_version(None, VersionBump.PATCH)\n# \u2192 \"1.0.0\"\n\n# Increment patch\nnext_ver = VersionManager.calculate_next_version(\"1.0.0\", VersionBump.PATCH)\n# \u2192 \"1.0.1\"\n\n# Minor with pre-release\nnext_ver = VersionManager.calculate_next_version(\n    \"1.0.0\",\n    VersionBump.MINOR,\n    PreReleaseLabel.SNAPSHOT\n)\n# \u2192 \"1.1.0-SNAPSHOT\"\n\n# Release candidate\nnext_ver = VersionManager.calculate_next_version(\n    \"1.0.0\",\n    VersionBump.MINOR,\n    PreReleaseLabel.RC,\n    1\n)\n# \u2192 \"1.1.0-RC.1\"\n\n# Promote RC to stable\nnext_ver = VersionManager.calculate_next_version(\n    \"1.0.0-RC.2\",\n    VersionBump.PATCH,\n    PreReleaseLabel.STABLE\n)\n# \u2192 \"1.0.0\"\n</code></pre></p>"},{"location":"api-reference/core/version-manager/#is_valid_semver","title":"is_valid_semver()","text":"<pre><code>@staticmethod\ndef is_valid_semver(version_string: str) -&gt; bool\n</code></pre> <p>Check if a version string is valid SemVer format.</p> <p>Parameters: - <code>version_string</code> (str): Version string to validate</p> <p>Returns: - <code>bool</code>: True if valid semantic version</p> <p>Example: <pre><code># Valid versions\nVersionManager.is_valid_semver(\"1.0.0\")        # \u2192 True\nVersionManager.is_valid_semver(\"2.1.3-RC.1\")   # \u2192 True\nVersionManager.is_valid_semver(\"0.0.1-SNAPSHOT\") # \u2192 True\n\n# Invalid versions\nVersionManager.is_valid_semver(\"1.0\")          # \u2192 False\nVersionManager.is_valid_semver(\"v1.0.0\")       # \u2192 False\nVersionManager.is_valid_semver(\"1.0.0.1\")      # \u2192 False\n</code></pre></p>"},{"location":"api-reference/core/version-manager/#compare_versions","title":"compare_versions()","text":"<pre><code>@staticmethod\ndef compare_versions(version1: str, version2: str) -&gt; int\n</code></pre> <p>Compare two semantic versions according to SemVer precedence rules.</p> <p>Parameters: - <code>version1</code> (str): First version to compare - <code>version2</code> (str): Second version to compare</p> <p>Returns: - <code>int</code>: -1 if version1 &lt; version2, 0 if equal, 1 if version1 &gt; version2</p> <p>Precedence Rules: 1. Major.minor.patch numbers are compared numerically 2. Stable versions have higher precedence than pre-release 3. Pre-release precedence: SNAPSHOT &lt; M &lt; RC 4. Pre-release numbers are compared numerically</p> <p>Example: <pre><code># Basic comparison\nVersionManager.compare_versions(\"1.0.0\", \"1.0.1\")    # \u2192 -1\nVersionManager.compare_versions(\"2.0.0\", \"1.9.9\")    # \u2192 1\nVersionManager.compare_versions(\"1.0.0\", \"1.0.0\")    # \u2192 0\n\n# Pre-release comparison\nVersionManager.compare_versions(\"1.0.0-RC.1\", \"1.0.0\")      # \u2192 -1 (stable &gt; pre-release)\nVersionManager.compare_versions(\"1.0.0-M.1\", \"1.0.0-RC.1\")  # \u2192 -1 (M &lt; RC)\nVersionManager.compare_versions(\"1.0.0-RC.1\", \"1.0.0-RC.2\") # \u2192 -1 (RC.1 &lt; RC.2)\n</code></pre></p>"},{"location":"api-reference/core/version-manager/#version-bump-types","title":"Version Bump Types","text":"<p>The Version Manager works with the <code>VersionBump</code> enum for semantic versioning:</p>"},{"location":"api-reference/core/version-manager/#versionbump-enum","title":"VersionBump Enum","text":"<pre><code>from prompt_versioner.core.enums import VersionBump\n\nclass VersionBump(Enum):\n    MAJOR = \"major\"  # Breaking changes (non-backward compatible)\n    MINOR = \"minor\"  # New features (backward compatible)\n    PATCH = \"patch\"  # Bug fixes (backward compatible)\n</code></pre>"},{"location":"api-reference/core/version-manager/#usage-examples","title":"Usage Examples","text":"<pre><code># Major version bump - breaking changes\nnext_version = VersionManager.calculate_next_version(\"1.5.2\", VersionBump.MAJOR)\n# \u2192 \"2.0.0\"\n\n# Minor version bump - new features\nnext_version = VersionManager.calculate_next_version(\"1.5.2\", VersionBump.MINOR)\n# \u2192 \"1.6.0\"\n\n# Patch version bump - bug fixes\nnext_version = VersionManager.calculate_next_version(\"1.5.2\", VersionBump.PATCH)\n# \u2192 \"1.5.3\"\n</code></pre>"},{"location":"api-reference/core/version-manager/#pre-release-labels","title":"Pre-Release Labels","text":"<p>The Version Manager supports pre-release versioning with labels:</p>"},{"location":"api-reference/core/version-manager/#prereleaselabel-enum","title":"PreReleaseLabel Enum","text":"<pre><code>from prompt_versioner.core.enums import PreReleaseLabel\n\nclass PreReleaseLabel(Enum):\n    SNAPSHOT = \"SNAPSHOT\"  # Development version\n    MILESTONE = \"M\"        # Milestone version\n    RC = \"RC\"             # Release Candidate\n    STABLE = None         # Stable release (no label)\n</code></pre>"},{"location":"api-reference/core/version-manager/#pre-release-workflows","title":"Pre-Release Workflows","text":"<pre><code># Development snapshot\ndev_version = VersionManager.calculate_next_version(\n    \"1.0.0\",\n    VersionBump.MINOR,\n    PreReleaseLabel.SNAPSHOT\n)\n# \u2192 \"1.1.0-SNAPSHOT\"\n\n# Milestone releases\nmilestone = VersionManager.calculate_next_version(\n    \"1.1.0-SNAPSHOT\",\n    VersionBump.PATCH,\n    PreReleaseLabel.MILESTONE,\n    1\n)\n# \u2192 \"1.1.0-M.1\"\n\n# Release candidate\nrc = VersionManager.calculate_next_version(\n    \"1.1.0-M.1\",\n    VersionBump.PATCH,\n    PreReleaseLabel.RC,\n    1\n)\n# \u2192 \"1.1.0-RC.1\"\n\n# Final stable release\nstable = VersionManager.calculate_next_version(\n    \"1.1.0-RC.1\",\n    VersionBump.PATCH,\n    PreReleaseLabel.STABLE\n)\n# \u2192 \"1.1.0\"\n</code></pre>"},{"location":"api-reference/core/version-manager/#integration-with-promptversioner","title":"Integration with PromptVersioner","text":"<p>The Version Manager is used internally by the main <code>PromptVersioner</code> class:</p> <pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\npv = PromptVersioner(\"my-project\")\n\n# When you call save_version, VersionManager calculates the next version\nversion = pv.save_version(\n    name=\"my_prompt\",\n    system_prompt=\"You are a helpful assistant.\",\n    user_prompt=\"Help with: {request}\",\n    bump_type=VersionBump.MAJOR  # VersionManager calculates \"1.0.0\"\n)\n</code></pre>"},{"location":"api-reference/core/version-manager/#error-handling","title":"Error Handling","text":"<p>The Version Manager handles invalid inputs gracefully:</p> <pre><code># Invalid version strings return None\nparsed = VersionManager.parse_version(\"invalid\")  # \u2192 None\n\n# Invalid bump types return None\nbump = VersionManager.parse_bump_type(\"invalid\")  # \u2192 None\n\n# Version comparison falls back to string comparison for invalid versions\nresult = VersionManager.compare_versions(\"invalid1\", \"invalid2\")  # \u2192 string comparison\n</code></pre>"},{"location":"api-reference/core/version-manager/#see-also","title":"See Also","text":"<ul> <li><code>PromptVersioner</code> - Main versioner class that uses VersionManager</li> <li><code>VersionBump</code> - Version increment types</li> <li><code>PreReleaseLabel</code> - Pre-release label types</li> </ul>"},{"location":"api-reference/core/versioner/","title":"Versioner","text":"<p>The <code>PromptVersioner</code> class is the main entry point for the Prompt Versioner library. It provides a high-level interface for managing prompts, versions, metrics, and testing.</p>"},{"location":"api-reference/core/versioner/#overview","title":"Overview","text":"<p>The <code>PromptVersioner</code> class orchestrates all the components of the system, providing a unified API for:</p> <ul> <li>Creating and managing prompt versions</li> <li>Version control and history tracking</li> <li>Metrics collection and analysis</li> <li>Git integration and tracking</li> <li>Import/export functionality</li> </ul>"},{"location":"api-reference/core/versioner/#class-reference","title":"Class Reference","text":"<p>The main <code>PromptVersioner</code> class can be found in <code>prompt_versioner.core.versioner.PromptVersioner</code>.</p>"},{"location":"api-reference/core/versioner/#key-methods","title":"Key Methods","text":"<ul> <li><code>save_version()</code> - Save a new prompt version</li> <li><code>get_version()</code> - Retrieve a specific version</li> <li><code>get_latest()</code> - Get the latest version of a prompt</li> <li><code>list_versions()</code> - List all versions of a prompt</li> <li><code>list_prompts()</code> - List all prompt names</li> <li><code>log_metrics()</code> - Track performance metrics</li> <li><code>diff()</code> - Compare versions</li> <li><code>rollback()</code> - Rollback to a previous version</li> </ul>"},{"location":"api-reference/core/versioner/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/core/versioner/#basic-initialization","title":"Basic Initialization","text":"<pre><code>from prompt_versioner import PromptVersioner\n\n# Default configuration (creates prompts.db in current directory)\nversioner = PromptVersioner(project_name=\"my-project\")\n\n# Custom database path and disable Git\nversioner = PromptVersioner(\n    project_name=\"my-project\",\n    db_path=\"/path/to/prompts.db\",\n    enable_git=False\n)\n\n# With Git integration enabled\nversioner = PromptVersioner(\n    project_name=\"my-project\",\n    enable_git=True,\n    auto_track=True\n)\n</code></pre>"},{"location":"api-reference/core/versioner/#creating-and-managing-prompt-versions","title":"Creating and Managing Prompt Versions","text":"<pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\nversioner = PromptVersioner(project_name=\"my-app\")\n\n# Save a new prompt version\nversion_id = versioner.save_version(\n    name=\"code_reviewer\",\n    system_prompt=\"You are an expert code reviewer.\",\n    user_prompt=\"Review this code:\\n{code}\",\n    bump_type=VersionBump.MAJOR,  # Creates version 1.0.0\n    metadata={\"type\": \"code_review\", \"model_target\": \"gpt-4\"}\n)\n\n# Get the latest version\nlatest = versioner.get_latest(\"code_reviewer\")\nprint(f\"Latest version: {latest['version']}\")\n\n# List all prompts\nprompts = versioner.list_prompts()\nfor prompt_name in prompts:\n    print(f\"Prompt: {prompt_name}\")\n\n# List versions for a specific prompt\nversions = versioner.list_versions(\"code_reviewer\")\nfor version in versions:\n    print(f\"v{version['version']}: {version['timestamp']}\")\n</code></pre>"},{"location":"api-reference/core/versioner/#version-management","title":"Version Management","text":"<pre><code># Create incremental versions\nversioner.save_version(\n    name=\"code_reviewer\",\n    system_prompt=\"You are an EXPERT code reviewer with deep knowledge.\",\n    user_prompt=\"Review this code thoroughly:\\n{code}\\n\\nProvide detailed feedback.\",\n    bump_type=VersionBump.MINOR,  # Creates version 1.1.0\n    metadata={\"improvement\": \"enhanced expertise\"}\n)\n\n# Get a specific version\nversion_data = versioner.get_version(\"code_reviewer\", \"1.0.0\")\nprint(f\"System prompt: {version_data['system_prompt']}\")\nprint(f\"User prompt: {version_data['user_prompt']}\")\n\n# Compare versions\ndiff = versioner.diff(\"code_reviewer\", \"1.0.0\", \"1.1.0\")\nprint(f\"Changes: {diff.summary}\")\n\n# Rollback to a previous version\nrollback_id = versioner.rollback(\"code_reviewer\", \"1.0.0\")\nprint(f\"Rolled back, new version ID: {rollback_id}\")\n</code></pre>"},{"location":"api-reference/core/versioner/#metrics-tracking","title":"Metrics Tracking","text":"<pre><code># Log metrics for a prompt usage\nversioner.log_metrics(\n    name=\"code_reviewer\",\n    version=\"1.1.0\",\n    model_name=\"gpt-4o\",\n    input_tokens=150,\n    output_tokens=250,\n    latency_ms=420.5,\n    quality_score=0.95,\n    cost_eur=0.003,\n    temperature=0.7,\n    success=True,\n    metadata={\"user_feedback\": \"excellent\"}\n)\n\n# Get metrics for analysis\nversion = versioner.get_version(\"code_reviewer\", \"1.1.0\")\nmetrics = versioner.storage.get_metrics(version_id=version[\"id\"], limit=100)\n\n# Calculate averages\nif metrics:\n    avg_quality = sum(m[\"quality_score\"] for m in metrics if m[\"quality_score\"]) / len(metrics)\n    avg_latency = sum(m[\"latency_ms\"] for m in metrics if m[\"latency_ms\"]) / len(metrics)\n    print(f\"Average quality: {avg_quality:.2f}\")\n    print(f\"Average latency: {avg_latency:.2f}ms\")\n</code></pre>"},{"location":"api-reference/core/versioner/#rendering-prompts","title":"Rendering Prompts","text":"<pre><code># Render a prompt with variables\nrendered = versioner.render_prompt(\n    prompt_id=prompt_id,\n    version=\"1.1.0\",\n    variables={\n        \"role\": \"Python developer\",\n        \"task\": \"debugging a memory leak\"\n    }\n)\nprint(rendered)\n# Output: \"You are an expert Python developer. Please assist with: debugging a memory leak\"\n\n# Render with different variables\nrendered = versioner.render_prompt(\n    prompt_id=prompt_id,\n    variables={\"role\": \"data scientist\", \"task\": \"analyzing customer churn\"}\n)\n</code></pre>"},{"location":"api-reference/core/versioner/#configuration-options","title":"Configuration Options","text":""},{"location":"api-reference/core/versioner/#database-configuration","title":"Database Configuration","text":"<pre><code># SQLite database (default)\nversioner = PromptVersioner(db_path=\"/opt/prompts/production.db\")\n\n# In-memory database (for testing)\nversioner = PromptVersioner(db_path=\":memory:\")\n\n# Custom connection string\nversioner = PromptVersioner(\n    connection_string=\"postgresql://user:pass@localhost/prompts\"\n)\n</code></pre>"},{"location":"api-reference/core/versioner/#git-integration","title":"Git Integration","text":"<pre><code># Enable Git tracking\nversioner = PromptVersioner(\n    db_path=\"prompts.db\",\n    git_repo=\"/path/to/git/repo\",\n    git_auto_commit=True\n)\n\n# Custom Git configuration\ngit_config = {\n    \"repository_path\": \"/path/to/git/repo\",\n    \"auto_commit\": True,\n    \"branch\": \"prompts\",\n    \"commit_message_template\": \"feat: {action} prompt {prompt_id} v{version}\"\n}\nversioner = PromptVersioner(git_config=git_config)\n</code></pre>"},{"location":"api-reference/core/versioner/#export-operations","title":"Export Operations","text":"<pre><code>from pathlib import Path\n\n# Export a prompt and all its versions\nversioner.export_prompt(\n    name=\"code_reviewer\",\n    output_file=Path(\"backups/code_reviewer.json\"),\n    format=\"json\",\n    include_metrics=True\n)\n\n# Export all prompts\nversioner.export_all(\n    output_dir=Path(\"backups/\"),\n    format=\"yaml\"\n)\n</code></pre>"},{"location":"api-reference/core/versioner/#annotations-and-metadata","title":"Annotations and Metadata","text":"<pre><code># Add annotation to a version\nversioner.add_annotation(\n    name=\"code_reviewer\",\n    version=\"1.1.0\",\n    text=\"This version performs significantly better on Python code\",\n    author=\"team@company.com\"\n)\n\n# Get annotations\nannotations = versioner.get_annotations(\"code_reviewer\", \"1.1.0\")\nfor annotation in annotations:\n    print(f\"{annotation['author']}: {annotation['text']}\")\n\n# Delete version if needed\nsuccess = versioner.delete_version(\"code_reviewer\", \"1.0.0\")\nif success:\n    print(\"Version deleted successfully\")\n\n# Delete entire prompt and all versions\nsuccess = versioner.delete_prompt(\"old_prompt\")\nif success:\n    print(\"Prompt and all versions deleted\")\n</code></pre>"},{"location":"api-reference/core/versioner/#integration-ab-testing","title":"Integration A/B Testing","text":"<pre><code>from prompt_versioner.testing import ABTest\n\n# Create A/B test comparing two versions\nab_test = ABTest(\n    versioner=versioner,\n    prompt_name=\"code_reviewer\",\n    version_a=\"1.0.0\",\n    version_b=\"1.1.0\",\n    metric_name=\"quality_score\"\n)\n\n# Log test results\nfor i in range(50):\n    # Simulate A/B test with random quality scores\n    quality_a = random.uniform(0.7, 0.85)\n    quality_b = random.uniform(0.75, 0.90)\n\n    ab_test.log_result(\"a\", quality_a)\n    ab_test.log_result(\"b\", quality_b)\n\n# Get results\nif ab_test.is_ready(min_samples=20):\n    result = ab_test.get_result()\n    print(f\"Winner: {result.winner}\")\n    print(f\"Improvement: {result.improvement:.2f}%\")\n    ab_test.print_result()\n</code></pre>"},{"location":"api-reference/core/versioner/#testing-integration","title":"Testing Integration","text":"<pre><code># Test a specific version\nwith versioner.test_version(\"code_reviewer\", \"1.1.0\") as test_context:\n    # Any metrics logged here will be tagged as test metrics\n    versioner.log_metrics(\n        name=\"code_reviewer\",\n        version=\"1.1.0\",\n        model_name=\"gpt-4o\",\n        quality_score=0.92,\n        metadata={\"test_case\": \"unit_test_1\"}\n    )\n</code></pre>"},{"location":"api-reference/core/versioner/#git-integration_1","title":"Git Integration","text":"<pre><code># Enable Git hooks for automatic versioning\nversioner.install_git_hooks()\n\n# Track decorator automatically creates versions on Git commits\n@versioner.track(\"code_reviewer\", auto_commit=True)\ndef review_code(code):\n    # This function will be automatically tracked\n    return f\"Review of: {code}\"\n\n# Uninstall hooks when done\nversioner.uninstall_git_hooks()\n</code></pre>"},{"location":"api-reference/core/versioner/#see-also","title":"See Also","text":"<ul> <li>Version Manager - Detailed version management</li> <li>Metrics Tracker - Metrics collection and analysis</li> <li>A/B Testing - Experimental testing framework</li> <li>Configuration - Setup and configuration guide</li> </ul>"},{"location":"api-reference/metrics/aggregator/","title":"Aggregator","text":"<p>The <code>prompt_versioner.metrics.aggregator</code> module provides functionality to aggregate metrics across multiple test runs.</p>"},{"location":"api-reference/metrics/aggregator/#metricaggregator","title":"MetricAggregator","text":"<p>Class for collecting and aggregating metrics from multiple LLM calls, providing summary statistics.</p>"},{"location":"api-reference/metrics/aggregator/#constructor","title":"Constructor","text":"<pre><code>def __init__(self) -&gt; None\n</code></pre> <p>Initializes an empty aggregator.</p>"},{"location":"api-reference/metrics/aggregator/#add-methods","title":"Add Methods","text":""},{"location":"api-reference/metrics/aggregator/#add","title":"add()","text":"<pre><code>def add(self, metric: ModelMetrics) -&gt; None\n</code></pre> <p>Adds a single metric to the aggregator.</p> <p>Parameters: - <code>metric</code> (ModelMetrics): ModelMetrics object to add</p> <p>Example: <pre><code>from prompt_versioner.metrics.aggregator import MetricAggregator\nfrom prompt_versioner.metrics.models import ModelMetrics\n\naggregator = MetricAggregator()\n\n# Add metrics one at a time\nmetric1 = ModelMetrics(\n    model_name=\"gpt-4\",\n    input_tokens=100,\n    output_tokens=50,\n    cost_eur=0.003,\n    latency_ms=1200,\n    quality_score=0.92\n)\n\naggregator.add(metric1)\n</code></pre></p>"},{"location":"api-reference/metrics/aggregator/#add_dict","title":"add_dict()","text":"<pre><code>def add_dict(self, **kwargs: Any) -&gt; None\n</code></pre> <p>Adds metrics from keyword arguments.</p> <p>Parameters: - <code>**kwargs</code>: Metric fields as keyword arguments</p> <p>Example: <pre><code># Add metrics directly from a dictionary\naggregator.add_dict(\n    model_name=\"gpt-3.5-turbo\",\n    input_tokens=150,\n    output_tokens=75,\n    cost_eur=0.001,\n    latency_ms=800,\n    quality_score=0.85,\n    success=True\n)\n</code></pre></p>"},{"location":"api-reference/metrics/aggregator/#add_batch","title":"add_batch()","text":"<pre><code>def add_batch(self, metrics: List[ModelMetrics]) -&gt; None\n</code></pre> <p>Adds multiple metrics at once.</p> <p>Parameters: - <code>metrics</code> (List[ModelMetrics]): List of ModelMetrics objects</p> <p>Example: <pre><code># Batch of metrics from a test\ntest_metrics = [\n    ModelMetrics(model_name=\"gpt-4\", quality_score=0.90, cost_eur=0.003),\n    ModelMetrics(model_name=\"gpt-4\", quality_score=0.88, cost_eur=0.0035),\n    ModelMetrics(model_name=\"gpt-4\", quality_score=0.92, cost_eur=0.0028)\n]\n\naggregator.add_batch(test_metrics)\n</code></pre></p>"},{"location":"api-reference/metrics/aggregator/#analysis-methods","title":"Analysis Methods","text":""},{"location":"api-reference/metrics/aggregator/#get_summary","title":"get_summary()","text":"<pre><code>def get_summary(self) -&gt; Dict[str, Any]\n</code></pre> <p>Gets a statistical summary of all aggregated metrics.</p> <p>Returns: - <code>Dict[str, Any]</code>: Dictionary with aggregate statistics</p> <p>Included statistics: - Counts: <code>call_count</code>, <code>success_count</code>, <code>failure_count</code>, <code>success_rate</code> - Tokens: <code>total_tokens</code>, <code>avg_input_tokens</code>, <code>avg_output_tokens</code>, <code>avg_total_tokens</code> - Costs: <code>total_cost</code>, <code>avg_cost</code>, <code>min_cost</code>, <code>max_cost</code> - Latency: <code>avg_latency</code>, <code>min_latency</code>, <code>max_latency</code>, <code>median_latency</code> - Quality: <code>avg_quality</code>, <code>min_quality</code>, <code>max_quality</code> - Accuracy: <code>avg_accuracy</code> - Models: <code>models_used</code>, <code>primary_model</code></p> <p>Example: <pre><code># After adding many metrics\nsummary = aggregator.get_summary()\n\nprint(f\"Total calls: {summary['call_count']}\")\nprint(f\"Success rate: {summary['success_rate']:.2%}\")\nprint(f\"Average cost: \u20ac{summary['avg_cost']:.4f}\")\nprint(f\"Average latency: {summary['avg_latency']:.1f}ms\")\nprint(f\"Average quality: {summary['avg_quality']:.2%}\")\nprint(f\"Models used: {summary['models_used']}\")\nprint(f\"Primary model: {summary['primary_model']}\")\n</code></pre></p>"},{"location":"api-reference/metrics/aggregator/#get_summary_by_model","title":"get_summary_by_model()","text":"<pre><code>def get_summary_by_model(self) -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Gets statistics grouped by model.</p> <p>Returns: - <code>Dict[str, Dict[str, Any]]</code>: Dictionary model -&gt; statistics</p> <p>Example: <pre><code># Analysis by model\nby_model = aggregator.get_summary_by_model()\n\nfor model_name, stats in by_model.items():\n    print(f\"\\n{model_name}:\")\n    print(f\"  Calls: {stats['call_count']}\")\n    print(f\"  Average quality: {stats['avg_quality']:.2%}\")\n    print(f\"  Average cost: \u20ac{stats['avg_cost']:.4f}\")\n    print(f\"  Average latency: {stats['avg_latency']:.1f}ms\")\n    print(f\"  Success rate: {stats['success_rate']:.2%}\")\n</code></pre></p>"},{"location":"api-reference/metrics/aggregator/#filter-methods","title":"Filter Methods","text":""},{"location":"api-reference/metrics/aggregator/#get_failures","title":"get_failures()","text":"<pre><code>def get_failures(self) -&gt; List[ModelMetrics]\n</code></pre> <p>Gets all metrics that failed.</p> <p>Returns: - <code>List[ModelMetrics]</code>: List of metrics with success=False</p> <p>Example: <pre><code>failures = aggregator.get_failures()\nif failures:\n    print(f\"\u274c {len(failures)} failed calls:\")\n    for failure in failures:\n        print(f\"  - {failure.model_name}: {failure.error_message}\")\n</code></pre></p>"},{"location":"api-reference/metrics/aggregator/#filter_by_model","title":"filter_by_model()","text":"<pre><code>def filter_by_model(self, model_name: str) -&gt; List[ModelMetrics]\n</code></pre> <p>Filters metrics by model name.</p> <p>Parameters: - <code>model_name</code> (str): Name of the model to filter</p> <p>Returns: - <code>List[ModelMetrics]</code>: List of metrics for the specified model</p> <p>Example: <pre><code>gpt4_metrics = aggregator.filter_by_model(\"gpt-4\")\nprint(f\"GPT-4 metrics: {len(gpt4_metrics)}\")\n\n# Specific analysis for GPT-4\ncosts = [m.cost_eur for m in gpt4_metrics if m.cost_eur]\navg_cost = sum(costs) / len(costs) if costs else 0\nprint(f\"Average GPT-4 cost: \u20ac{avg_cost:.4f}\")\n</code></pre></p>"},{"location":"api-reference/metrics/aggregator/#utility-methods","title":"Utility Methods","text":""},{"location":"api-reference/metrics/aggregator/#clear","title":"clear()","text":"<p>Clears all aggregated metrics.</p>"},{"location":"api-reference/metrics/aggregator/#to_list","title":"to_list()","text":"<p>Exports metrics as a list of dictionaries.</p>"},{"location":"api-reference/metrics/aggregator/#len-int","title":"len() -&gt; int","text":"<p>Returns the number of aggregated metrics.</p>"},{"location":"api-reference/metrics/aggregator/#iter","title":"iter()","text":"<p>Allows iteration over metrics.</p> <p>Example: <pre><code>print(f\"Number of metrics: {len(aggregator)}\")\n\n# Iterate over all metrics\nfor metric in aggregator:\n    if metric.cost_eur and metric.cost_eur &gt; 0.01:\n        print(f\"Expensive call: \u20ac{metric.cost_eur}\")\n\n# Export as list\nexported = aggregator.to_list()\n</code></pre></p>"},{"location":"api-reference/metrics/aggregator/#statistical-methods","title":"Statistical Methods","text":""},{"location":"api-reference/metrics/aggregator/#implemented-statistical-calculations","title":"Implemented Statistical Calculations","text":"<p>Average (<code>_avg</code>): <pre><code>valid_values = [v for v in values if v is not None]\nreturn sum(valid_values) / len(valid_values) if valid_values else 0.0\n</code></pre></p> <p>Median (<code>_median</code>): <pre><code>valid_values = sorted([v for v in values if v is not None])\nn = len(valid_values)\nif n % 2 == 0:\n    return (valid_values[n // 2 - 1] + valid_values[n // 2]) / 2\nelse:\n    return valid_values[n // 2]\n</code></pre></p> <p>Most common value (<code>_most_common</code>): <pre><code>from collections import Counter\ncounter = Counter(values)\nmost_common = counter.most_common(1)\nreturn most_common[0][0] if most_common else None\n</code></pre></p>"},{"location":"api-reference/metrics/aggregator/#see-also","title":"See Also","text":"<ul> <li><code>Calculator</code> - Utility for single-call metric calculations</li> <li><code>Analyzer</code> - Functionality for analyzing and comparing metrics between versions</li> <li><code>Models</code> - Data models for metrics and comparison structures</li> <li><code>Pricing</code> - Manages model pricing and calculates LLM call costs</li> <li><code>Tracker</code> - Functionality for tracking and statistical analysis of metrics</li> </ul>"},{"location":"api-reference/metrics/analyzer/","title":"Analyzer","text":"<p>The <code>prompt_versioner.metrics.analyzer</code> module provides functionality for analyzing and comparing metrics between versions.</p>"},{"location":"api-reference/metrics/analyzer/#metricsanalyzer","title":"MetricsAnalyzer","text":"<p>Static class for analyzing and comparing metrics between different versions.</p>"},{"location":"api-reference/metrics/analyzer/#methods","title":"Methods","text":""},{"location":"api-reference/metrics/analyzer/#compare_metrics","title":"compare_metrics()","text":"<pre><code>@staticmethod\n    def compare_metrics(\n        baseline_metrics: Dict[str, List[float]],\n        new_metrics: Dict[str, List[float]],\n    ) -&gt; List[MetricComparison]\n</code></pre> <p>Compares metrics between two versions.</p> <p>Parameters: - <code>baseline_metrics</code> (Dict[str, List[float]]): Baseline version metrics - <code>new_metrics</code> (Dict[str, List[float]]): New version metrics</p> <p>Returns: - <code>List[MetricComparison]</code>: List of MetricComparison objects</p> <p>Example: <pre><code>from prompt_versioner.metrics.analyzer import MetricsAnalyzer\n\nbaseline = {\n    \"latency_ms\": [100, 110, 95, 105],\n    \"cost_eur\": [0.001, 0.0012, 0.0011, 0.0013]\n}\n\nnew = {\n    \"latency_ms\": [85, 90, 80, 88],\n    \"cost_eur\": [0.0008, 0.0009, 0.0007, 0.0010]\n}\n\ncomparisons = MetricsAnalyzer.compare_metrics(baseline, new)\nfor comp in comparisons:\n    status = \"improved\" if comp.improved else \"regressed\"\n    print(f\"{comp.metric_name}: {comp.mean_pct_change:.1f}% ({status})\")\n</code></pre></p>"},{"location":"api-reference/metrics/analyzer/#format_comparison","title":"format_comparison()","text":"<pre><code>@staticmethod\n    def format_comparison(comparisons: List[MetricComparison]) -&gt; str\n</code></pre> <p>Formats metric comparisons as readable text.</p> <p>Parameters: - <code>comparisons</code> (List[MetricComparison]): List of MetricComparison objects</p> <p>Returns: - <code>str</code>: Formatted string</p> <p>Example: <pre><code>formatted = MetricsAnalyzer.format_comparison(comparisons)\nprint(formatted)\n# Output:\n# ================================================================================\n# METRICS COMPARISON\n# ================================================================================\n#\n# LATENCY_MS:\n#   Baseline: 102.5000 (\u00b16.4550)\n#   New:      85.7500 (\u00b14.3507)\n#   Change:   \u2191 16.7500 (-16.34%) \u2713 IMPROVED\n</code></pre></p>"},{"location":"api-reference/metrics/analyzer/#detect_regressions","title":"detect_regressions()","text":"<pre><code>@staticmethod\n    def detect_regressions(\n        comparisons: List[MetricComparison],\n        threshold: float = 0.05,\n    ) -&gt; List[MetricComparison]\n</code></pre> <p>Detects regressions in metrics.</p> <p>Parameters: - <code>comparisons</code> (List[MetricComparison]): List of MetricComparison objects - <code>threshold</code> (float): Relative threshold for regression (default: 0.05 = 5%)</p> <p>Returns: - <code>List[MetricComparison]</code>: List of regressed metrics</p> <p>Example: <pre><code>regressions = MetricsAnalyzer.detect_regressions(comparisons, threshold=0.10)\nif regressions:\n    print(\"Regressions detected:\")\n    for reg in regressions:\n        print(f\"- {reg.metric_name}: {reg.mean_pct_change:.1f}%\")\n</code></pre></p>"},{"location":"api-reference/metrics/analyzer/#get_best_version","title":"get_best_version()","text":"<pre><code>@staticmethod\n    def get_best_version(\n        versions_metrics: Dict[str, Dict[str, List[float]]],\n        metric_name: str,\n        higher_is_better: bool = True,\n    ) -&gt; tuple[str, float]\n</code></pre> <p>Finds the best version for a specific metric.</p> <p>Parameters: - <code>versions_metrics</code> (Dict[str, Dict[str, List[float]]]): Dict version -&gt; metrics - <code>metric_name</code> (str): Name of the metric to compare - <code>higher_is_better</code> (bool): Whether higher values are better (default: True)</p> <p>Returns: - <code>tuple[str, float]</code>: Tuple (best_version_name, best_value)</p> <p>Example: <pre><code>versions = {\n    \"v1.0.0\": {\"accuracy\": [0.85, 0.87, 0.86]},\n    \"v1.1.0\": {\"accuracy\": [0.90, 0.92, 0.91]},\n    \"v1.2.0\": {\"accuracy\": [0.88, 0.89, 0.87]}\n}\n\nbest_version, best_score = MetricsAnalyzer.get_best_version(\n    versions, \"accuracy\", higher_is_better=True\n)\nprint(f\"Best version: {best_version} (accuracy: {best_score:.3f})\")\n</code></pre></p>"},{"location":"api-reference/metrics/analyzer/#rank_versions","title":"rank_versions()","text":"<pre><code>@staticmethod\n    def rank_versions(\n        versions_metrics: Dict[str, Dict[str, List[float]]],\n        metric_name: str,\n        higher_is_better: bool = True,\n    ) -&gt; List[tuple[str, float]]\n</code></pre> <p>Ranks all versions for a specific metric.</p> <p>Parameters: - <code>versions_metrics</code> (Dict[str, Dict[str, List[float]]]): Dict version -&gt; metrics - <code>metric_name</code> (str): Name of the metric for ranking - <code>higher_is_better</code> (bool): Whether higher values are better (default: True)</p> <p>Returns: - <code>List[tuple[str, float]]</code>: List of tuples (version_name, mean_value) ordered by ranking</p> <p>Example: <pre><code>rankings = MetricsAnalyzer.rank_versions(versions, \"accuracy\")\nprint(\"Ranking by accuracy:\")\nfor i, (version, score) in enumerate(rankings, 1):\n    print(f\"{i}. {version}: {score:.3f}\")\n</code></pre></p>"},{"location":"api-reference/metrics/analyzer/#calculate_improvement_score","title":"calculate_improvement_score()","text":"<pre><code>@staticmethod\n    def calculate_improvement_score(\n        comparisons: List[MetricComparison], weights: Dict[str, float] | None = None\n    ) -&gt; float\n</code></pre> <p>Calculates an overall improvement score from comparisons.</p> <p>Parameters: - <code>comparisons</code> (List[MetricComparison]): List of MetricComparison objects - <code>weights</code> (Dict[str, float] | None): Optional weights for each metric (default: equal weights)</p> <p>Returns: - <code>float</code>: Overall improvement score (from -100 to +100)</p> <p>Example: <pre><code># Custom weights to give more importance to latency\nweights = {\n    \"latency_ms\": 2.0,\n    \"cost_eur\": 1.0,\n    \"accuracy\": 1.5\n}\n\nimprovement_score = MetricsAnalyzer.calculate_improvement_score(comparisons, weights)\nprint(f\"Improvement score: {improvement_score:.1f}\")\n\nif improvement_score &gt; 0:\n    print(\"\u2713 Overall improvement\")\nelif improvement_score &lt; -5:\n    print(\"\u2717 Significant regression\")\nelse:\n    print(\"\u2248 Stable performance\")\n</code></pre></p>"},{"location":"api-reference/metrics/analyzer/#improvement-logic","title":"Improvement Logic","text":"<p>The analyzer automatically determines if a metric is improved based on its type:</p> <ul> <li>HIGHER_IS_BETTER: accuracy, throughput, success_rate, etc.</li> <li>LOWER_IS_BETTER: latency_ms, cost_eur, error_rate, etc.</li> </ul> <p>The mapping is defined in <code>MetricType</code> and <code>METRIC_DIRECTIONS</code> in the metrics models.</p>"},{"location":"api-reference/metrics/analyzer/#ranking-algorithms","title":"Ranking Algorithms","text":""},{"location":"api-reference/metrics/analyzer/#improvement-score","title":"Improvement Score","text":"<p>The improvement score is calculated as a weighted average of percent changes:</p> <pre><code>score = \u03a3(weight_i * pct_change_i) / \u03a3(weight_i)\n</code></pre> <p>Where: - <code>pct_change_i</code> is the percent change for metric i - <code>weight_i</code> is the weight assigned to metric i - The result is limited between -100 and +100</p>"},{"location":"api-reference/metrics/analyzer/#see-also","title":"See Also","text":"<ul> <li><code>Calculator</code> - Utility for single-call metric calculations</li> <li><code>Aggregator</code> - Functionality to aggregate metrics across multiple test runs</li> <li><code>Models</code> - Data models for metrics and comparison structures</li> <li><code>Pricing</code> - Manages model pricing and calculates LLM call costs</li> <li><code>Tracker</code> - Functionality for tracking and statistical analysis of metrics</li> </ul>"},{"location":"api-reference/metrics/calculator/","title":"Calculator","text":"<p>The <code>prompt_versioner.metrics.calculator</code> module provides utilities for calculating various metrics and derived scores for LLM calls.</p>"},{"location":"api-reference/metrics/calculator/#metricscalculator","title":"MetricsCalculator","text":"<p>Class for calculating additional metrics and enriching LLM call data.</p>"},{"location":"api-reference/metrics/calculator/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, pricing_manager: Optional[PricingManager] = None)\n</code></pre> <p>Parameters: - <code>pricing_manager</code> (Optional[PricingManager]): Custom pricing manager (default: default PricingManager)</p>"},{"location":"api-reference/metrics/calculator/#methods","title":"Methods","text":""},{"location":"api-reference/metrics/calculator/#calculate_cost","title":"calculate_cost()","text":"<pre><code>def calculate_cost(self, model_name: str, input_tokens: int, output_tokens: int) -&gt; float\n</code></pre> <p>Calculates the cost in EUR for a model call.</p> <p>Parameters: - <code>model_name</code> (str): Model name - <code>input_tokens</code> (int): Number of input tokens - <code>output_tokens</code> (int): Number of output tokens</p> <p>Returns: - <code>float</code>: Cost in EUR</p> <p>Example: <pre><code>from prompt_versioner.metrics.calculator import MetricsCalculator\n\ncalculator = MetricsCalculator()\n\n# Calculate cost for GPT-4\ncost = calculator.calculate_cost(\"gpt-4\", input_tokens=100, output_tokens=50)\nprint(f\"Cost: \u20ac{cost:.4f}\")\n\n# Calculate cost for different models\nmodels_test = [\n    (\"gpt-3.5-turbo\", 200, 100),\n    (\"gpt-4\", 200, 100),\n    (\"claude-3-haiku\", 200, 100)\n]\n\nfor model, input_tokens, output_tokens in models_test:\n    cost = calculator.calculate_cost(model, input_tokens, output_tokens)\n    print(f\"{model}: \u20ac{cost:.4f} per {input_tokens + output_tokens} tokens\")\n</code></pre></p>"},{"location":"api-reference/metrics/calculator/#enrich_metrics","title":"enrich_metrics()","text":"<pre><code>def enrich_metrics(self, metrics: ModelMetrics) -&gt; ModelMetrics\n</code></pre> <p>Enriches metrics with automatically calculated values.</p> <p>Features: - Calculates <code>total_tokens</code> if missing - Calculates <code>cost_eur</code> if missing - Adds derived metrics to metadata:   - <code>cost_per_token</code>: Cost per token   - <code>tokens_per_second</code>: Throughput in tokens/second   - <code>cost_per_second</code>: Cost per second</p> <p>Parameters: - <code>metrics</code> (ModelMetrics): ModelMetrics object to enrich</p> <p>Returns: - <code>ModelMetrics</code>: Enriched object (modifies the original object)</p> <p>Example: <pre><code>from prompt_versioner.metrics.models import ModelMetrics\nfrom prompt_versioner.metrics.calculator import MetricsCalculator\n\n# Create base metrics\nmetrics = ModelMetrics(\n    model_name=\"gpt-4\",\n    input_tokens=150,\n    output_tokens=75,\n    latency_ms=1200,\n    quality_score=0.92\n    # cost_eur not specified - will be calculated\n    # total_tokens not specified - will be calculated\n)\n\ncalculator = MetricsCalculator()\nenriched = calculator.enrich_metrics(metrics)\n\nprint(f\"Total tokens: {enriched.total_tokens}\")  # 225\nprint(f\"Cost: \u20ac{enriched.cost_eur:.4f}\")  # Automatically calculated\nprint(f\"Cost per token: \u20ac{enriched.metadata['cost_per_token']:.6f}\")\nprint(f\"Tokens/sec: {enriched.metadata['tokens_per_second']:.1f}\")\nprint(f\"Cost/sec: \u20ac{enriched.metadata['cost_per_second']:.4f}\")\n</code></pre></p>"},{"location":"api-reference/metrics/calculator/#calculate_efficiency_score","title":"calculate_efficiency_score()","text":"<pre><code>def calculate_efficiency_score(self, metrics: ModelMetrics) -&gt; float\n</code></pre> <p>Calculates an efficiency score (quality per cost).</p> <p>Formula: <pre><code>efficiency = (quality_score / cost_eur) * normalization\n</code></pre></p> <p>Parameters: - <code>metrics</code> (ModelMetrics): Object with quality_score and cost_eur</p> <p>Returns: - <code>float</code>: Efficiency score (0-100)</p> <p>Example: <pre><code># Compare efficiency of different versions\nversions = [\n    ModelMetrics(quality_score=0.85, cost_eur=0.002),  # Baseline\n    ModelMetrics(quality_score=0.90, cost_eur=0.003),  # Higher quality, more expensive\n    ModelMetrics(quality_score=0.82, cost_eur=0.001),  # Lower quality, cheaper\n]\n\nfor i, metrics in enumerate(versions):\n    efficiency = calculator.calculate_efficiency_score(metrics)\n    print(f\"Version {i+1}: Efficiency = {efficiency:.1f}\")\n    print(f\"  Quality: {metrics.quality_score:.2%}\")\n    print(f\"  Cost: \u20ac{metrics.cost_eur:.4f}\")\n</code></pre></p>"},{"location":"api-reference/metrics/calculator/#calculate_value_score","title":"calculate_value_score()","text":"<pre><code>def calculate_value_score(\n        self,\n        metrics: ModelMetrics,\n        quality_weight: float = 0.5,\n        cost_weight: float = 0.3,\n        latency_weight: float = 0.2,\n    ) -&gt; float\n</code></pre> <p>Calculates an overall value score combining quality, cost, and latency.</p> <p>Formula: <pre><code>value_score = (quality * quality_weight +\n               cost_score * cost_weight +\n               latency_score * latency_weight) / total_weight\n</code></pre></p> <p>Parameters: - <code>metrics</code> (ModelMetrics): Object with metrics to evaluate - <code>quality_weight</code> (float): Weight for quality (default: 0.5) - <code>cost_weight</code> (float): Weight for cost (default: 0.3) - <code>latency_weight</code> (float): Weight for latency (default: 0.2)</p> <p>Returns: - <code>float</code>: Value score (0-100)</p> <p>Example: <pre><code># Multi-dimensional analysis\nmetrics = ModelMetrics(\n    quality_score=0.88,\n    cost_eur=0.0025,\n    latency_ms=850\n)\n\n# Different weighting profiles\nprofiles = {\n    \"balanced\": {\"quality_weight\": 0.5, \"cost_weight\": 0.3, \"latency_weight\": 0.2},\n    \"quality_focused\": {\"quality_weight\": 0.7, \"cost_weight\": 0.2, \"latency_weight\": 0.1},\n    \"cost_focused\": {\"quality_weight\": 0.3, \"cost_weight\": 0.5, \"latency_weight\": 0.2},\n    \"speed_focused\": {\"quality_weight\": 0.3, \"cost_weight\": 0.2, \"latency_weight\": 0.5}\n}\n\nfor profile_name, weights in profiles.items():\n    score = calculator.calculate_value_score(metrics, **weights)\n    print(f\"{profile_name.title()}: {score:.1f}/100\")\n</code></pre></p>"},{"location":"api-reference/metrics/calculator/#see-also","title":"See Also","text":"<ul> <li><code>Aggregator</code> - Functionality to aggregate metrics across multiple test runs</li> <li><code>Analyzer</code> - Functionality for analyzing and comparing metrics between versions</li> <li><code>Models</code> - Data models for metrics and comparison structures</li> <li><code>Pricing</code> - Manages model pricing and calculates LLM call costs</li> <li><code>Tracker</code> - Functionality for tracking and statistical analysis of metrics</li> </ul>"},{"location":"api-reference/metrics/models/","title":"Models","text":"<p>The <code>prompt_versioner.metrics.models</code> module defines data models for metrics and comparison structures.</p>"},{"location":"api-reference/metrics/models/#modelmetrics","title":"ModelMetrics","text":"<p>Dataclass representing metrics for a single LLM call.</p>"},{"location":"api-reference/metrics/models/#attributes","title":"Attributes","text":""},{"location":"api-reference/metrics/models/#model-information","title":"Model Information","text":"<ul> <li><code>model_name</code> (Optional[str]): Name of the model used</li> </ul>"},{"location":"api-reference/metrics/models/#token-usage","title":"Token Usage","text":"<ul> <li><code>input_tokens</code> (Optional[int]): Number of input tokens</li> <li><code>output_tokens</code> (Optional[int]): Number of output tokens</li> <li><code>total_tokens</code> (Optional[int]): Total number of tokens</li> </ul>"},{"location":"api-reference/metrics/models/#costs","title":"Costs","text":"<ul> <li><code>cost_eur</code> (Optional[float]): Cost in EUR</li> </ul>"},{"location":"api-reference/metrics/models/#performance","title":"Performance","text":"<ul> <li><code>latency_ms</code> (Optional[float]): Latency in milliseconds</li> </ul>"},{"location":"api-reference/metrics/models/#quality-metrics","title":"Quality Metrics","text":"<ul> <li><code>quality_score</code> (Optional[float]): Quality score</li> <li><code>accuracy</code> (Optional[float]): Accuracy</li> </ul>"},{"location":"api-reference/metrics/models/#model-parameters","title":"Model Parameters","text":"<ul> <li><code>temperature</code> (Optional[float]): Temperature used</li> <li><code>max_tokens</code> (Optional[int]): Maximum number of tokens</li> <li><code>top_p</code> (Optional[float]): Top_p value</li> </ul>"},{"location":"api-reference/metrics/models/#status","title":"Status","text":"<ul> <li><code>success</code> (bool): Whether the call was successful (default: True)</li> <li><code>error_message</code> (Optional[str]): Error message if present</li> </ul>"},{"location":"api-reference/metrics/models/#additional-data","title":"Additional Data","text":"<ul> <li><code>metadata</code> (Optional[Dict[str, Any]]): Additional metadata</li> </ul>"},{"location":"api-reference/metrics/models/#methods","title":"Methods","text":""},{"location":"api-reference/metrics/models/#to_dict","title":"to_dict()","text":"<p><pre><code>def to_dict(self) -&gt; Dict[str, Any]\n</code></pre> Converts the object to a dictionary.</p> <p>Returns: - <code>Dict[str, Any]</code>: Dictionary representation</p>"},{"location":"api-reference/metrics/models/#from_dict","title":"from_dict()","text":"<pre><code>@classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"ModelMetrics\"\n</code></pre> <p>Creates an instance from a dictionary.</p> <p>Parameters: - <code>data</code> (Dict[str, Any]): Data in dictionary format</p> <p>Returns: - <code>ModelMetrics</code>: New instance</p> <p>Example: <pre><code>from prompt_versioner.metrics.models import ModelMetrics\n\n# Manual creation\nmetrics = ModelMetrics(\n    model_name=\"gpt-4\",\n    input_tokens=100,\n    output_tokens=50,\n    total_tokens=150,\n    cost_eur=0.003,\n    latency_ms=1200,\n    quality_score=0.95,\n    temperature=0.7,\n    success=True\n)\n\n# Convert to dictionary\ndata = metrics.to_dict()\nprint(f\"Cost: \u20ac{data['cost_eur']}\")\n\n# Create from dictionary\nmetrics_copy = ModelMetrics.from_dict(data)\n</code></pre></p>"},{"location":"api-reference/metrics/models/#metricstats","title":"MetricStats","text":"<p>Dataclass representing a statistical summary of a metric.</p>"},{"location":"api-reference/metrics/models/#attributes_1","title":"Attributes","text":"<ul> <li><code>name</code> (str): Name of the metric</li> <li><code>count</code> (int): Number of values</li> <li><code>mean</code> (float): Mean</li> <li><code>median</code> (float): Median</li> <li><code>std_dev</code> (float): Standard deviation</li> <li><code>min_val</code> (float): Minimum value</li> <li><code>max_val</code> (float): Maximum value</li> </ul>"},{"location":"api-reference/metrics/models/#methods_1","title":"Methods","text":""},{"location":"api-reference/metrics/models/#to_dict_1","title":"to_dict()","text":"<pre><code>def to_dict(self) -&gt; Dict[str, Any]\n</code></pre> <p>Converts to dictionary.</p>"},{"location":"api-reference/metrics/models/#format-str","title":"format() -&gt; str","text":"<pre><code>def format(self) -&gt; str\n</code></pre> <p>Formats as a readable string.</p> <p>Example: <pre><code>from prompt_versioner.metrics.models import MetricStats\n\nstats = MetricStats(\n    name=\"latency_ms\",\n    count=100,\n    mean=150.5,\n    median=145.0,\n    std_dev=25.3,\n    min_val=95.0,\n    max_val=250.0\n)\n\nprint(stats.format())\n# Output: latency_ms: mean=150.5000, median=145.0000, std=25.3000, range=[95.0000, 250.0000], n=100\n</code></pre></p>"},{"location":"api-reference/metrics/models/#metriccomparison","title":"MetricComparison","text":"<p>Dataclass representing a comparison between two sets of metrics.</p>"},{"location":"api-reference/metrics/models/#attributes_2","title":"Attributes","text":"<ul> <li><code>metric_name</code> (str): Name of the metric</li> <li><code>baseline_mean</code> (float): Baseline mean</li> <li><code>new_mean</code> (float): New version mean</li> <li><code>mean_diff</code> (float): Difference between means</li> <li><code>mean_pct_change</code> (float): Percent change</li> <li><code>improved</code> (bool): Whether the metric improved</li> <li><code>baseline_stats</code> (Dict[str, float]): Baseline statistics</li> <li><code>new_stats</code> (Dict[str, float]): New version statistics</li> </ul>"},{"location":"api-reference/metrics/models/#methods_2","title":"Methods","text":""},{"location":"api-reference/metrics/models/#format","title":"format()","text":"<pre><code>def format(self) -&gt; str\n</code></pre> <p>Formats the comparison as a readable string.</p> <p>Example: <pre><code>from prompt_versioner.metrics.models import MetricComparison\n\ncomparison = MetricComparison(\n    metric_name=\"latency_ms\",\n    baseline_mean=150.0,\n    new_mean=120.0,\n    mean_diff=-30.0,\n    mean_pct_change=-20.0,\n    improved=True,\n    baseline_stats={\"std_dev\": 15.0},\n    new_stats={\"std_dev\": 12.0}\n)\n\nprint(comparison.format())\n# Output: latency_ms: 150.0000 \u2192 120.0000 (\u2191 20.00%)\n</code></pre></p>"},{"location":"api-reference/metrics/models/#metrictype","title":"MetricType","text":"<p>Enum defining common metric types for LLM prompts.</p>"},{"location":"api-reference/metrics/models/#values","title":"Values","text":""},{"location":"api-reference/metrics/models/#token-metrics","title":"Token Metrics","text":"<ul> <li><code>INPUT_TOKENS = \"input_tokens\"</code></li> <li><code>OUTPUT_TOKENS = \"output_tokens\"</code></li> <li><code>TOTAL_TOKENS = \"total_tokens\"</code></li> </ul>"},{"location":"api-reference/metrics/models/#cost-metrics","title":"Cost Metrics","text":"<ul> <li><code>COST = \"cost_eur\"</code></li> <li><code>COST_PER_TOKEN = \"cost_per_token\"</code></li> </ul>"},{"location":"api-reference/metrics/models/#performance-metrics","title":"Performance Metrics","text":"<ul> <li><code>LATENCY = \"latency_ms\"</code></li> <li><code>THROUGHPUT = \"throughput\"</code></li> </ul>"},{"location":"api-reference/metrics/models/#quality-metrics_1","title":"Quality Metrics","text":"<ul> <li><code>QUALITY = \"quality_score\"</code></li> <li><code>ACCURACY = \"accuracy\"</code></li> <li><code>RELEVANCE = \"relevance\"</code></li> <li><code>COHERENCE = \"coherence\"</code></li> <li><code>FACTUALITY = \"factuality\"</code></li> <li><code>FLUENCY = \"fluency\"</code></li> </ul>"},{"location":"api-reference/metrics/models/#success-metrics","title":"Success Metrics","text":"<ul> <li><code>SUCCESS_RATE = \"success_rate\"</code></li> <li><code>ERROR_RATE = \"error_rate\"</code></li> </ul> <p>Example: <pre><code>from prompt_versioner.metrics.models import MetricType\n\n# Using constants\nprint(MetricType.LATENCY)  # \"latency_ms\"\nprint(MetricType.ACCURACY)  # \"accuracy\"\n\n# Check if a string is a valid type\nmetric_name = \"cost_eur\"\nif metric_name in [mt.value for mt in MetricType]:\n    print(f\"{metric_name} is a valid metric type\")\n</code></pre></p>"},{"location":"api-reference/metrics/models/#metricdirection","title":"MetricDirection","text":"<p>Enum defining the optimization direction for metrics.</p>"},{"location":"api-reference/metrics/models/#values_1","title":"Values","text":"<ul> <li><code>HIGHER_IS_BETTER = \"higher\"</code>: Higher values are better</li> <li><code>LOWER_IS_BETTER = \"lower\"</code>: Lower values are better</li> <li><code>NEUTRAL = \"neutral\"</code>: Neutral direction</li> </ul>"},{"location":"api-reference/metrics/models/#metric_directions","title":"METRIC_DIRECTIONS","text":"<p>Dictionary mapping metric types to their optimization direction.</p> <pre><code>METRIC_DIRECTIONS = {\n    MetricType.COST: MetricDirection.LOWER_IS_BETTER,\n    MetricType.COST_PER_TOKEN: MetricDirection.LOWER_IS_BETTER,\n    MetricType.LATENCY: MetricDirection.LOWER_IS_BETTER,\n    MetricType.ERROR_RATE: MetricDirection.LOWER_IS_BETTER,\n    MetricType.QUALITY: MetricDirection.HIGHER_IS_BETTER,\n    MetricType.ACCURACY: MetricDirection.HIGHER_IS_BETTER,\n    MetricType.RELEVANCE: MetricDirection.HIGHER_IS_BETTER,\n    MetricType.COHERENCE: MetricDirection.HIGHER_IS_BETTER,\n    MetricType.FACTUALITY: MetricDirection.HIGHER_IS_BETTER,\n    MetricType.FLUENCY: MetricDirection.HIGHER_IS_BETTER,\n    MetricType.THROUGHPUT: MetricDirection.HIGHER_IS_BETTER,\n    MetricType.SUCCESS_RATE: MetricDirection.HIGHER_IS_BETTER,\n}\n</code></pre> <p>Example: <pre><code>from prompt_versioner.metrics.models import MetricType, METRIC_DIRECTIONS, MetricDirection\n\n# Check optimization direction\ndirection = METRIC_DIRECTIONS.get(MetricType.LATENCY, MetricDirection.NEUTRAL)\nprint(f\"For latency, {direction.value} is better\")  # \"lower is better\"\n\nif direction == MetricDirection.LOWER_IS_BETTER:\n    print(\"We aim to reduce latency\")\n</code></pre></p>"},{"location":"api-reference/metrics/models/#metricthreshold","title":"MetricThreshold","text":"<p>Dataclass for configuring warning thresholds for a metric.</p>"},{"location":"api-reference/metrics/models/#attributes_3","title":"Attributes","text":"<ul> <li><code>metric_type</code> (MetricType): Type of metric</li> <li><code>warning_threshold</code> (float): Warning threshold</li> <li><code>critical_threshold</code> (float): Critical threshold</li> <li><code>direction</code> (MetricDirection): Optimization direction (default: HIGHER_IS_BETTER)</li> </ul>"},{"location":"api-reference/metrics/models/#methods_3","title":"Methods","text":""},{"location":"api-reference/metrics/models/#check","title":"check()","text":"<pre><code>def check(self, value: float) -&gt; str:\n</code></pre> <p>Checks if a value meets the thresholds.</p> <p>Parameters: - <code>value</code> (float): Value to check</p> <p>Returns: - <code>str</code>: 'ok', 'warning', or 'critical'</p> <p>Example: <pre><code>from prompt_versioner.metrics.models import MetricThreshold, MetricType, MetricDirection\n\n# Thresholds for latency (lower is better)\nlatency_threshold = MetricThreshold(\n    metric_type=MetricType.LATENCY,\n    warning_threshold=200.0,  # warning if &gt; 200ms\n    critical_threshold=500.0,  # critical if &gt; 500ms\n    direction=MetricDirection.LOWER_IS_BETTER\n)\n\n# Test values\nvalues = [150.0, 250.0, 600.0]\nfor val in values:\n    status = latency_threshold.check(val)\n    print(f\"Latency {val}ms: {status}\")\n\n# Thresholds for accuracy (higher is better)\naccuracy_threshold = MetricThreshold(\n    metric_type=MetricType.ACCURACY,\n    warning_threshold=0.8,   # warning if &lt; 0.8\n    critical_threshold=0.6,  # critical if &lt; 0.6\n    direction=MetricDirection.HIGHER_IS_BETTER\n)\n\naccuracy_values = [0.95, 0.75, 0.5]\nfor val in accuracy_values:\n    status = accuracy_threshold.check(val)\n    print(f\"Accuracy {val}: {status}\")\n</code></pre></p>"},{"location":"api-reference/metrics/models/#see-also","title":"See Also","text":"<ul> <li><code>Aggregator</code> - Functionality to aggregate metrics across multiple test runs</li> <li><code>Analyzer</code> - Functionality for analyzing and comparing metrics between versions</li> <li><code>Calculator</code> - Utility for single-call metric calculations</li> <li><code>Pricing</code> - Manages model pricing and calculates LLM call costs</li> <li><code>Tracker</code> - Functionality for tracking and statistical analysis of metrics</li> </ul>"},{"location":"api-reference/metrics/pricing/","title":"Pricing","text":"<p>The <code>prompt_versioner.metrics.pricing</code> module manages model pricing and calculates LLM call costs.</p>"},{"location":"api-reference/metrics/pricing/#modelpricing","title":"ModelPricing","text":"<p>Class representing pricing information for a specific model.</p>"},{"location":"api-reference/metrics/pricing/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, input_price: float, output_price: float, currency: str = \"EUR\")\n</code></pre> <p>Parameters: - <code>input_price</code> (float): Price per 1M input tokens - <code>output_price</code> (float): Price per 1M output tokens - <code>currency</code> (str): Currency code (default: \"EUR\")</p>"},{"location":"api-reference/metrics/pricing/#methods","title":"Methods","text":""},{"location":"api-reference/metrics/pricing/#calculate_cost","title":"calculate_cost()","text":"<pre><code>def calculate_cost(self, input_tokens: int, output_tokens: int) -&gt; float\n</code></pre> <p>Calculates the cost for a specific token usage.</p> <p>Parameters: - <code>input_tokens</code> (int): Number of input tokens - <code>output_tokens</code> (int): Number of output tokens</p> <p>Returns: - <code>float</code>: Total cost in the specified currency</p> <p>Example: <pre><code>from prompt_versioner.metrics.pricing import ModelPricing\n\n# Define pricing for GPT-4\ngpt4_pricing = ModelPricing(\n    input_price=0.92,   # \u20ac0.92 per 1M input tokens\n    output_price=3.68   # \u20ac3.68 per 1M output tokens\n)\n\n# Calculate cost for a call\ncost = gpt4_pricing.calculate_cost(input_tokens=1000, output_tokens=500)\nprint(f\"Cost: \u20ac{cost:.6f}\")  # \u20ac0.002760\n</code></pre></p>"},{"location":"api-reference/metrics/pricing/#to_dict","title":"to_dict()","text":"<pre><code>def to_dict(self) -&gt; Dict[str, float | str]\n</code></pre> <p>Converts the object to a dictionary.</p> <p>Returns: - <code>Dict[str, float | str]</code>: Dictionary representation</p>"},{"location":"api-reference/metrics/pricing/#pricingmanager","title":"PricingManager","text":"<p>Class for managing pricing of multiple models and calculating costs.</p>"},{"location":"api-reference/metrics/pricing/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(self, custom_pricing: Optional[Dict[str, Dict[str, float]]] = None)\n</code></pre> <p>Parameters: - <code>custom_pricing</code> (Optional[Dict]): Custom prices to override defaults</p>"},{"location":"api-reference/metrics/pricing/#default-prices","title":"Default Prices","text":"<p>The manager includes up-to-date prices for major LLM models:</p> <pre><code>DEFAULT_MODEL_PRICING = {\n    # Claude models (Anthropic)\n    \"claude-opus-4-1\": {\"input\": 13.80, \"output\": 69.00},\n    \"claude-sonnet-4-5\": {\"input\": 5.06, \"output\": 23.00},\n    \"claude-haiku-4\": {\"input\": 0.92, \"output\": 4.60},\n    # Mistral models\n    \"mistral-large-24-11\": {\"input\": 1.84, \"output\": 5.52},\n    \"mistral-medium-3\": {\"input\": 0.37, \"output\": 1.84},\n    \"mistral-small-3-1\": {\"input\": 0.09, \"output\": 0.28},\n    # OpenAI models\n    \"gpt-5\": {\"input\": 1.15, \"output\": 9.20},\n    \"gpt-4-1\": {\"input\": 0.92, \"output\": 3.68},\n    \"gpt-4o\": {\"input\": 1.15, \"output\": 4.60},\n    \"gpt-4o-mini\": {\"input\": 0.18, \"output\": 0.73},\n}\n</code></pre>"},{"location":"api-reference/metrics/pricing/#methods_1","title":"Methods","text":""},{"location":"api-reference/metrics/pricing/#get_pricing","title":"get_pricing()","text":"<pre><code>def get_pricing(self, model_name: str) -&gt; Optional[ModelPricing]\n</code></pre> <p>Gets pricing information for a model.</p> <p>Parameters: - <code>model_name</code> (str): Model name</p> <p>Returns: - <code>Optional[ModelPricing]</code>: ModelPricing object or None if not found</p>"},{"location":"api-reference/metrics/pricing/#add_model","title":"add_model()","text":"<pre><code>def add_model(self, model_name: str, input_price: float, output_price: float) -&gt; None\n</code></pre> <p>Adds or updates pricing for a model.</p> <p>Parameters: - <code>model_name</code> (str): Model name - <code>input_price</code> (float): Price per 1M input tokens - <code>output_price</code> (float): Price per 1M output tokens</p>"},{"location":"api-reference/metrics/pricing/#remove_model","title":"remove_model()","text":"<pre><code>def remove_model(self, model_name: str) -&gt; bool\n</code></pre> <p>Removes a model from pricing.</p> <p>Parameters: - <code>model_name</code> (str): Model name</p> <p>Returns: - <code>bool</code>: True if removed, False if not found</p>"},{"location":"api-reference/metrics/pricing/#list_models","title":"list_models()","text":"<pre><code>def list_models(self) -&gt; list[str]\n</code></pre> <p>Gets the list of all models with pricing.</p> <p>Returns: - <code>List[str]</code>: List of model names</p> <p>Example: <pre><code>from prompt_versioner.metrics.pricing import PricingManager\n\n# Create manager with custom pricing\ncustom_pricing = {\n    \"custom-model\": {\"input\": 0.5, \"output\": 1.0}\n}\nmanager = PricingManager(custom_pricing)\n\n# Add a new model\nmanager.add_model(\"my-model\", input_price=0.3, output_price=0.8)\n\n# List all models\nmodels = manager.list_models()\nprint(f\"Available models: {len(models)}\")\nfor model in sorted(models):\n    pricing = manager.get_pricing(model)\n    if pricing:\n        print(f\"  {model}: \u20ac{pricing.input_price}/\u20ac{pricing.output_price}\")\n</code></pre></p>"},{"location":"api-reference/metrics/pricing/#calculate_cost_1","title":"calculate_cost()","text":"<pre><code>def calculate_cost(self, model_name: str, input_tokens: int, output_tokens: int) -&gt; float\n</code></pre> <p>Calculates the cost for a model call.</p> <p>Parameters: - <code>model_name</code> (str): Model name - <code>input_tokens</code> (int): Number of input tokens - <code>output_tokens</code> (int): Number of output tokens</p> <p>Returns: - <code>float</code>: Cost in EUR (0.0 if model not found)</p> <p>Example: <pre><code>manager = PricingManager()\n\n# Calculate costs for different models\nmodels_to_test = [\"gpt-4o\", \"gpt-4o-mini\", \"claude-haiku-4\"]\ninput_tokens, output_tokens = 1000, 500\n\nprint(\"Cost comparison:\")\nfor model in models_to_test:\n    cost = manager.calculate_cost(model, input_tokens, output_tokens)\n    print(f\"  {model}: \u20ac{cost:.6f}\")\n</code></pre></p>"},{"location":"api-reference/metrics/pricing/#estimate_cost","title":"estimate_cost()","text":"<p><pre><code>def estimate_cost(\n        self, model_name: str, input_tokens: int, output_tokens: int, num_calls: int = 1\n    ) -&gt; Dict[str, float]\n</code></pre> Estimates costs for multiple calls.</p> <p>Parameters: - <code>model_name</code> (str): Model name - <code>input_tokens</code> (int): Input tokens per call - <code>output_tokens</code> (int): Output tokens per call - <code>num_calls</code> (int): Number of calls (default: 1)</p> <p>Returns: - <code>Dict[str, float]</code>: Dictionary with cost breakdown</p> <p>Example: <pre><code># Estimate costs for a batch of calls\nestimate = manager.estimate_cost(\n    model_name=\"gpt-4o\",\n    input_tokens=500,\n    output_tokens=200,\n    num_calls=1000\n)\n\nprint(f\"Estimate for 1000 calls:\")\nprint(f\"  Cost per call: \u20ac{estimate['cost_per_call']:.6f}\")\nprint(f\"  Total cost: \u20ac{estimate['total_cost']:.2f}\")\nprint(f\"  Total input tokens: {estimate['total_input_tokens']:,}\")\nprint(f\"  Total output tokens: {estimate['total_output_tokens']:,}\")\n</code></pre></p>"},{"location":"api-reference/metrics/pricing/#compare_models","title":"compare_models()","text":"<pre><code>def compare_models(self, input_tokens: int, output_tokens: int) -&gt; Dict[str, float]\n</code></pre> <p>Compares costs across all models.</p> <p>Parameters: - <code>input_tokens</code> (int): Number of input tokens - <code>output_tokens</code> (int): Number of output tokens</p> <p>Returns: - <code>Dict[str, float]</code>: Dictionary model -&gt; cost, sorted by ascending cost</p>"},{"location":"api-reference/metrics/pricing/#get_cheapest_model","title":"get_cheapest_model()","text":"<pre><code>def get_cheapest_model(self, input_tokens: int, output_tokens: int) -&gt; tuple[str, float]:\n</code></pre> <p>Finds the cheapest model for a given usage.</p> <p>Parameters: - <code>input_tokens</code> (int): Number of input tokens - <code>output_tokens</code> (int): Number of output tokens</p> <p>Returns: - <code>tuple[str, float]</code>: Tuple (model_name, cost)</p> <p>Example: <pre><code># Compare all models\ncosts = manager.compare_models(input_tokens=2000, output_tokens=1000)\n\nprint(\"Cost ranking (cheapest first):\")\nfor i, (model, cost) in enumerate(costs.items(), 1):\n    print(f\"  {i}. {model}: \u20ac{cost:.6f}\")\n\n# Find the cheapest\ncheapest_model, cheapest_cost = manager.get_cheapest_model(2000, 1000)\nprint(f\"\\n\ud83c\udfc6 Cheapest model: {cheapest_model} (\u20ac{cheapest_cost:.6f})\")\n</code></pre></p>"},{"location":"api-reference/metrics/pricing/#see-also","title":"See Also","text":"<ul> <li><code>Aggregator</code> - Functionality to aggregate metrics across multiple test runs</li> <li><code>Analyzer</code> - Functionality for analyzing and comparing metrics between versions</li> <li><code>Models</code> - Data models for metrics and comparison structures</li> <li><code>Calculator</code> - Utility for single-call metric calculations</li> <li><code>Tracker</code> - Functionality for tracking and statistical analysis of metrics</li> </ul>"},{"location":"api-reference/metrics/tracker/","title":"Tracker","text":"<p>The <code>prompt_versioner.metrics.tracker</code> module provides functionality for tracking and statistical analysis of metrics.</p>"},{"location":"api-reference/metrics/tracker/#metricstracker","title":"MetricsTracker","text":"<p>Static class for tracking and analyzing prompt version metrics.</p>"},{"location":"api-reference/metrics/tracker/#methods","title":"Methods","text":""},{"location":"api-reference/metrics/tracker/#compute_stats","title":"compute_stats()","text":"<pre><code>@staticmethod\n    def compute_stats(values: List[float]) -&gt; Dict[str, float]\n</code></pre> <p>Computes a statistical summary of metric values.</p> <p>Parameters: - <code>values</code> (List[float]): List of metric values</p> <p>Returns: - <code>Dict[str, float]</code>: Dictionary with statistical measures:   - <code>count</code>: Number of values   - <code>mean</code>: Arithmetic mean   - <code>median</code>: Median   - <code>std_dev</code>: Standard deviation   - <code>min</code>: Minimum value   - <code>max</code>: Maximum value   - <code>sum</code>: Total sum</p> <p>Example: <pre><code>from prompt_versioner.metrics.tracker import MetricsTracker\n\nvalues = [1.5, 2.3, 1.8, 3.1, 2.0, 1.9, 2.5]\nstats = MetricsTracker.compute_stats(values)\nprint(f\"Mean: {stats['mean']:.2f}\")\nprint(f\"Standard deviation: {stats['std_dev']:.2f}\")\n</code></pre></p>"},{"location":"api-reference/metrics/tracker/#compute_percentiles","title":"compute_percentiles()","text":"<pre><code>@staticmethod\n    def compute_percentiles(\n        values: List[float], percentiles: List[int] = [25, 50, 75, 90, 95, 99]\n    ) -&gt; Dict[int, float]\n</code></pre> <p>Computes percentiles of metric values.</p> <p>Parameters: - <code>values</code> (List[float]): List of metric values - <code>percentiles</code> (List[int]): List of percentiles to compute (default: [25, 50, 75, 90, 95, 99])</p> <p>Returns: - <code>Dict[int, float]</code>: Dictionary percentile -&gt; value</p> <p>Example: <pre><code>values = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\npercentiles = MetricsTracker.compute_percentiles(values, [25, 50, 75, 95])\nprint(f\"25th percentile: {percentiles[25]}\")\nprint(f\"95th percentile: {percentiles[95]}\")\n</code></pre></p>"},{"location":"api-reference/metrics/tracker/#analyze_metrics","title":"analyze_metrics()","text":"<pre><code>@staticmethod\n    def analyze_metrics(metrics: Dict[str, List[float]]) -&gt; List[MetricStats]\n</code></pre> <p>Analyzes metrics and returns statistical summaries.</p> <p>Parameters: - <code>metrics</code> (Dict[str, List[float]]): Dictionary metric name -&gt; list of values</p> <p>Returns: - <code>List[MetricStats]</code>: List of MetricStats objects</p> <p>Example: <pre><code>metrics = {\n    \"latency_ms\": [100, 120, 95, 110, 105],\n    \"cost_eur\": [0.001, 0.0015, 0.0012, 0.0018, 0.0014]\n}\nstats_list = MetricsTracker.analyze_metrics(metrics)\nfor stats in stats_list:\n    print(f\"{stats.name}: mean={stats.mean:.4f}, std={stats.std_dev:.4f}\")\n</code></pre></p>"},{"location":"api-reference/metrics/tracker/#detect_outliers","title":"detect_outliers()","text":"<pre><code>@staticmethod\n    def detect_outliers(\n        values: List[float], method: str = \"iqr\", threshold: float = 1.5\n    ) -&gt; List[int]\n</code></pre> <p>Detects outliers in metric values.</p> <p>Parameters: - <code>values</code> (List[float]): List of metric values - <code>method</code> (str): Method to use ('iqr' or 'zscore', default: 'iqr') - <code>threshold</code> (float): Threshold for outlier detection (default: 1.5)</p> <p>Returns: - <code>List[int]</code>: List of indices of outlier values</p> <p>Detection methods: - IQR (Interquartile Range): Uses Q1 - threshold*IQR and Q3 + threshold*IQR as limits - Z-Score: Uses standard deviation and considers values with |z-score| &gt; threshold as outliers</p> <p>Example: <pre><code>values = [1.0, 1.1, 1.2, 1.1, 1.0, 5.0, 1.1, 1.0, 1.2]  # 5.0 is an outlier\noutliers = MetricsTracker.detect_outliers(values, method=\"iqr\")\nprint(f\"Outlier indices: {outliers}\")  # [5]\n\n# Using z-score\noutliers_z = MetricsTracker.detect_outliers(values, method=\"zscore\", threshold=2.0)\n</code></pre></p>"},{"location":"api-reference/metrics/tracker/#calculate_trend","title":"calculate_trend()","text":"<pre><code>@staticmethod\n    def calculate_trend(values: List[float]) -&gt; Dict[str, Any]\n</code></pre> <p>Calculates the trend in metric values over time.</p> <p>Parameters: - <code>values</code> (List[float]): List of metric values in chronological order</p> <p>Returns: - <code>Dict[str, Any]</code>: Dictionary with trend information:   - <code>trend</code>: Trend type ('increasing', 'decreasing', 'stable', 'insufficient_data')   - <code>direction</code>: Direction ('up', 'down', None)   - <code>slope</code>: Linear regression slope   - <code>start_value</code>: First value   - <code>end_value</code>: Last value   - <code>change</code>: Absolute change   - <code>pct_change</code>: Percent change</p> <p>Example: <pre><code># Increasing values over time\nvalues = [1.0, 1.2, 1.5, 1.8, 2.0]\ntrend = MetricsTracker.calculate_trend(values)\nprint(f\"Trend: {trend['trend']}\")  # 'increasing'\nprint(f\"Change: {trend['change']}\")  # 1.0\nprint(f\"Percent change: {trend['pct_change']:.1f}%\")  # 100.0%\n</code></pre></p>"},{"location":"api-reference/metrics/tracker/#algorithms-used","title":"Algorithms Used","text":""},{"location":"api-reference/metrics/tracker/#linear-regression-for-trend","title":"Linear Regression for Trend","text":"<p>Trend calculation uses simple linear regression: - Slope: Indicates direction and intensity of the trend - R\u00b2: Implicitly evaluated through the slope - Stability threshold: |slope| &lt; 0.01 indicates a stable trend</p>"},{"location":"api-reference/metrics/tracker/#outlier-detection","title":"Outlier Detection","text":"<p>IQR Method: <pre><code>Lower Bound = Q1 - 1.5 * IQR\nUpper Bound = Q3 + 1.5 * IQR\n</code></pre></p> <p>Z-Score Method: <pre><code>Z = |value - mean| / std_dev\nOutlier if Z &gt; threshold\n</code></pre></p>"},{"location":"api-reference/metrics/tracker/#see-also","title":"See Also","text":"<ul> <li><code>Aggregator</code> - Functionality to aggregate metrics across multiple test runs</li> <li><code>Analyzer</code> - Functionality for analyzing and comparing metrics between versions</li> <li><code>Models</code> - Data models for metrics and comparison structures</li> <li><code>Calculator</code> - Utility for single-call metric calculations</li> <li><code>Pricing</code> - Manages model pricing and calculates LLM call costs</li> </ul>"},{"location":"api-reference/storage/database/","title":"Database Manager","text":"<p>The <code>DatabaseManager</code> class handles SQLite database connections and operations for the Prompt Versioner storage layer.</p>"},{"location":"api-reference/storage/database/#overview","title":"Overview","text":"<p>The Database Manager provides: - SQLite database connection management with context managers - Automatic schema initialization and migration - Connection pooling and transaction management - Database utility operations (backup, vacuum, statistics) - SQL injection protection through input validation</p>"},{"location":"api-reference/storage/database/#class-reference","title":"Class Reference","text":""},{"location":"api-reference/storage/database/#databasemanager","title":"DatabaseManager","text":"<pre><code>from prompt_versioner.storage.database import DatabaseManager\n</code></pre> <p>The main database management class that handles all low-level database operations.</p> <p>Constructor: <pre><code>def __init__(self, db_path: Optional[Path] = None)\n</code></pre></p> <p>Parameters: - <code>db_path</code> (Optional[Path]): Path to SQLite database file. Defaults to <code>.prompt_versions/db.sqlite</code></p> <p>Example: <pre><code>from pathlib import Path\nfrom prompt_versioner.storage.database import DatabaseManager\n\n# Default location\ndb = DatabaseManager()  # Creates .prompt_versions/db.sqlite\n\n# Custom location\ndb = DatabaseManager(Path(\"/opt/prompts/database.db\"))\n\n# In-memory database (for testing)\ndb = DatabaseManager(Path(\":memory:\"))\n</code></pre></p>"},{"location":"api-reference/storage/database/#connection-management","title":"Connection Management","text":""},{"location":"api-reference/storage/database/#get_connection","title":"get_connection()","text":"<pre><code>@contextmanager\ndef get_connection(self) -&gt; Generator[sqlite3.Connection, None, None]\n</code></pre> <p>Context manager for database connections with automatic transaction handling.</p> <p>Returns: - <code>Generator[sqlite3.Connection, None, None]</code>: Database connection context manager</p> <p>Features: - Automatic transaction commit on success - Automatic rollback on exceptions - Proper connection cleanup - Row factory set to <code>sqlite3.Row</code> for dict-like access</p> <p>Example: <pre><code># Safe database operations with automatic cleanup\nwith db.get_connection() as conn:\n    cursor = conn.execute(\"SELECT * FROM prompts WHERE name = ?\", (\"my_prompt\",))\n    result = cursor.fetchone()\n\n    # Automatic commit on successful completion\n    # Automatic rollback if exception occurs\n</code></pre></p> <p>Transaction Behavior: <pre><code>try:\n    with db.get_connection() as conn:\n        # Multiple operations in single transaction\n        conn.execute(\"INSERT INTO prompts (name, system_prompt) VALUES (?, ?)\",\n                    (\"prompt1\", \"System prompt 1\"))\n        conn.execute(\"INSERT INTO prompts (name, system_prompt) VALUES (?, ?)\",\n                    (\"prompt2\", \"System prompt 2\"))\n        # Both inserts committed together\nexcept Exception as e:\n    # Both inserts rolled back on any error\n    print(f\"Transaction failed: {e}\")\n</code></pre></p>"},{"location":"api-reference/storage/database/#query-execution","title":"Query Execution","text":""},{"location":"api-reference/storage/database/#execute","title":"execute()","text":"<pre><code>def execute(\n    self,\n    query: str,\n    params: tuple = (),\n    fetch: str | None = None\n) -&gt; Any\n</code></pre> <p>Execute a single SQL query with optional result fetching.</p> <p>Parameters: - <code>query</code> (str): SQL query string - <code>params</code> (tuple): Query parameters for safe parameter binding - <code>fetch</code> (str | None): Result fetch mode - 'one', 'all', or None</p> <p>Returns: - <code>Any</code>: Query results based on fetch parameter   - <code>'one'</code>: Returns single row or None   - <code>'all'</code>: Returns list of all rows   - <code>None</code>: Returns cursor for manual fetching</p> <p>Example: <pre><code># Insert with no result\ndb.execute(\n    \"INSERT INTO prompts (name, system_prompt) VALUES (?, ?)\",\n    (\"my_prompt\", \"You are a helpful assistant.\")\n)\n\n# Fetch single result\nprompt = db.execute(\n    \"SELECT * FROM prompts WHERE name = ?\",\n    (\"my_prompt\",),\n    fetch=\"one\"\n)\n\n# Fetch all results\nall_prompts = db.execute(\n    \"SELECT name, version FROM prompts ORDER BY created_at DESC\",\n    fetch=\"all\"\n)\n\n# Manual cursor handling\ncursor = db.execute(\"SELECT COUNT(*) FROM prompts\")\ncount = cursor.fetchone()[0]\n</code></pre></p>"},{"location":"api-reference/storage/database/#execute_many","title":"execute_many()","text":"<pre><code>def execute_many(self, query: str, params_list: List[tuple]) -&gt; None\n</code></pre> <p>Execute a query multiple times with different parameter sets (bulk operations).</p> <p>Parameters: - <code>query</code> (str): SQL query string - <code>params_list</code> (List[tuple]): List of parameter tuples</p> <p>Example: <pre><code># Bulk insert multiple prompts\nprompts_data = [\n    (\"prompt1\", \"System prompt 1\", \"User prompt 1\"),\n    (\"prompt2\", \"System prompt 2\", \"User prompt 2\"),\n    (\"prompt3\", \"System prompt 3\", \"User prompt 3\"),\n]\n\ndb.execute_many(\n    \"INSERT INTO prompts (name, system_prompt, user_prompt) VALUES (?, ?, ?)\",\n    prompts_data\n)\n\n# Bulk metrics insertion\nmetrics_data = [\n    (\"prompt1\", \"1.0.0\", 0.85, 1200),\n    (\"prompt1\", \"1.0.0\", 0.87, 1150),\n    (\"prompt1\", \"1.0.0\", 0.86, 1300),\n]\n\ndb.execute_many(\n    \"INSERT INTO metrics (prompt_name, version, quality_score, latency_ms) VALUES (?, ?, ?, ?)\",\n    metrics_data\n)\n</code></pre></p>"},{"location":"api-reference/storage/database/#database-introspection","title":"Database Introspection","text":""},{"location":"api-reference/storage/database/#get_table_info","title":"get_table_info()","text":"<pre><code>def get_table_info(self, table_name: str) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get detailed information about a database table's schema.</p> <p>Parameters: - <code>table_name</code> (str): Name of the table to inspect</p> <p>Returns: - <code>List[Dict[str, Any]]</code>: List of column information dictionaries</p> <p>Column Information: - <code>cid</code>: Column ID - <code>name</code>: Column name - <code>type</code>: SQL data type - <code>notnull</code>: Whether column is NOT NULL (1 or 0) - <code>dflt_value</code>: Default value - <code>pk</code>: Whether column is primary key (1 or 0)</p> <p>Example: <pre><code># Get table schema information\ntable_info = db.get_table_info(\"prompts\")\n\nfor column in table_info:\n    print(f\"Column: {column['name']}\")\n    print(f\"  Type: {column['type']}\")\n    print(f\"  Not Null: {bool(column['notnull'])}\")\n    print(f\"  Primary Key: {bool(column['pk'])}\")\n    print(f\"  Default: {column['dflt_value']}\")\n</code></pre></p> <p>Output: <pre><code>Column: id\n  Type: INTEGER\n  Not Null: True\n  Primary Key: True\n  Default: None\n\nColumn: name\n  Type: TEXT\n  Not Null: True\n  Primary Key: False\n  Default: None\n\nColumn: system_prompt\n  Type: TEXT\n  Not Null: True\n  Primary Key: False\n  Default: None\n</code></pre></p>"},{"location":"api-reference/storage/database/#get_tables","title":"get_tables()","text":"<pre><code>def get_tables(self) -&gt; List[str]\n</code></pre> <p>Get list of all tables in the database.</p> <p>Returns: - <code>List[str]</code>: List of table names sorted alphabetically</p> <p>Example: <pre><code>tables = db.get_tables()\nprint(\"Database tables:\", tables)\n# Output: ['annotations', 'metrics', 'prompts', 'versions']\n\n# Check if specific table exists\nif \"prompts\" in db.get_tables():\n    print(\"Prompts table exists\")\n</code></pre></p>"},{"location":"api-reference/storage/database/#database-maintenance","title":"Database Maintenance","text":""},{"location":"api-reference/storage/database/#vacuum","title":"vacuum()","text":"<pre><code>def vacuum(self) -&gt; None\n</code></pre> <p>Vacuum the database to reclaim space and optimize performance.</p> <p>Side Effects: - Rebuilds database file to remove unused space - Updates internal statistics for query optimization - May take time for large databases</p> <p>Example: <pre><code># Regular maintenance\ndb.vacuum()\nprint(\"Database vacuumed and optimized\")\n\n# Check size before and after\nsize_before = db.get_db_size()\ndb.vacuum()\nsize_after = db.get_db_size()\nprint(f\"Reclaimed {size_before - size_after} bytes\")\n</code></pre></p>"},{"location":"api-reference/storage/database/#get_db_size","title":"get_db_size()","text":"<pre><code>def get_db_size(self) -&gt; int\n</code></pre> <p>Get the current database file size in bytes.</p> <p>Returns: - <code>int</code>: Size in bytes, or 0 if database file doesn't exist</p> <p>Example: <pre><code>size_bytes = db.get_db_size()\nsize_mb = size_bytes / (1024 * 1024)\nprint(f\"Database size: {size_mb:.2f} MB\")\n\n# Monitor growth\ndef monitor_db_growth():\n    sizes = []\n    for i in range(10):\n        # ... perform operations ...\n        sizes.append(db.get_db_size())\n\n    growth = sizes[-1] - sizes[0]\n    print(f\"Database grew by {growth} bytes\")\n</code></pre></p>"},{"location":"api-reference/storage/database/#backup","title":"backup()","text":"<pre><code>def backup(self, backup_path: Path) -&gt; None\n</code></pre> <p>Create a backup of the database to another file.</p> <p>Parameters: - <code>backup_path</code> (Path): Path for the backup file</p> <p>Features: - Creates parent directories if they don't exist - Uses SQLite's built-in backup API for consistent backups - Can backup while database is in use</p> <p>Example: <pre><code>from pathlib import Path\nfrom datetime import datetime\n\n# Daily backup\nbackup_dir = Path(\"/backups/prompts\")\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nbackup_file = backup_dir / f\"prompts_backup_{timestamp}.db\"\n\ndb.backup(backup_file)\nprint(f\"Backup created: {backup_file}\")\n\n# Automated backup function\ndef create_backup():\n    backup_path = Path(f\"backups/prompts_{datetime.now().isoformat()}.db\")\n    db.backup(backup_path)\n    return backup_path\n</code></pre></p>"},{"location":"api-reference/storage/database/#database-statistics","title":"Database Statistics","text":""},{"location":"api-reference/storage/database/#get_stats","title":"get_stats()","text":"<pre><code>def get_stats(self) -&gt; Dict[str, Any]\n</code></pre> <p>Get comprehensive database statistics and information.</p> <p>Returns: - <code>Dict[str, Any]</code>: Dictionary containing database statistics</p> <p>Statistics Included: - <code>db_path</code>: Database file path - <code>db_size_bytes</code>: Database file size in bytes - <code>tables</code>: Dictionary mapping table names to row counts</p> <p>Example: <pre><code>stats = db.get_stats()\n\nprint(f\"Database: {stats['db_path']}\")\nprint(f\"Size: {stats['db_size_bytes']} bytes\")\nprint(\"\\nTable Row Counts:\")\nfor table, count in stats['tables'].items():\n    print(f\"  {table}: {count:,} rows\")\n</code></pre></p> <p>Output: <pre><code>Database: /path/to/prompts.db\nSize: 2,048,576 bytes\n\nTable Row Counts:\n  annotations: 15 rows\n  metrics: 1,247 rows\n  prompts: 8 rows\n  versions: 23 rows\n</code></pre></p>"},{"location":"api-reference/storage/database/#security-features","title":"Security Features","text":""},{"location":"api-reference/storage/database/#sql-injection-protection","title":"SQL Injection Protection","text":"<p>The DatabaseManager includes automatic protection against SQL injection:</p> <pre><code>def _validate_table_name(self, table_name: str) -&gt; None:\n    \"\"\"Validate table name to prevent SQL injection.\"\"\"\n    if not re.match(r\"^[A-Za-z_][A-Za-z0-9_]*$\", table_name):\n        raise ValueError(f\"Invalid table name: {table_name}\")\n</code></pre> <p>Safe Practices: <pre><code># \u2705 Safe - uses parameterized queries\ndb.execute(\"SELECT * FROM prompts WHERE name = ?\", (\"user_input\",))\n\n# \u2705 Safe - table name validated\ntable_info = db.get_table_info(\"prompts\")  # Validates table name\n\n# \u274c Unsafe - don't do this\n# db.execute(f\"SELECT * FROM prompts WHERE name = '{user_input}'\")\n</code></pre></p>"},{"location":"api-reference/storage/database/#integration-promptstorage","title":"Integration PromptStorage","text":"<pre><code>from prompt_versioner.storage import PromptStorage\n\n# Storage layer uses DatabaseManager internally\nstorage = PromptStorage()  # Creates DatabaseManager automatically\n\n# Access underlying database if needed\ndb_stats = storage.db.get_stats()\nprint(f\"Total prompts: {db_stats['tables']['prompts']}\")\n</code></pre>"},{"location":"api-reference/storage/database/#see-also","title":"See Also","text":"<ul> <li><code>Schema</code> - Database schema definitions and indexes</li> <li><code>Queries</code> - Pre-built query functions</li> <li><code>SQLite Documentation</code> - SQLite reference</li> </ul>"},{"location":"api-reference/storage/queries/","title":"Queries","text":"<p>The <code>prompt_versioner.storage.queries</code> module provides utilities for safe SQL query construction and common predefined queries.</p>"},{"location":"api-reference/storage/queries/#querybuilder","title":"QueryBuilder","text":"<p>A static class for safely building SQL queries with protection against SQL injection.</p>"},{"location":"api-reference/storage/queries/#methods","title":"Methods","text":""},{"location":"api-reference/storage/queries/#validate_table_name","title":"validate_table_name()","text":"<pre><code>@staticmethod\n    def _validate_table_name(table_name: str) -&gt; None\n</code></pre> <p>Validates a table name to prevent SQL injection.</p> <p>Parameters: - <code>table</code> (str): Name of the table to validate</p> <p>Raises: - <code>ValueError</code>: If the table name contains invalid characters</p> <p>Example: <pre><code>QueryBuilder._validate_table_name(\"prompt_versions\")  # OK\nQueryBuilder._validate_table_name(\"invalid; DROP TABLE\") # Raises ValueError\n</code></pre></p>"},{"location":"api-reference/storage/queries/#_validate_column_names","title":"_validate_column_names()","text":"<pre><code>@staticmethod\n    def _validate_column_names(columns: list[str]) -&gt; None\n</code></pre> <p>Validates a list of column names to prevent SQL injection.</p> <p>Parameters: - <code>columns</code> (List[str]): List of column names to validate</p> <p>Raises: - <code>ValueError</code>: If any column name contains invalid characters</p> <p>Example: <pre><code>QueryBuilder._validate_column_names([\"id\", \"name\", \"version\"])  # OK\nQueryBuilder._validate_column_names([\"id; DROP TABLE\"])  # Raises ValueError\n</code></pre></p>"},{"location":"api-reference/storage/queries/#build_where_clause","title":"build_where_clause()","text":"<pre><code>@staticmethod\n    def build_where_clause(conditions: Dict[str, Any]) -&gt; tuple[str, List[Any]]\n</code></pre> <p>Builds a safe WHERE clause from a dictionary of conditions.</p> <p>Parameters: - <code>conditions</code> (Dict[str, Any]): Dictionary of conditions {column: value}</p> <p>Returns: - <code>tuple[str, List[Any]]</code>: Tuple containing (WHERE clause, parameters)</p> <p>Example: <pre><code>conditions = {\"name\": \"my_prompt\", \"version\": \"1.0.0\"}\nwhere_clause, params = QueryBuilder.build_where_clause(conditions)\n# where_clause: \"WHERE name = ? AND version = ?\"\n# params: [\"my_prompt\", \"1.0.0\"]\n</code></pre></p>"},{"location":"api-reference/storage/queries/#build_select","title":"build_select()","text":"<pre><code>@staticmethod\ndef build_select(\n        table: str,\n        columns: list[str] | None = None,\n        where: Dict[str, Any] | None = None,\n        order_by: str | None = None,\n        limit: int | None = None,\n    ) -&gt; tuple[str, List[Any]]\n</code></pre> <p>Builds a complete and safe SELECT query.</p> <p>Parameters: - <code>table</code> (str): Table name - <code>columns</code> (List[str] | None): List of columns to select (None for *) - <code>where</code> (Dict[str, Any] | None): WHERE conditions - <code>order_by</code> (str | None): ORDER BY clause - <code>limit</code> (int | None): Maximum number of results</p> <p>Returns: - <code>tuple[str, List[Any]]</code>: Tuple containing (SQL query, parameters)</p> <p>Example: <pre><code>query, params = QueryBuilder.build_select(\n    table=\"prompt_versions\",\n    columns=[\"id\", \"name\", \"version\"],\n    where={\"name\": \"my_prompt\"},\n    order_by=\"timestamp DESC\",\n    limit=10\n)\n# query: \"SELECT id, name, version FROM prompt_versions WHERE name = ? ORDER BY timestamp DESC LIMIT 10\"\n# params: [\"my_prompt\"]\n</code></pre></p>"},{"location":"api-reference/storage/queries/#commonqueries","title":"CommonQueries","text":"<p>A collection of predefined queries for common operations.</p>"},{"location":"api-reference/storage/queries/#methods_1","title":"Methods","text":""},{"location":"api-reference/storage/queries/#get_version_stats","title":"get_version_stats()","text":"<pre><code>@staticmethod\n    def get_version_stats(db_manager: DatabaseManager) -&gt; Dict[str, Any]\n</code></pre> <p>Gets general statistics about versions.</p> <p>Parameters: - <code>db_manager</code> (DatabaseManager): Instance of DatabaseManager</p> <p>Returns: - <code>Dict[str, Any]</code>: Dictionary with statistics:   - <code>total_versions</code>: Total number of versions   - <code>total_prompts</code>: Total number of unique prompts   - <code>total_metrics</code>: Total number of metrics   - <code>total_annotations</code>: Total number of annotations</p> <p>Example: <pre><code>from prompt_versioner.storage.database import DatabaseManager\nfrom prompt_versioner.storage.queries import CommonQueries\n\ndb_manager = DatabaseManager(\"path/to/database.db\")\nstats = CommonQueries.get_version_stats(db_manager)\nprint(f\"Total versions: {stats['total_versions']}\")\nprint(f\"Total prompts: {stats['total_prompts']}\")\n</code></pre></p>"},{"location":"api-reference/storage/queries/#get_most_used_models","title":"get_most_used_models()","text":"<pre><code>@staticmethod\n    def get_most_used_models(db_manager: DatabaseManager, limit: int = 10) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Gets the most used models with usage statistics.</p> <p>Parameters: - <code>db_manager</code> (DatabaseManager): Instance of DatabaseManager - <code>limit</code> (int): Maximum number of results (default: 10)</p> <p>Returns: - <code>List[Dict[str, Any]]</code>: List of dictionaries with statistics per model:   - <code>model_name</code>: Model name   - <code>usage_count</code>: Number of uses   - <code>avg_cost</code>: Average cost in EUR   - <code>avg_latency</code>: Average latency in ms</p> <p>Example: <pre><code>models = CommonQueries.get_most_used_models(db_manager, limit=5)\nfor model in models:\n    print(f\"{model['model_name']}: {model['usage_count']} uses, \"\n          f\"avg cost: \u20ac{model['avg_cost']:.4f}\")\n</code></pre></p>"},{"location":"api-reference/storage/queries/#get_recent_activity","title":"get_recent_activity()","text":"<pre><code>@staticmethod\n    def get_recent_activity(db_manager: DatabaseManager, days: int = 7) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Gets recent version activity.</p> <p>Parameters: - <code>db_manager</code> (DatabaseManager): Instance of DatabaseManager - <code>days</code> (int): Number of days to consider (default: 7)</p> <p>Returns: - <code>List[Dict[str, Any]]</code>: List of recent versions ordered by descending timestamp</p> <p>Example: <pre><code>recent_versions = CommonQueries.get_recent_activity(db_manager, days=3)\nfor version in recent_versions:\n    print(f\"{version['name']} v{version['version']} - {version['timestamp']}\")\n</code></pre></p>"},{"location":"api-reference/storage/queries/#usage-with-databasemanager","title":"Usage with DatabaseManager","text":"<p>Queries built with <code>QueryBuilder</code> are designed to be used with <code>DatabaseManager</code>:</p> <pre><code>from prompt_versioner.storage.database import DatabaseManager\nfrom prompt_versioner.storage.queries import QueryBuilder, CommonQueries\n\n# Initialize database manager\ndb_manager = DatabaseManager(\"path/to/database.db\")\n\n# Build custom query\nquery, params = QueryBuilder.build_select(\n    table=\"prompt_versions\",\n    where={\"name\": \"my_prompt\"},\n    order_by=\"timestamp DESC\"\n)\n\n# Execute query\nresults = db_manager.execute(query, params, fetch=\"all\")\n\n# Or use predefined queries\nstats = CommonQueries.get_version_stats(db_manager)\n</code></pre>"},{"location":"api-reference/storage/queries/#see-also","title":"See Also","text":"<ul> <li><code>DatabaseManager</code> - Database connection and operations</li> <li><code>Queries</code> - Pre-built query functions</li> <li>SQLite Foreign Keys - Foreign key documentation</li> <li>SQLite Indexes - Index optimization guide</li> </ul>"},{"location":"api-reference/storage/schema/","title":"Database Schema","text":"<p>Database schema definitions and table structures for the Prompt Versioner SQLite database.</p>"},{"location":"api-reference/storage/schema/#overview","title":"Overview","text":"<p>The database schema defines the structure for storing prompts, versions, metrics, annotations, and related data. The schema uses SQLite with foreign key constraints and optimized indexes for performance.</p>"},{"location":"api-reference/storage/schema/#schema-module","title":"Schema Module","text":""},{"location":"api-reference/storage/schema/#schema-definitions","title":"Schema Definitions","text":"<pre><code>from prompt_versioner.storage.schema import SCHEMA_DEFINITIONS, INDEXES, SCHEMA_VERSION\n</code></pre> <p>The schema module provides: - Table Definitions: DDL statements for all database tables - Index Definitions: Performance-optimized indexes - Schema Versioning: Migration support through version tracking</p>"},{"location":"api-reference/storage/schema/#database-tables","title":"Database Tables","text":""},{"location":"api-reference/storage/schema/#prompt_versions","title":"prompt_versions","text":"<p>Main table storing prompt versions and their content.</p> <pre><code>CREATE TABLE IF NOT EXISTS prompt_versions (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name TEXT NOT NULL,\n    version TEXT NOT NULL,\n    system_prompt TEXT NOT NULL,\n    user_prompt TEXT NOT NULL,\n    metadata TEXT,\n    git_commit TEXT,\n    timestamp TEXT NOT NULL,\n    created_by TEXT,\n    tags TEXT,\n    UNIQUE(name, version)\n)\n</code></pre> <p>Columns:</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Unique prompt version ID <code>name</code> TEXT NOT NULL Prompt name identifier <code>version</code> TEXT NOT NULL Semantic version (e.g., \"1.2.0\") <code>system_prompt</code> TEXT NOT NULL System prompt content <code>user_prompt</code> TEXT NOT NULL User prompt template <code>metadata</code> TEXT - JSON metadata (optional) <code>git_commit</code> TEXT - Git commit hash (optional) <code>timestamp</code> TEXT NOT NULL ISO timestamp of creation <code>created_by</code> TEXT - Author identifier (optional) <code>tags</code> TEXT - Comma-separated tags (deprecated, use version_tags) <p>Constraints: - <code>UNIQUE(name, version)</code>: Prevents duplicate versions for same prompt</p> <p>Example Data: <pre><code>INSERT INTO prompt_versions (\n    name, version, system_prompt, user_prompt, metadata, timestamp\n) VALUES (\n    'code_reviewer',\n    '1.2.0',\n    'You are an expert code reviewer.',\n    'Review this code: {code}',\n    '{\"author\": \"john.doe\", \"purpose\": \"code review\"}',\n    '2025-01-15T10:30:00Z'\n);\n</code></pre></p>"},{"location":"api-reference/storage/schema/#prompt_metrics","title":"prompt_metrics","text":"<p>Performance metrics and usage data for prompt versions.</p> <pre><code>CREATE TABLE IF NOT EXISTS prompt_metrics (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    version_id INTEGER NOT NULL,\n    model_name TEXT,\n    input_tokens INTEGER,\n    output_tokens INTEGER,\n    total_tokens INTEGER,\n    cost_eur REAL,\n    latency_ms REAL,\n    quality_score REAL,\n    accuracy REAL,\n    temperature REAL,\n    top_p REAL,\n    max_tokens INTEGER,\n    success BOOLEAN,\n    error_message TEXT,\n    timestamp TEXT NOT NULL,\n    metadata TEXT,\n    FOREIGN KEY (version_id) REFERENCES prompt_versions(id) ON DELETE CASCADE\n)\n</code></pre> <p>Columns:</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Unique metric record ID <code>version_id</code> INTEGER NOT NULL, FK Reference to prompt_versions.id <code>model_name</code> TEXT - AI model used (e.g., \"gpt-4o\") <code>input_tokens</code> INTEGER - Number of input tokens <code>output_tokens</code> INTEGER - Number of output tokens <code>total_tokens</code> INTEGER - Total tokens (calculated) <code>cost_eur</code> REAL - Cost in euros <code>latency_ms</code> REAL - Response latency in milliseconds <code>quality_score</code> REAL - Quality assessment (0-1) <code>accuracy</code> REAL - Accuracy measurement (0-1) <code>temperature</code> REAL - Model temperature parameter <code>top_p</code> REAL - Model top_p parameter <code>max_tokens</code> INTEGER - Model max_tokens parameter <code>success</code> BOOLEAN - Whether request succeeded <code>error_message</code> TEXT - Error details if failed <code>timestamp</code> TEXT NOT NULL ISO timestamp of metric collection <code>metadata</code> TEXT - Additional JSON metadata <p>Relationships: - <code>FOREIGN KEY (version_id) REFERENCES prompt_versions(id) ON DELETE CASCADE</code></p> <p>Example Data: <pre><code>INSERT INTO prompt_metrics (\n    version_id, model_name, input_tokens, output_tokens,\n    cost_eur, latency_ms, quality_score, success, timestamp\n) VALUES (\n    1, 'gpt-4o-mini', 150, 200, 0.004, 1250.5, 0.87, 1, '2025-01-15T10:35:00Z'\n);\n</code></pre></p>"},{"location":"api-reference/storage/schema/#annotations","title":"annotations","text":"<p>Team annotations and comments on prompt versions.</p> <pre><code>CREATE TABLE IF NOT EXISTS annotations (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    version_id INTEGER NOT NULL,\n    author TEXT NOT NULL,\n    text TEXT NOT NULL,\n    timestamp TEXT NOT NULL,\n    annotation_type TEXT DEFAULT 'comment',\n    resolved BOOLEAN DEFAULT 0,\n    FOREIGN KEY (version_id) REFERENCES prompt_versions(id) ON DELETE CASCADE\n)\n</code></pre> <p>Columns:</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Unique annotation ID <code>version_id</code> INTEGER NOT NULL, FK Reference to prompt_versions.id <code>author</code> TEXT NOT NULL Annotation author <code>text</code> TEXT NOT NULL Annotation content <code>timestamp</code> TEXT NOT NULL ISO timestamp of creation <code>annotation_type</code> TEXT DEFAULT 'comment' Type: comment, review, approval <code>resolved</code> BOOLEAN DEFAULT 0 Whether annotation is resolved <p>Annotation Types: - <code>comment</code>: General comment or note - <code>review</code>: Code/prompt review feedback - <code>approval</code>: Approval for production deployment - <code>issue</code>: Issue or problem identification</p> <p>Example Data: <pre><code>INSERT INTO annotations (\n    version_id, author, text, annotation_type, timestamp\n) VALUES (\n    1, 'team_lead', 'Approved for production deployment', 'approval', '2025-01-15T11:00:00Z'\n);\n</code></pre></p>"},{"location":"api-reference/storage/schema/#version_tags","title":"version_tags","text":"<p>Tag system for organizing and categorizing prompt versions.</p> <pre><code>CREATE TABLE IF NOT EXISTS version_tags (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    version_id INTEGER NOT NULL,\n    tag TEXT NOT NULL,\n    FOREIGN KEY (version_id) REFERENCES prompt_versions(id) ON DELETE CASCADE,\n    UNIQUE(version_id, tag)\n)\n</code></pre> <p>Columns:</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Unique tag association ID <code>version_id</code> INTEGER NOT NULL, FK Reference to prompt_versions.id <code>tag</code> TEXT NOT NULL Tag name <p>Constraints: - <code>UNIQUE(version_id, tag)</code>: Prevents duplicate tags on same version</p> <p>Common Tags: - <code>production</code>: Production-ready versions - <code>experimental</code>: Experimental versions - <code>deprecated</code>: Deprecated versions - <code>stable</code>: Stable releases - <code>hotfix</code>: Emergency fixes</p> <p>Example Data: <pre><code>INSERT INTO version_tags (version_id, tag) VALUES\n    (1, 'production'),\n    (1, 'stable'),\n    (2, 'experimental');\n</code></pre></p>"},{"location":"api-reference/storage/schema/#database-indexes","title":"Database Indexes","text":"<p>Performance-optimized indexes for common query patterns:</p> <pre><code>INDEXES = [\n    \"CREATE INDEX IF NOT EXISTS idx_name_version ON prompt_versions(name, version)\",\n    \"CREATE INDEX IF NOT EXISTS idx_timestamp ON prompt_versions(timestamp DESC)\",\n    \"CREATE INDEX IF NOT EXISTS idx_name ON prompt_versions(name)\",\n    \"CREATE INDEX IF NOT EXISTS idx_metrics_version ON prompt_metrics(version_id)\",\n    \"CREATE INDEX IF NOT EXISTS idx_metrics_timestamp ON prompt_metrics(timestamp DESC)\",\n    \"CREATE INDEX IF NOT EXISTS idx_annotations_version ON annotations(version_id)\",\n    \"CREATE INDEX IF NOT EXISTS idx_tags_version ON version_tags(version_id)\",\n    \"CREATE INDEX IF NOT EXISTS idx_tags_tag ON version_tags(tag)\",\n]\n</code></pre>"},{"location":"api-reference/storage/schema/#index-purpose","title":"Index Purpose","text":"Index Purpose Query Optimization <code>idx_name_version</code> Composite index Fast lookup by name + version <code>idx_timestamp</code> Temporal ordering Recent versions first <code>idx_name</code> Name-based queries List versions by prompt name <code>idx_metrics_version</code> Metrics lookup Get metrics for specific version <code>idx_metrics_timestamp</code> Temporal metrics Recent metrics analysis <code>idx_annotations_version</code> Annotation lookup Get annotations for version <code>idx_tags_version</code> Tag queries Find tags for version <code>idx_tags_tag</code> Tag filtering Find versions with specific tag"},{"location":"api-reference/storage/schema/#query-performance-examples","title":"Query Performance Examples","text":"<pre><code>-- \u2705 Optimized: Uses idx_name_version\nSELECT * FROM prompt_versions WHERE name = 'code_reviewer' AND version = '1.2.0';\n\n-- \u2705 Optimized: Uses idx_timestamp\nSELECT * FROM prompt_versions ORDER BY timestamp DESC LIMIT 10;\n\n-- \u2705 Optimized: Uses idx_metrics_version\nSELECT AVG(quality_score) FROM prompt_metrics WHERE version_id = 1;\n\n-- \u2705 Optimized: Uses idx_tags_tag\nSELECT pv.* FROM prompt_versions pv\nJOIN version_tags vt ON pv.id = vt.version_id\nWHERE vt.tag = 'production';\n</code></pre>"},{"location":"api-reference/storage/schema/#schema-versioning","title":"Schema Versioning","text":""},{"location":"api-reference/storage/schema/#current-schema-version","title":"Current Schema Version","text":"<pre><code>SCHEMA_VERSION = 1\n</code></pre> <p>The schema version enables database migrations and compatibility checking:</p> <pre><code>def check_schema_version(db):\n    \"\"\"Check if database schema matches current version.\"\"\"\n    current_version = get_user_version(db)\n    if current_version != SCHEMA_VERSION:\n        raise SchemaVersionMismatch(\n            f\"Database schema v{current_version} != required v{SCHEMA_VERSION}\"\n        )\n</code></pre>"},{"location":"api-reference/storage/schema/#migration-support","title":"Migration Support","text":"<p>Future schema changes will increment <code>SCHEMA_VERSION</code> and provide migration scripts:</p> <pre><code># Future migration example\nMIGRATIONS = {\n    1: [\n        \"ALTER TABLE prompt_versions ADD COLUMN new_field TEXT\",\n        \"CREATE INDEX idx_new_field ON prompt_versions(new_field)\"\n    ],\n    2: [\n        \"CREATE TABLE new_table (...)\",\n        \"INSERT INTO new_table SELECT ... FROM old_table\"\n    ]\n}\n</code></pre>"},{"location":"api-reference/storage/schema/#relationships-and-constraints","title":"Relationships and Constraints","text":""},{"location":"api-reference/storage/schema/#entity-relationship-diagram","title":"Entity Relationship Diagram","text":"<pre><code>prompt_versions (1) \u2500\u2500\u2500\u2500 (N) prompt_metrics\n       \u2502\n       \u2502 (1)\n       \u2502\n       \u2514\u2500\u2500 (N) annotations\n       \u2502\n       \u2502 (1)\n       \u2502\n       \u2514\u2500\u2500 (N) version_tags\n</code></pre>"},{"location":"api-reference/storage/schema/#foreign-key-behavior","title":"Foreign Key Behavior","text":"<p>All foreign keys use <code>ON DELETE CASCADE</code> for automatic cleanup:</p> <pre><code>-- When a prompt version is deleted:\nDELETE FROM prompt_versions WHERE id = 1;\n\n-- Automatically deletes:\n-- - All metrics for that version\n-- - All annotations for that version\n-- - All tags for that version\n</code></pre>"},{"location":"api-reference/storage/schema/#data-integrity-rules","title":"Data Integrity Rules","text":"<ol> <li>Unique Versions: Each prompt can have only one version with a given number</li> <li>Required Fields: Core fields (name, version, prompts, timestamp) are mandatory</li> <li>Referential Integrity: Metrics, annotations, and tags must reference valid versions</li> <li>Cascade Deletion: Related data is automatically cleaned up</li> </ol>"},{"location":"api-reference/storage/schema/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/storage/schema/#creating-tables","title":"Creating Tables","text":"<pre><code>from prompt_versioner.storage.database import DatabaseManager\nfrom prompt_versioner.storage.schema import SCHEMA_DEFINITIONS, INDEXES\n\ndb = DatabaseManager()\n\n# Tables are created automatically during initialization\n# But you can create manually if needed:\nwith db.get_connection() as conn:\n    for table_name, table_sql in SCHEMA_DEFINITIONS.items():\n        conn.execute(table_sql)\n\n    for index_sql in INDEXES:\n        conn.execute(index_sql)\n</code></pre>"},{"location":"api-reference/storage/schema/#querying-related-data","title":"Querying Related Data","text":"<pre><code># Get version with all related data\ndef get_version_with_details(db, version_id):\n    with db.get_connection() as conn:\n        # Main version data\n        version = conn.execute(\n            \"SELECT * FROM prompt_versions WHERE id = ?\",\n            (version_id,)\n        ).fetchone()\n\n        # Metrics summary\n        metrics = conn.execute(\"\"\"\n            SELECT COUNT(*) as count, AVG(quality_score) as avg_quality,\n                   AVG(latency_ms) as avg_latency, AVG(cost_eur) as avg_cost\n            FROM prompt_metrics WHERE version_id = ?\n        \"\"\", (version_id,)).fetchone()\n\n        # Annotations\n        annotations = conn.execute(\n            \"SELECT * FROM annotations WHERE version_id = ? ORDER BY timestamp\",\n            (version_id,)\n        ).fetchall()\n\n        # Tags\n        tags = conn.execute(\n            \"SELECT tag FROM version_tags WHERE version_id = ?\",\n            (version_id,)\n        ).fetchall()\n\n        return {\n            'version': dict(version),\n            'metrics': dict(metrics),\n            'annotations': [dict(a) for a in annotations],\n            'tags': [t['tag'] for t in tags]\n        }\n</code></pre>"},{"location":"api-reference/storage/schema/#complex-queries","title":"Complex Queries","text":"<pre><code># Find best performing versions by tag\ndef get_top_versions_by_tag(db, tag, limit=10):\n    query = \"\"\"\n        SELECT pv.name, pv.version, AVG(pm.quality_score) as avg_quality\n        FROM prompt_versions pv\n        JOIN version_tags vt ON pv.id = vt.version_id\n        JOIN prompt_metrics pm ON pv.id = pm.version_id\n        WHERE vt.tag = ? AND pm.quality_score IS NOT NULL\n        GROUP BY pv.id\n        ORDER BY avg_quality DESC\n        LIMIT ?\n    \"\"\"\n    return db.execute(query, (tag, limit), fetch=\"all\")\n</code></pre>"},{"location":"api-reference/storage/schema/#see-also","title":"See Also","text":"<ul> <li><code>DatabaseManager</code> - Database connection and operations</li> <li><code>Queries</code> - Pre-built query functions</li> <li>SQLite Foreign Keys - Foreign key documentation</li> <li>SQLite Indexes - Index optimization guide</li> </ul>"},{"location":"api-reference/testing/ab-test/","title":"AB Test","text":"<p>The <code>prompt_versioner.testing.ab_test</code> module provides a framework for A/B testing between prompt versions.</p>"},{"location":"api-reference/testing/ab-test/#abtest","title":"ABTest","text":"<p>Class to implement A/B tests and compare performance between two prompt versions.</p>"},{"location":"api-reference/testing/ab-test/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n        self,\n        versioner: Any,\n        prompt_name: str,\n        version_a: str,\n        version_b: str,\n        metric_name: str = \"quality_score\",\n    ):\n</code></pre> <p>Parameters: - <code>versioner</code> (Any): Instance of PromptVersioner - <code>prompt_name</code> (str): Name of the prompt to test - <code>version_a</code> (str): First version (baseline) - <code>version_b</code> (str): Second version (challenger) - <code>metric_name</code> (str): Metric to compare (default: \"quality_score\")</p>"},{"location":"api-reference/testing/ab-test/#methods","title":"Methods","text":""},{"location":"api-reference/testing/ab-test/#log_result","title":"log_result()","text":"<pre><code>def log_result(self, version: str, metric_value: float) -&gt; None\n</code></pre> <p>Logs a test result for a specific version.</p> <p>Parameters: - <code>version</code> (str): Which version ('a' or 'b') - <code>metric_value</code> (float): Value of the metric</p> <p>Raises: - <code>ValueError</code>: If the version is not 'a' or 'b'</p> <p>Example: <pre><code>from prompt_versioner.testing.ab_test import ABTest\n\n# Initialize A/B test\nab_test = ABTest(\n    versioner=versioner,\n    prompt_name=\"email_classifier\",\n    version_a=\"v1.0.0\",\n    version_b=\"v1.1.0\",\n    metric_name=\"accuracy\"\n)\n\n# Log results for version A\nab_test.log_result(\"a\", 0.85)\nab_test.log_result(\"a\", 0.87)\nab_test.log_result(\"a\", 0.84)\n\n# Log results for version B\nab_test.log_result(\"b\", 0.90)\nab_test.log_result(\"b\", 0.92)\nab_test.log_result(\"b\", 0.89)\n</code></pre></p>"},{"location":"api-reference/testing/ab-test/#log_batch_results","title":"log_batch_results()","text":"<pre><code>def log_batch_results(self, version: str, metric_values: List[float]) -&gt; None\n</code></pre> <p>Logs multiple results at once for a version.</p> <p>Parameters: - <code>version</code> (str): Which version ('a' or 'b') - <code>metric_values</code> (List[float]): List of metric values</p> <p>Example: <pre><code># Log batch results\nresults_a = [0.85, 0.87, 0.84, 0.86, 0.88]\nresults_b = [0.90, 0.92, 0.89, 0.91, 0.93]\n\nab_test.log_batch_results(\"a\", results_a)\nab_test.log_batch_results(\"b\", results_b)\n</code></pre></p>"},{"location":"api-reference/testing/ab-test/#get_result","title":"get_result()","text":"<pre><code>def get_result(self) -&gt; ABTestResult\n</code></pre> <p>Gets the A/B test result with statistical analysis.</p> <p>Returns: - <code>ABTestResult</code>: Object with winner and statistics</p> <p>Raises: - <code>ValueError</code>: If there is not enough data for both versions</p> <p>Example: <pre><code>result = ab_test.get_result()\nprint(f\"Winner: {result.winner}\")\nprint(f\"Improvement: {result.improvement:.2f}%\")\nprint(f\"Confidence: {result.confidence:.2f}\")\n</code></pre></p>"},{"location":"api-reference/testing/ab-test/#print_result","title":"print_result()","text":"<pre><code>def print_result(self) -&gt; None\n</code></pre> <p>Prints the A/B test result in a readable format.</p> <p>Example: <pre><code>ab_test.print_result()\n# Output:\n# ================================================================================\n# A/B TEST RESULT: email_classifier\n# ================================================================================\n# Version A (v1.0.0): accuracy = 0.8560 (\u00b10.0151)\n# Version B (v1.1.0): accuracy = 0.9100 (\u00b10.0141)\n#\n# \ud83c\udf89 WINNER: v1.1.0 with 6.31% improvement\n# Confidence: 85%\n</code></pre></p>"},{"location":"api-reference/testing/ab-test/#clear_results","title":"clear_results()","text":"<pre><code>def clear_results(self) -&gt; None\n</code></pre> <p>Clears all logged results.</p> <p>Example: <pre><code>ab_test.clear_results()\n# Removes all data to restart the test\n</code></pre></p>"},{"location":"api-reference/testing/ab-test/#get_sample_counts","title":"get_sample_counts()","text":"<pre><code>def get_sample_counts(self) -&gt; tuple[int, int]\n</code></pre> <p>Gets the number of samples for each version.</p> <p>Returns: - <code>tuple[int, int]</code>: Tuple (count_a, count_b)</p> <p>Example: <pre><code>count_a, count_b = ab_test.get_sample_counts()\nprint(f\"Version A: {count_a} samples\")\nprint(f\"Version B: {count_b} samples\")\n</code></pre></p>"},{"location":"api-reference/testing/ab-test/#is_ready","title":"is_ready()","text":"<pre><code>def is_ready(self, min_samples: int = 30) -&gt; bool\n</code></pre> <p>Checks if enough samples have been collected for reliable results.</p> <p>Parameters: - <code>min_samples</code> (int): Minimum number of samples per version (default: 30)</p> <p>Returns: - <code>bool</code>: True if both versions have enough samples</p> <p>Example: <pre><code>if ab_test.is_ready(min_samples=50):\n    result = ab_test.get_result()\n    ab_test.print_result()\nelse:\n    count_a, count_b = ab_test.get_sample_counts()\n    print(f\"Need more data: A={count_a}, B={count_b}\")\n</code></pre></p>"},{"location":"api-reference/testing/ab-test/#complete-workflow","title":"Complete Workflow","text":"<p>Example of a complete usage of the A/B testing framework:</p> <pre><code>from prompt_versioner.testing.ab_test import ABTest\nfrom prompt_versioner import PromptVersioner\nimport random\n\n# 1. Initialize versioner and test\nversioner = PromptVersioner(\"my_prompts.db\")\nab_test = ABTest(\n    versioner=versioner,\n    prompt_name=\"sentiment_analysis\",\n    version_a=\"v1.0.0\",\n    version_b=\"v1.1.0\",\n    metric_name=\"accuracy\"\n)\n\n# 2. Simulate data collection in production\ndef simulate_production_testing():\n    # Simulate results for version A (baseline)\n    for _ in range(100):\n        # Simulate baseline accuracy ~0.85\n        accuracy_a = random.normalvariate(0.85, 0.05)\n        accuracy_a = max(0, min(1, accuracy_a))  # Clamp 0-1\n        ab_test.log_result(\"a\", accuracy_a)\n\n    # Simulate results for version B (improved)\n    for _ in range(100):\n        # Simulate improved accuracy ~0.89\n        accuracy_b = random.normalvariate(0.89, 0.04)\n        accuracy_b = max(0, min(1, accuracy_b))  # Clamp 0-1\n        ab_test.log_result(\"b\", accuracy_b)\n\n# 3. Monitor test progress\ndef monitor_test():\n    count_a, count_b = ab_test.get_sample_counts()\n    print(f\"Progress: A={count_a}, B={count_b}\")\n\n    if ab_test.is_ready(min_samples=30):\n        print(\"\u2713 Enough samples for preliminary analysis\")\n\n        result = ab_test.get_result()\n        if result.confidence &gt; 0.8:\n            print(\"\u2713 High confidence in results\")\n            return True\n        else:\n            print(\"\u26a0\ufe0f  Low confidence, keep collecting data\")\n\n    return False\n\n# 4. Run test\nprint(\"\ud83e\uddea Starting A/B test...\")\nsimulate_production_testing()\n\n# 5. Analyze results\nif monitor_test():\n    print(\"\\n\ud83d\udcca Final results:\")\n    ab_test.print_result()\n\n    result = ab_test.get_result()\n\n    # Decision based on results\n    if result.improvement &gt; 5.0 and result.confidence &gt; 0.8:\n        print(f\"\\n\ud83d\ude80 Promote {result.winner} to production!\")\n    elif result.improvement &lt; 1.0:\n        print(\"\\n\ud83d\udcca No significant difference, keep current version\")\n    else:\n        print(\"\\n\u23f3 Collect more data before deciding\")\n</code></pre>"},{"location":"api-reference/testing/ab-test/#statistical-analysis","title":"Statistical Analysis","text":""},{"location":"api-reference/testing/ab-test/#winner-calculation","title":"Winner Calculation","text":"<p>The winner is determined by comparing means: - Winner: Version with the highest mean - Improvement: <code>|mean_b - mean_a| / mean_a * 100</code></p>"},{"location":"api-reference/testing/ab-test/#confidence-calculation","title":"Confidence Calculation","text":"<p>Confidence is calculated considering: 1. Sample size: <code>min(samples) / 30.0</code> 2. Variance: Penalty for high variance in data 3. Simplified formula: In production use proper t-tests</p> <pre><code>confidence = min(min_samples / 30.0, 1.0)\nif high_variance:\n    confidence *= variance_penalty\n</code></pre>"},{"location":"api-reference/testing/ab-test/#recommendations","title":"Recommendations","text":"<ul> <li>Minimum samples: At least 30 per version for reliable results</li> <li>Significance: Improvement &gt; 5% with confidence &gt; 80%</li> <li>Test duration: Continue until results stabilize</li> <li>Balancing: Keep a 50/50 ratio between versions during the test</li> </ul>"},{"location":"api-reference/testing/ab-test/#integration-with-metrics","title":"Integration with Metrics","text":"<p>The A/B test can use any tracked metric:</p> <pre><code># Test for different metrics\ntest_accuracy = ABTest(versioner, \"classifier\", \"v1\", \"v2\", \"accuracy\")\ntest_latency = ABTest(versioner, \"classifier\", \"v1\", \"v2\", \"latency_ms\")\ntest_cost = ABTest(versioner, \"classifier\", \"v1\", \"v2\", \"cost_eur\")\n\n# Multi-metric analysis\nresults = {\n    \"accuracy\": test_accuracy.get_result(),\n    \"latency\": test_latency.get_result(),\n    \"cost\": test_cost.get_result()\n}\n\nfor metric, result in results.items():\n    print(f\"{metric}: {result.winner} wins by {result.improvement:.1f}%\")\n</code></pre>"},{"location":"api-reference/testing/ab-test/#see-also","title":"See Also","text":"<ul> <li><code>Runner</code> - Framework for testing</li> </ul>"},{"location":"api-reference/testing/runner/","title":"Runner","text":"<p>The <code>prompt_versioner.testing.runner</code> module provides a framework for running automated tests on prompts.</p>"},{"location":"api-reference/testing/runner/#prompttestrunner","title":"PromptTestRunner","text":"<p>Class for running prompt tests with support for parallel execution and metric collection.</p>"},{"location":"api-reference/testing/runner/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, max_workers: int = 4)\n</code></pre> <p>Parameters: - <code>max_workers</code> (int): Maximum number of parallel workers (default: 4)</p>"},{"location":"api-reference/testing/runner/#methods","title":"Methods","text":""},{"location":"api-reference/testing/runner/#run_test","title":"run_test()","text":"<pre><code>def run_test(\n        self,\n        test_case: TestCase,\n        prompt_fn: Callable[[Dict[str, Any]], Any],\n        metric_fn: Optional[Callable[[Any], Dict[str, float]]] = None,\n    ) -&gt; TestResult\n</code></pre> <p>Runs a single test case.</p> <p>Parameters: - <code>test_case</code> (TestCase): Test case to run - <code>prompt_fn</code> (Callable): Function that takes input and returns LLM output - <code>metric_fn</code> (Optional[Callable]): Optional function to compute metrics from output</p> <p>Returns: - <code>TestResult</code>: Object with test results</p> <p>Example: <pre><code>from prompt_versioner.testing.runner import PromptTestRunner\nfrom prompt_versioner.testing.models import TestCase\n\n# Define prompt function\ndef my_prompt_function(inputs: Dict[str, Any]) -&gt; str:\n    prompt = f\"Classify sentiment: {inputs['text']}\"\n    # Simulate LLM call\n    return \"positive\"\n\n# Define metrics function\ndef calculate_metrics(output: str) -&gt; Dict[str, float]:\n    return {\n        \"confidence\": 0.95,\n        \"accuracy\": 1.0 if output in [\"positive\", \"negative\", \"neutral\"] else 0.0\n    }\n\n# Create test case\ntest_case = TestCase(\n    name=\"sentiment_test_1\",\n    inputs={\"text\": \"I love this product!\"},\n    expected_output=\"positive\"\n)\n\n# Run test\nrunner = PromptTestRunner()\nresult = runner.run_test(test_case, my_prompt_function, calculate_metrics)\n\nprint(f\"Success: {result.success}\")\nprint(f\"Output: {result.output}\")\nprint(f\"Duration: {result.duration_ms:.2f}ms\")\nprint(f\"Metrics: {result.metrics}\")\n</code></pre></p>"},{"location":"api-reference/testing/runner/#run_tests","title":"run_tests()","text":"<pre><code>def run_tests(\n        self,\n        test_cases: List[TestCase],\n        prompt_fn: Callable[[Dict[str, Any]], Any],\n        metric_fn: Optional[Callable[[Any], Dict[str, float]]] = None,\n        parallel: bool = True,\n    ) -&gt; List[TestResult]\n</code></pre> <p>Runs multiple test cases sequentially or in parallel.</p> <p>Parameters: - <code>test_cases</code> (List[TestCase]): List of test cases - <code>prompt_fn</code> (Callable): Prompt function - <code>metric_fn</code> (Optional[Callable]): Optional metrics function - <code>parallel</code> (bool): Whether to run tests in parallel (default: True)</p> <p>Returns: - <code>List[TestResult]</code>: List of test results</p> <p>Example: <pre><code># Create multiple test cases\ntest_cases = [\n    TestCase(\n        name=\"positive_sentiment\",\n        inputs={\"text\": \"I love this!\"},\n        expected_output=\"positive\"\n    ),\n    TestCase(\n        name=\"negative_sentiment\",\n        inputs={\"text\": \"This is terrible\"},\n        expected_output=\"negative\"\n    ),\n    TestCase(\n        name=\"neutral_sentiment\",\n        inputs={\"text\": \"It's okay\"},\n        expected_output=\"neutral\"\n    )\n]\n\n# Run all tests\nresults = runner.run_tests(test_cases, my_prompt_function, calculate_metrics)\n\n# Analyze results\nfor result in results:\n    status = \"\u2713\" if result.success else \"\u2717\"\n    print(f\"{status} {result.test_case.name}: {result.output}\")\n</code></pre></p>"},{"location":"api-reference/testing/runner/#get_summary","title":"get_summary()","text":"<pre><code>def get_summary(self, results: List[TestResult]) -&gt; Dict[str, Any]\n</code></pre> <p>Generates a summary of test results.</p> <p>Parameters: - <code>results</code> (List[TestResult]): List of test results</p> <p>Returns: - <code>Dict[str, Any]</code>: Dictionary with statistics:   - <code>total</code>: Total number of tests   - <code>passed</code>: Number of passed tests   - <code>failed</code>: Number of failed tests   - <code>pass_rate</code>: Success rate   - <code>metrics</code>: Aggregated metric statistics</p> <p>Example: <pre><code>summary = runner.get_summary(results)\nprint(f\"Pass rate: {summary['pass_rate']:.2%}\")\nprint(f\"Total: {summary['total']}, Passed: {summary['passed']}, Failed: {summary['failed']}\")\nprint(f\"Metrics: {summary['metrics']}\")\n</code></pre></p>"},{"location":"api-reference/testing/runner/#format_summarysummary-dictstr-any-str","title":"<code>format_summary(summary: Dict[str, Any]) -&gt; str</code>","text":"<p>Formats the summary as readable text.</p> <p>Parameters: - <code>summary</code> (Dict[str, Any]): Summary from <code>get_summary()</code></p> <p>Returns: - <code>str</code>: Formatted summary</p> <p>Example: <pre><code>formatted = runner.format_summary(summary)\nprint(formatted)\n# Output:\n# ================================================================================\n# TEST SUMMARY\n# ================================================================================\n# Total: 3 | Passed: 2 | Failed: 1 | Pass Rate: 66.67%\n#\n# METRICS SUMMARY:\n# - duration_ms: mean=45.23, std=12.45, range=[32.10, 58.90]\n# - accuracy: mean=0.83, std=0.41, range=[0.00, 1.00]\n</code></pre></p>"},{"location":"api-reference/testing/runner/#complete-workflow","title":"Complete Workflow","text":"<p>Example of a complete usage of the test runner:</p> <pre><code>from prompt_versioner.testing.runner import PromptTestRunner\nfrom prompt_versioner.testing.models import TestCase\nfrom typing import Dict, Any\nimport random\n\n# 1. Define prompt function\ndef sentiment_classifier(inputs: Dict[str, Any]) -&gt; str:\n    \"\"\"Simulate a sentiment classifier.\"\"\"\n    text = inputs.get(\"text\", \"\")\n\n    # Simulate classification logic\n    if \"love\" in text.lower() or \"great\" in text.lower():\n        return \"positive\"\n    elif \"hate\" in text.lower() or \"terrible\" in text.lower():\n        return \"negative\"\n    else:\n        return \"neutral\"\n\n# 2. Define metrics function\ndef sentiment_metrics(output: str) -&gt; Dict[str, float]:\n    \"\"\"Compute metrics for sentiment output.\"\"\"\n    valid_sentiments = [\"positive\", \"negative\", \"neutral\"]\n\n    return {\n        \"validity\": 1.0 if output in valid_sentiments else 0.0,\n        \"confidence\": random.uniform(0.7, 0.99),  # Simulate confidence\n        \"response_length\": len(output)\n    }\n\n# 3. Define custom validation function\ndef validate_sentiment(output: str) -&gt; bool:\n    \"\"\"Validate that the output is a valid sentiment.\"\"\"\n    return output in [\"positive\", \"negative\", \"neutral\"]\n\n# 4. Create test dataset\ntest_cases = [\n    TestCase(\n        name=\"clearly_positive\",\n        inputs={\"text\": \"I love this product, it's great!\"},\n        expected_output=\"positive\",\n        validation_fn=validate_sentiment\n    ),\n    TestCase(\n        name=\"clearly_negative\",\n        inputs={\"text\": \"I hate this, it's terrible!\"},\n        expected_output=\"negative\",\n        validation_fn=validate_sentiment\n    ),\n    TestCase(\n        name=\"neutral_text\",\n        inputs={\"text\": \"This is a product.\"},\n        expected_output=\"neutral\",\n        validation_fn=validate_sentiment\n    ),\n    TestCase(\n        name=\"edge_case_empty\",\n        inputs={\"text\": \"\"},\n        expected_output=\"neutral\",\n        validation_fn=validate_sentiment\n    ),\n    TestCase(\n        name=\"mixed_sentiment\",\n        inputs={\"text\": \"I love the design but hate the price\"},\n        # No expected_output, use only validation_fn\n        validation_fn=validate_sentiment\n    )\n]\n\n# 5. Initialize runner and run tests\nprint(\"\ud83e\uddea Starting test suite...\")\nrunner = PromptTestRunner(max_workers=2)\n\n# Sequential tests for debugging\nprint(\"\\n\ud83d\udcdd Sequential execution for debugging:\")\nsequential_results = runner.run_tests(\n    test_cases[:2],\n    sentiment_classifier,\n    sentiment_metrics,\n    parallel=False\n)\n\nfor result in sequential_results:\n    status = \"\u2713\" if result.success else \"\u2717\"\n    print(f\"{status} {result.test_case.name}: {result.output} ({result.duration_ms:.1f}ms)\")\n\n# Parallel tests for speed\nprint(\"\\n\u26a1 Full parallel execution:\")\nall_results = runner.run_tests(\n    test_cases,\n    sentiment_classifier,\n    sentiment_metrics,\n    parallel=True\n)\n\n# 6. Analyze results\nsummary = runner.get_summary(all_results)\nprint(f\"\\n\ud83d\udcca Results:\")\nprint(f\"Pass rate: {summary['pass_rate']:.1%}\")\nprint(f\"Tests passed: {summary['passed']}/{summary['total']}\")\n\n# 7. Show detailed summary\nprint(\"\\n\" + runner.format_summary(summary))\n\n# 8. Error analysis\nfailed_tests = [r for r in all_results if not r.success]\nif failed_tests:\n    print(\"\\n\u274c Failed tests:\")\n    for result in failed_tests:\n        print(f\"- {result.test_case.name}: {result.error or 'Validation failed'}\")\n        print(f\"  Input: {result.test_case.inputs}\")\n        print(f\"  Output: {result.output}\")\n        print(f\"  Expected: {result.test_case.expected_output}\")\n\n# 9. Performance analysis\ndurations = [r.duration_ms for r in all_results if r.duration_ms]\nif durations:\n    avg_duration = sum(durations) / len(durations)\n    max_duration = max(durations)\n    print(f\"\\n\u23f1\ufe0f  Performance:\")\n    print(f\"Average duration: {avg_duration:.1f}ms\")\n    print(f\"Max duration: {max_duration:.1f}ms\")\n</code></pre>"},{"location":"api-reference/testing/runner/#advanced-features","title":"Advanced Features","text":""},{"location":"api-reference/testing/runner/#regression-testing","title":"Regression Testing","text":"<pre><code>def regression_test_suite():\n    \"\"\"Suite for regression tests.\"\"\"\n\n    # Historical test cases that must always pass\n    regression_cases = [\n        TestCase(name=\"regression_1\", inputs={\"text\": \"baseline test\"}),\n        TestCase(name=\"regression_2\", inputs={\"text\": \"edge case\"}),\n        # ... other critical tests\n    ]\n\n    results = runner.run_tests(regression_cases, sentiment_classifier)\n\n    # Ensure all pass\n    all_passed = all(r.success for r in results)\n    if not all_passed:\n        raise AssertionError(\"Regression test failed!\")\n\n    return results\n</code></pre>"},{"location":"api-reference/testing/runner/#performance-testing","title":"Performance Testing","text":"<pre><code>def performance_benchmark():\n    \"\"\"Performance benchmark.\"\"\"\n\n    # Create many test cases for stress testing\n    stress_cases = [\n        TestCase(\n            name=f\"perf_test_{i}\",\n            inputs={\"text\": f\"Performance test number {i}\"}\n        )\n        for i in range(100)\n    ]\n\n    # Measure performance\n    start_time = time.time()\n    results = runner.run_tests(stress_cases, sentiment_classifier, parallel=True)\n    total_time = time.time() - start_time\n\n    # Analyze performance metrics\n    durations = [r.duration_ms for r in results]\n    throughput = len(results) / total_time\n\n    print(f\"Throughput: {throughput:.1f} tests/sec\")\n    print(f\"Average latency: {sum(durations)/len(durations):.1f}ms\")\n\n    return results\n</code></pre>"},{"location":"api-reference/testing/runner/#cicd-integration","title":"CI/CD Integration","text":"<pre><code>def ci_test_suite() -&gt; bool:\n    \"\"\"Test suite for CI/CD pipeline.\"\"\"\n    try:\n        # Run all tests\n        results = runner.run_tests(all_test_cases, prompt_function)\n        summary = runner.get_summary(results)\n\n        # Success criteria for CI\n        success_criteria = {\n            \"min_pass_rate\": 0.95,  # 95% of tests must pass\n            \"max_avg_duration\": 1000,  # Max 1 second on average\n        }\n\n        pass_rate = summary[\"pass_rate\"]\n        avg_duration = summary[\"metrics\"][\"duration_ms\"][\"mean\"]\n\n        # Check criteria\n        if pass_rate &lt; success_criteria[\"min_pass_rate\"]:\n            print(f\"\u274c Pass rate too low: {pass_rate:.1%}\")\n            return False\n\n        if avg_duration &gt; success_criteria[\"max_avg_duration\"]:\n            print(f\"\u274c Average duration too high: {avg_duration:.1f}ms\")\n            return False\n\n        print(f\"\u2705 CI test passed: {pass_rate:.1%} pass rate, {avg_duration:.1f}ms avg\")\n        return True\n\n    except Exception as e:\n        print(f\"\u274c CI test failed: {e}\")\n        return False\n\n# Usage in CI\nif __name__ == \"__main__\":\n    import sys\n    success = ci_test_suite()\n    sys.exit(0 if success else 1)\n</code></pre>"},{"location":"api-reference/testing/runner/#error-handling","title":"Error Handling","text":"<p>The runner automatically handles:</p> <ul> <li>Timeouts: Timeout for hanging tests</li> <li>Exceptions: Catches exceptions and logs them as errors</li> <li>Validation: Supports custom validation via <code>validation_fn</code></li> <li>Metrics: Collects metrics even in case of partial errors</li> </ul>"},{"location":"api-reference/testing/runner/#error-handling-example","title":"Error Handling Example","text":"<pre><code>def robust_prompt_function(inputs: Dict[str, Any]) -&gt; str:\n    \"\"\"Prompt function with error handling.\"\"\"\n    try:\n        text = inputs.get(\"text\", \"\")\n        if not text.strip():\n            raise ValueError(\"Empty input text\")\n\n        # Simulate possible failure\n        if \"error\" in text.lower():\n            raise RuntimeError(\"Simulated LLM error\")\n\n        return sentiment_classifier(inputs)\n\n    except Exception as e:\n        # Log the error but continue with a default value\n        print(f\"Error in prompt function: {e}\")\n        return \"neutral\"  # Default safe value\n\n# The runner will still catch any unhandled exceptions\n</code></pre>"},{"location":"api-reference/testing/runner/#see-also","title":"See Also","text":"<ul> <li><code>A/B Test</code> - AB Testing for testing</li> </ul>"},{"location":"api-reference/web/controllers/","title":"Controllers","text":"<p>Il modulo <code>prompt_versioner.app.controllers</code> contiene i controller Flask per gestire le route API del dashboard web.</p>"},{"location":"api-reference/web/controllers/#prompts_viewpy","title":"prompts_view.py","text":"<p>Blueprint per le route relative ai prompt.</p>"},{"location":"api-reference/web/controllers/#prompts_bp","title":"prompts_bp","text":"<p>Blueprint Flask registrato con prefisso <code>/api/prompts</code>.</p>"},{"location":"api-reference/web/controllers/#route","title":"Route","text":""},{"location":"api-reference/web/controllers/#get-apiprompts","title":"<code>GET /api/prompts</code>","text":"<p>Ottiene tutti i prompt con metadati e statistiche.</p> <p>Returns: - <code>200</code>: JSON con statistiche globali dei prompt - <code>500</code>: Errore interno del server</p> <p>Risposta: <pre><code>{\n  \"total_prompts\": 10,\n  \"total_versions\": 45,\n  \"total_calls\": 1250,\n  \"avg_quality\": 0.87,\n  \"avg_cost\": 0.0023,\n  \"avg_latency\": 350.5\n}\n</code></pre></p> <p>Esempio: <pre><code>import requests\n\nresponse = requests.get(\"http://localhost:5000/api/prompts\")\ndata = response.json()\nprint(f\"Total prompts: {data['total_prompts']}\")\n</code></pre></p>"},{"location":"api-reference/web/controllers/#get-apipromptsnamestats","title":"<code>GET /api/prompts/&lt;name&gt;/stats</code>","text":"<p>Ottiene statistiche aggregate per un prompt specifico.</p> <p>Parametri: - <code>name</code> (str): Nome del prompt</p> <p>Returns: - <code>200</code>: JSON con statistiche del prompt - <code>404</code>: Prompt non trovato - <code>500</code>: Errore interno del server</p> <p>Risposta: <pre><code>{\n  \"name\": \"email_classifier\",\n  \"version_count\": 5,\n  \"total_calls\": 234,\n  \"latest_version\": \"v1.2.0\",\n  \"metrics\": {\n    \"avg_accuracy\": 0.89,\n    \"avg_cost\": 0.0015,\n    \"avg_latency\": 280.3\n  }\n}\n</code></pre></p> <p>Esempio: <pre><code>response = requests.get(\"http://localhost:5000/api/prompts/email_classifier/stats\")\nif response.status_code == 200:\n    stats = response.json()\n    print(f\"Accuracy: {stats['metrics']['avg_accuracy']:.2%}\")\n</code></pre></p>"},{"location":"api-reference/web/controllers/#get-apipromptsnameab-tests","title":"<code>GET /api/prompts/&lt;name&gt;/ab-tests</code>","text":"<p>Ottiene versioni disponibili per A/B testing.</p> <p>Parametri: - <code>name</code> (str): Nome del prompt</p> <p>Returns: - <code>200</code>: JSON con versioni testabili - <code>500</code>: Errore interno del server</p> <p>Criteri per A/B Testing: - La versione deve avere almeno <code>MIN_CALLS_FOR_AB_TEST</code> chiamate (configurabile) - Devono essere disponibili metriche sufficienti</p> <p>Risposta: <pre><code>[\n  {\n    \"version\": \"v1.1.0\",\n    \"timestamp\": \"2024-01-15T10:30:00Z\",\n    \"call_count\": 150,\n    \"avg_quality\": 0.87,\n    \"avg_cost\": 0.0020,\n    \"avg_latency\": 320.5\n  },\n  {\n    \"version\": \"v1.2.0\",\n    \"timestamp\": \"2024-01-20T14:15:00Z\",\n    \"call_count\": 84,\n    \"avg_quality\": 0.91,\n    \"avg_cost\": 0.0018,\n    \"avg_latency\": 280.3\n  }\n]\n</code></pre></p> <p>Esempio: <pre><code>response = requests.get(\"http://localhost:5000/api/prompts/email_classifier/ab-tests\")\nversions = response.json()\n\nfor version in versions:\n    print(f\"Version {version['version']}: {version['call_count']} calls\")\n    print(f\"  Quality: {version['avg_quality']:.2%}\")\n    print(f\"  Cost: \u20ac{version['avg_cost']:.4f}\")\n</code></pre></p>"},{"location":"api-reference/web/controllers/#delete-apipromptsname","title":"<code>DELETE /api/prompts/&lt;name&gt;</code>","text":"<p>Elimina un prompt e tutte le sue versioni e dati correlati.</p> <p>Parametri: - <code>name</code> (str): Nome del prompt da eliminare</p> <p>Returns: - <code>200</code>: JSON con conferma di eliminazione - <code>404</code>: Prompt non trovato - <code>500</code>: Errore interno del server</p> <p>Risposta Successo: <pre><code>{\n  \"success\": true,\n  \"message\": \"Prompt 'email_classifier' and all its versions deleted.\"\n}\n</code></pre></p> <p>Risposta Errore: <pre><code>{\n  \"success\": false,\n  \"error\": \"Prompt not found.\"\n}\n</code></pre></p> <p>Esempio: <pre><code>response = requests.delete(\"http://localhost:5000/api/prompts/old_prompt\")\nresult = response.json()\n\nif result[\"success\"]:\n    print(f\"\u2713 {result['message']}\")\nelse:\n    print(f\"\u2717 Error: {result['error']}\")\n</code></pre></p>"},{"location":"api-reference/web/controllers/#versions_viewpy","title":"versions_view.py","text":"<p>Blueprint per le route relative alle versioni.</p>"},{"location":"api-reference/web/controllers/#versions_bp","title":"versions_bp","text":"<p>Blueprint Flask registrato con prefisso <code>/api/versions</code>.</p>"},{"location":"api-reference/web/controllers/#route-principali","title":"Route Principali","text":""},{"location":"api-reference/web/controllers/#get-apiversionsprompt_name","title":"<code>GET /api/versions/&lt;prompt_name&gt;</code>","text":"<p>Ottiene tutte le versioni per un prompt.</p>"},{"location":"api-reference/web/controllers/#get-apiversionsprompt_nameversion","title":"<code>GET /api/versions/&lt;prompt_name&gt;/&lt;version&gt;</code>","text":"<p>Ottiene dettagli di una versione specifica.</p>"},{"location":"api-reference/web/controllers/#post-apiversionsprompt_namecompare","title":"<code>POST /api/versions/&lt;prompt_name&gt;/compare</code>","text":"<p>Confronta due versioni di un prompt.</p>"},{"location":"api-reference/web/controllers/#alerts_viewpy","title":"alerts_view.py","text":"<p>Blueprint per le route relative agli avvisi.</p>"},{"location":"api-reference/web/controllers/#alerts_bp","title":"alerts_bp","text":"<p>Blueprint Flask registrato con prefisso <code>/api/alerts</code>.</p>"},{"location":"api-reference/web/controllers/#route-principali_1","title":"Route Principali","text":""},{"location":"api-reference/web/controllers/#get-apialerts","title":"<code>GET /api/alerts</code>","text":"<p>Ottiene tutti gli avvisi attivi.</p>"},{"location":"api-reference/web/controllers/#post-apialertsalert_idacknowledge","title":"<code>POST /api/alerts/&lt;alert_id&gt;/acknowledge</code>","text":"<p>Conferma un avviso.</p>"},{"location":"api-reference/web/controllers/#get-apialertsconfig","title":"<code>GET /api/alerts/config</code>","text":"<p>Ottiene configurazione degli avvisi.</p>"},{"location":"api-reference/web/controllers/#export_import_viewpy","title":"export_import_view.py","text":"<p>Blueprint per le route di import/export.</p>"},{"location":"api-reference/web/controllers/#export_import_bp","title":"export_import_bp","text":"<p>Blueprint Flask registrato con prefisso <code>/api/export</code>.</p>"},{"location":"api-reference/web/controllers/#route-principali_2","title":"Route Principali","text":""},{"location":"api-reference/web/controllers/#get-apiexportprompts","title":"<code>GET /api/export/prompts</code>","text":"<p>Esporta tutti i prompt.</p>"},{"location":"api-reference/web/controllers/#post-apiexportimport","title":"<code>POST /api/export/import</code>","text":"<p>Importa prompt da file.</p>"},{"location":"api-reference/web/controllers/#utilizzo-completo","title":"Utilizzo Completo","text":""},{"location":"api-reference/web/controllers/#client-api-completo","title":"Client API Completo","text":"<pre><code>import requests\nfrom typing import Dict, Any, List\n\nclass PromptVersionerClient:\n    \"\"\"Client per API del dashboard Prompt Versioner.\"\"\"\n\n    def __init__(self, base_url: str = \"http://localhost:5000\"):\n        self.base_url = base_url.rstrip('/')\n\n    def get_all_prompts(self) -&gt; Dict[str, Any]:\n        \"\"\"Ottiene statistiche di tutti i prompt.\"\"\"\n        response = requests.get(f\"{self.base_url}/api/prompts\")\n        response.raise_for_status()\n        return response.json()\n\n    def get_prompt_stats(self, name: str) -&gt; Dict[str, Any]:\n        \"\"\"Ottiene statistiche di un prompt specifico.\"\"\"\n        response = requests.get(f\"{self.base_url}/api/prompts/{name}/stats\")\n        response.raise_for_status()\n        return response.json()\n\n    def get_ab_test_versions(self, name: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"Ottiene versioni disponibili per A/B test.\"\"\"\n        response = requests.get(f\"{self.base_url}/api/prompts/{name}/ab-tests\")\n        response.raise_for_status()\n        return response.json()\n\n    def delete_prompt(self, name: str) -&gt; Dict[str, Any]:\n        \"\"\"Elimina un prompt.\"\"\"\n        response = requests.delete(f\"{self.base_url}/api/prompts/{name}\")\n        response.raise_for_status()\n        return response.json()\n\n# Esempio di utilizzo\nclient = PromptVersionerClient(\"http://localhost:5000\")\n\n# Ottieni overview\noverview = client.get_all_prompts()\nprint(f\"Prompts totali: {overview['total_prompts']}\")\n\n# Analizza prompt specifico\nstats = client.get_prompt_stats(\"email_classifier\")\nprint(f\"Versioni: {stats['version_count']}\")\nprint(f\"Chiamate totali: {stats['total_calls']}\")\n\n# Controlla versioni per A/B test\nab_versions = client.get_ab_test_versions(\"email_classifier\")\nif len(ab_versions) &gt;= 2:\n    print(\"\u2713 Pronto per A/B testing\")\n    for v in ab_versions:\n        print(f\"  {v['version']}: {v['call_count']} calls\")\nelse:\n    print(\"\u26a0\ufe0f  Servono pi\u00f9 versioni per A/B testing\")\n</code></pre>"},{"location":"api-reference/web/controllers/#dashboard-monitoring","title":"Dashboard Monitoring","text":"<pre><code>import time\nfrom datetime import datetime\n\ndef monitor_dashboard(client: PromptVersionerClient, interval: int = 60):\n    \"\"\"Monitora lo stato del dashboard.\"\"\"\n\n    while True:\n        try:\n            # Ottieni statistiche globali\n            stats = client.get_all_prompts()\n            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n            print(f\"\\n[{timestamp}] Dashboard Status:\")\n            print(f\"  Prompts: {stats['total_prompts']}\")\n            print(f\"  Versions: {stats['total_versions']}\")\n            print(f\"  Total calls: {stats['total_calls']}\")\n            print(f\"  Avg quality: {stats['avg_quality']:.2%}\")\n            print(f\"  Avg cost: \u20ac{stats['avg_cost']:.4f}\")\n            print(f\"  Avg latency: {stats['avg_latency']:.1f}ms\")\n\n            # Controlla se ci sono problemi\n            if stats['avg_quality'] &lt; 0.8:\n                print(\"  \u26a0\ufe0f  Quality sotto soglia!\")\n            if stats['avg_latency'] &gt; 1000:\n                print(\"  \u26a0\ufe0f  Latenza alta!\")\n\n        except Exception as e:\n            print(f\"\u274c Errore monitoring: {e}\")\n\n        time.sleep(interval)\n\n# Avvia monitoring\n# monitor_dashboard(client, interval=30)\n</code></pre>"},{"location":"api-reference/web/controllers/#gestione-errori","title":"Gestione Errori","text":"<pre><code>def safe_api_call(func, *args, **kwargs):\n    \"\"\"Wrapper per chiamate API sicure.\"\"\"\n    try:\n        return func(*args, **kwargs)\n    except requests.exceptions.ConnectionError:\n        print(\"\u274c Impossibile connettersi al server\")\n        return None\n    except requests.exceptions.HTTPError as e:\n        if e.response.status_code == 404:\n            print(\"\u274c Risorsa non trovata\")\n        elif e.response.status_code == 500:\n            print(\"\u274c Errore interno del server\")\n        else:\n            print(f\"\u274c Errore HTTP: {e.response.status_code}\")\n        return None\n    except Exception as e:\n        print(f\"\u274c Errore generico: {e}\")\n        return None\n\n# Utilizzo sicuro\nstats = safe_api_call(client.get_prompt_stats, \"nonexistent_prompt\")\nif stats:\n    print(\"Stats ottenute con successo\")\nelse:\n    print(\"Impossibile ottenere stats\")\n</code></pre>"},{"location":"api-reference/web/controllers/#configurazione","title":"Configurazione","text":""},{"location":"api-reference/web/controllers/#configurazione-minima-per-ab-testing","title":"Configurazione Minima per A/B Testing","text":"<pre><code># config.py\nclass Config:\n    MIN_CALLS_FOR_AB_TEST = 30  # Minimo 30 chiamate per A/B test\n    MAX_AB_TEST_VERSIONS = 10   # Massimo 10 versioni in lista\n    CACHE_TIMEOUT = 300         # Cache risultati per 5 minuti\n</code></pre>"},{"location":"api-reference/web/controllers/#middleware-per-logging","title":"Middleware per Logging","text":"<pre><code>from flask import request\nimport logging\n\n@prompts_bp.before_request\ndef log_prompt_request():\n    \"\"\"Log delle richieste ai prompt.\"\"\"\n    logging.info(f\"Prompt API: {request.method} {request.path}\")\n\n@prompts_bp.after_request\ndef log_prompt_response(response):\n    \"\"\"Log delle risposte ai prompt.\"\"\"\n    logging.info(f\"Prompt API Response: {response.status_code}\")\n    return response\n</code></pre>"},{"location":"api-reference/web/controllers/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from flask_limiter import Limiter\n\n# Applica rate limiting ai prompt API\n@prompts_bp.route(\"\", methods=[\"GET\"])\n@limiter.limit(\"100 per hour\")  # Max 100 richieste/ora\ndef get_prompts():\n    # ... logica esistente\n</code></pre>"},{"location":"api-reference/web/flask-builder/","title":"Flask Builder","text":"<p>Il modulo <code>prompt_versioner.app.flask_builder</code> fornisce la factory per creare l'applicazione web Flask.</p>"},{"location":"api-reference/web/flask-builder/#create_app","title":"create_app","text":"<p>Funzione factory per creare e configurare l'applicazione Flask del dashboard web.</p>"},{"location":"api-reference/web/flask-builder/#sintassi","title":"Sintassi","text":"<pre><code>create_app(versioner: Any, config_name: str | None = None) -&gt; Flask\n</code></pre> <p>Parametri: - <code>versioner</code> (Any): Istanza di PromptVersioner - <code>config_name</code> (str | None): Nome della configurazione ('development', 'production', o 'default')</p> <p>Returns: - <code>Flask</code>: Applicazione Flask configurata</p>"},{"location":"api-reference/web/flask-builder/#configurazioni-disponibili","title":"Configurazioni Disponibili","text":"<p>L'applicazione supporta diverse configurazioni:</p> <ul> <li><code>development</code>: Modalit\u00e0 di sviluppo con debug abilitato</li> <li><code>production</code>: Modalit\u00e0 di produzione ottimizzata</li> <li><code>default</code>: Configurazione di default</li> </ul> <p>La configurazione viene selezionata automaticamente dalla variabile d'ambiente <code>FLASK_ENV</code> se non specificata.</p>"},{"location":"api-reference/web/flask-builder/#servizi-inizializzati","title":"Servizi Inizializzati","text":"<p>L'applicazione inizializza automaticamente i seguenti servizi:</p> <ul> <li><code>MetricsService</code>: Gestione e analisi delle metriche</li> <li><code>DiffService</code>: Confronto tra versioni</li> <li><code>AlertService</code>: Sistema di notifiche e avvisi</li> </ul>"},{"location":"api-reference/web/flask-builder/#blueprint-registrati","title":"Blueprint Registrati","text":"<p>L'applicazione registra i seguenti blueprint:</p> <ul> <li><code>prompts_bp</code>: Gestione dei prompt</li> <li><code>versions_bp</code>: Gestione delle versioni</li> <li><code>alerts_bp</code>: Gestione degli avvisi</li> <li><code>export_import_bp</code>: Import/export dei dati</li> </ul>"},{"location":"api-reference/web/flask-builder/#route-principali","title":"Route Principali","text":""},{"location":"api-reference/web/flask-builder/#get","title":"<code>/</code> (GET)","text":"<p>Route principale che renderizza la dashboard.</p> <p>Returns: - Template <code>dashboard.html</code> con interfaccia principale</p>"},{"location":"api-reference/web/flask-builder/#gestione-errori","title":"Gestione Errori","text":"<p>L'applicazione include gestori per errori comuni:</p>"},{"location":"api-reference/web/flask-builder/#404-not-found","title":"404 - Not Found","text":"<p>Returns: - <code>{\"error\": \"Not found\"}</code> con status code 404</p>"},{"location":"api-reference/web/flask-builder/#500-internal-server-error","title":"500 - Internal Server Error","text":"<p>Returns: - <code>{\"error\": \"Internal server error\"}</code> con status code 500</p>"},{"location":"api-reference/web/flask-builder/#esempio-di-utilizzo","title":"Esempio di Utilizzo","text":""},{"location":"api-reference/web/flask-builder/#utilizzo-base","title":"Utilizzo Base","text":"<pre><code>from prompt_versioner.app.flask_builder import create_app\nfrom prompt_versioner import PromptVersioner\n\n# Inizializza versioner\nversioner = PromptVersioner(\"my_prompts.db\")\n\n# Crea app Flask\napp = create_app(versioner, config_name=\"development\")\n\n# Avvia server di sviluppo\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n</code></pre>"},{"location":"api-reference/web/flask-builder/#utilizzo-con-configurazione-personalizzata","title":"Utilizzo con Configurazione Personalizzata","text":"<pre><code>import os\nfrom prompt_versioner.app.flask_builder import create_app\nfrom prompt_versioner import PromptVersioner\n\n# Configura ambiente\nos.environ[\"FLASK_ENV\"] = \"production\"\n\n# Inizializza con database esistente\nversioner = PromptVersioner(\"production_prompts.db\")\n\n# Crea app per produzione\napp = create_app(versioner)\n\n# L'app \u00e8 pronta per deployment con WSGI server\n</code></pre>"},{"location":"api-reference/web/flask-builder/#accesso-ai-servizi","title":"Accesso ai Servizi","text":"<pre><code># All'interno dell'applicazione Flask, i servizi sono accessibili tramite app\n\n@app.route(\"/api/custom-metrics\")\ndef custom_metrics():\n    # Accesso al MetricsService\n    metrics_service = app.metrics_service\n    stats = metrics_service.get_overall_stats()\n    return {\"stats\": stats}\n\n@app.route(\"/api/compare/&lt;version_a&gt;/&lt;version_b&gt;\")\ndef compare_versions(version_a: str, version_b: str):\n    # Accesso al DiffService\n    diff_service = app.diff_service\n    diff = diff_service.compare_versions(version_a, version_b)\n    return {\"diff\": diff}\n</code></pre>"},{"location":"api-reference/web/flask-builder/#deployment-con-gunicorn","title":"Deployment con Gunicorn","text":"<pre><code># app.py\nfrom prompt_versioner.app.flask_builder import create_app\nfrom prompt_versioner import PromptVersioner\n\n# Crea app per produzione\nversioner = PromptVersioner(\"prompts.db\")\napp = create_app(versioner, \"production\")\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre> <pre><code># Comando per avvio con Gunicorn\ngunicorn -w 4 -b 0.0.0.0:5000 app:app\n</code></pre>"},{"location":"api-reference/web/flask-builder/#deployment-con-docker","title":"Deployment con Docker","text":"<pre><code># Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\nCOPY . .\nRUN pip install -r requirements.txt\n\n# Esponi porta\nEXPOSE 5000\n\n# Avvia applicazione\nCMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"app:app\"]\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  prompt-versioner:\n    build: .\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./data:/app/data\n    environment:\n      - FLASK_ENV=production\n</code></pre>"},{"location":"api-reference/web/flask-builder/#personalizzazione","title":"Personalizzazione","text":""},{"location":"api-reference/web/flask-builder/#aggiunta-di-route-personalizzate","title":"Aggiunta di Route Personalizzate","text":"<pre><code>from prompt_versioner.app.flask_builder import create_app\n\n# Crea app base\napp = create_app(versioner)\n\n# Aggiungi route personalizzate\n@app.route(\"/api/health\")\ndef health_check():\n    return {\"status\": \"healthy\", \"version\": \"1.0.0\"}\n\n@app.route(\"/api/custom-dashboard\")\ndef custom_dashboard():\n    # Logica personalizzata per dashboard\n    versioner = app.versioner\n    prompts = versioner.list_prompts()\n    return {\"prompts\": prompts}\n</code></pre>"},{"location":"api-reference/web/flask-builder/#middleware-personalizzato","title":"Middleware Personalizzato","text":"<pre><code>from flask import request, jsonify\nimport time\n\n# Aggiungi middleware per logging delle richieste\n@app.before_request\ndef log_request():\n    request.start_time = time.time()\n\n@app.after_request\ndef log_response(response):\n    duration = time.time() - request.start_time\n    print(f\"{request.method} {request.path} - {response.status_code} ({duration:.3f}s)\")\n    return response\n</code></pre>"},{"location":"api-reference/web/flask-builder/#configurazione-personalizzata","title":"Configurazione Personalizzata","text":"<pre><code># custom_config.py\nimport os\n\nclass CustomConfig:\n    SECRET_KEY = os.environ.get('SECRET_KEY') or 'dev-secret-key'\n    TEMPLATE_FOLDER = 'custom_templates'\n    STATIC_FOLDER = 'custom_static'\n\n    # Configurazioni personalizzate\n    CUSTOM_FEATURE_ENABLED = True\n    API_RATE_LIMIT = 100\n\n# Usa configurazione personalizzata\napp = create_app(versioner)\napp.config.from_object(CustomConfig)\n</code></pre>"},{"location":"api-reference/web/flask-builder/#integrazione-con-autenticazione","title":"Integrazione con Autenticazione","text":"<pre><code>from flask_login import LoginManager, login_required\n\n# Aggiungi autenticazione\ndef create_authenticated_app(versioner):\n    app = create_app(versioner)\n\n    # Configura LoginManager\n    login_manager = LoginManager()\n    login_manager.init_app(app)\n    login_manager.login_view = 'auth.login'\n\n    # Proteggi route sensibili\n    @app.before_request\n    def require_auth():\n        if request.endpoint and not request.endpoint.startswith('auth.'):\n            # Richiedi autenticazione per tutte le route eccetto auth\n            pass\n\n    return app\n</code></pre>"},{"location":"api-reference/web/flask-builder/#monitoraggio-e-logging","title":"Monitoraggio e Logging","text":"<pre><code>import logging\nfrom flask.logging import default_handler\n\ndef configure_logging(app):\n    # Rimuovi handler default in produzione\n    if app.config.get('ENV') == 'production':\n        app.logger.removeHandler(default_handler)\n\n    # Configura logging personalizzato\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(\n        '[%(asctime)s] %(levelname)s in %(module)s: %(message)s'\n    ))\n    app.logger.addHandler(handler)\n    app.logger.setLevel(logging.INFO)\n\n# Usa con l'app\napp = create_app(versioner)\nconfigure_logging(app)\n</code></pre>"},{"location":"api-reference/web/flask-builder/#struttura-dellapplicazione","title":"Struttura dell'Applicazione","text":"<p>L'applicazione Flask creata dalla factory ha la seguente struttura:</p> <pre><code>Flask App\n\u251c\u2500\u2500 Templates (dashboard.html, etc.)\n\u251c\u2500\u2500 Static Files (CSS, JS, images)\n\u251c\u2500\u2500 Services\n\u2502   \u251c\u2500\u2500 MetricsService (analisi metriche)\n\u2502   \u251c\u2500\u2500 DiffService (confronti versioni)\n\u2502   \u2514\u2500\u2500 AlertService (notifiche)\n\u251c\u2500\u2500 Controllers (Blueprint)\n\u2502   \u251c\u2500\u2500 prompts_bp (gestione prompt)\n\u2502   \u251c\u2500\u2500 versions_bp (gestione versioni)\n\u2502   \u251c\u2500\u2500 alerts_bp (gestione avvisi)\n\u2502   \u2514\u2500\u2500 export_import_bp (import/export)\n\u2514\u2500\u2500 Error Handlers (404, 500)\n</code></pre>"},{"location":"api-reference/web/flask-builder/#sicurezza","title":"Sicurezza","text":""},{"location":"api-reference/web/flask-builder/#configurazione-sicura","title":"Configurazione Sicura","text":"<pre><code>class ProductionConfig:\n    SECRET_KEY = os.environ.get('SECRET_KEY')  # Sempre da variabile d'ambiente\n    SESSION_COOKIE_SECURE = True  # Solo HTTPS\n    SESSION_COOKIE_HTTPONLY = True  # Proteggi da XSS\n    SESSION_COOKIE_SAMESITE = 'Lax'  # Protezione CSRF\n\n    # Configurazioni sicurezza aggiuntive\n    WTF_CSRF_ENABLED = True\n    WTF_CSRF_TIME_LIMIT = 3600\n</code></pre>"},{"location":"api-reference/web/flask-builder/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from flask_limiter import Limiter\nfrom flask_limiter.util import get_remote_address\n\ndef add_rate_limiting(app):\n    limiter = Limiter(\n        app,\n        key_func=get_remote_address,\n        default_limits=[\"200 per day\", \"50 per hour\"]\n    )\n\n    # Rate limit specifici per API\n    @app.route(\"/api/metrics\")\n    @limiter.limit(\"10 per minute\")\n    def api_metrics():\n        return app.metrics_service.get_summary()\n</code></pre>"},{"location":"examples/advanced-workflows/","title":"Advanced Workflows","text":"<p>Explore advanced patterns and workflows with Prompt Versioner for complex use cases.</p>"},{"location":"examples/advanced-workflows/#automated-version-management","title":"\ud83d\ude80 Automated Version Management","text":""},{"location":"examples/advanced-workflows/#continuous-integration-pipeline","title":"Continuous Integration Pipeline","text":"<pre><code>from prompt_versioner import PromptVersioner, VersionBump\nimport os\n\ndef ci_pipeline_versioning():\n    \"\"\"Automated versioning in CI/CD pipeline\"\"\"\n\n    pv = PromptVersioner(project_name=\"production-ai\", enable_git=True)\n\n    # Get current branch and commit info\n    branch = os.getenv(\"CI_BRANCH\", \"main\")\n    commit_hash = os.getenv(\"CI_COMMIT_SHA\", \"unknown\")\n\n    # Automatic version bump based on branch\n    if branch == \"main\":\n        bump_type = VersionBump.PATCH\n    elif branch.startswith(\"feature/\"):\n        bump_type = VersionBump.MINOR\n    elif branch.startswith(\"breaking/\"):\n        bump_type = VersionBump.MAJOR\n    else:\n        bump_type = VersionBump.PATCH\n\n    # Create version with CI context\n    version_id = pv.save_version(\n        name=\"production_assistant\",\n        system_prompt=load_prompt_from_file(\"prompts/system.txt\"),\n        user_prompt=load_prompt_from_file(\"prompts/user.txt\"),\n        bump_type=bump_type,\n        metadata={\n            \"ci_build\": True,\n            \"branch\": branch,\n            \"commit\": commit_hash,\n            \"pipeline_id\": os.getenv(\"CI_PIPELINE_ID\"),\n            \"automated\": True\n        }\n    )\n\n    print(f\"\u2705 CI: Created version {version_id} for branch {branch}\")\n    return version_id\n\ndef load_prompt_from_file(filepath):\n    \"\"\"Load prompt content from file\"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        return f.read().strip()\n\n# Run in CI\nif os.getenv(\"CI\"):\n    ci_pipeline_versioning()\n</code></pre>"},{"location":"examples/advanced-workflows/#auto-versioning-with-decorators","title":"Auto-versioning with Decorators","text":"<pre><code>from functools import wraps\nimport hashlib\n\ndef auto_version(prompt_name, check_changes=True):\n    \"\"\"Decorator for automatic versioning when prompt logic changes\"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Execute function to get prompts\n            result = func(*args, **kwargs)\n\n            # Extract prompts\n            if isinstance(result, dict):\n                system_prompt = result.get(\"system\", \"\")\n                user_prompt = result.get(\"user\", \"\")\n            else:\n                return result\n\n            # Check if prompts changed (optional optimization)\n            if check_changes:\n                content_hash = hashlib.md5(\n                    (system_prompt + user_prompt).encode()\n                ).hexdigest()\n\n                latest = pv.get_latest(prompt_name)\n                if latest and latest.get(\"content_hash\") == content_hash:\n                    return result  # No changes, skip versioning\n\n            # Auto-version the prompts\n            pv.save_version(\n                name=prompt_name,\n                system_prompt=system_prompt,\n                user_prompt=user_prompt,\n                bump_type=VersionBump.PATCH,\n                metadata={\n                    \"auto_generated\": True,\n                    \"function\": func.__name__,\n                    \"content_hash\": content_hash\n                }\n            )\n\n            return result\n\n        return wrapper\n    return decorator\n\n# Usage\n@auto_version(\"dynamic_assistant\")\ndef create_context_aware_prompt(user_type, domain):\n    \"\"\"Create prompts based on context\"\"\"\n\n    system_prompt = f\"You are an AI assistant specialized in {domain} for {user_type} users.\"\n    user_prompt = f\"Help with {domain}-related questions. User type: {user_type}. Question: {{question}}\"\n\n    return {\n        \"system\": system_prompt,\n        \"user\": user_prompt\n    }\n\n# This will auto-version when called with different parameters\ncreate_context_aware_prompt(\"developer\", \"software engineering\")\ncreate_context_aware_prompt(\"designer\", \"user experience\")\n</code></pre>"},{"location":"examples/advanced-workflows/#advanced-testing-patterns","title":"\ud83e\uddea Advanced Testing Patterns","text":""},{"location":"examples/advanced-workflows/#multi-environment-testing","title":"Multi-Environment Testing","text":"<pre><code>class MultiEnvironmentTester:\n    \"\"\"Test prompts across multiple environments\"\"\"\n\n    def __init__(self, environments):\n        self.environments = environments\n        self.results = {}\n\n    def test_across_environments(self, prompt_name, version, test_cases):\n        \"\"\"Test same prompt version across different environments\"\"\"\n\n        for env_name, env_config in self.environments.items():\n            print(f\"\ud83e\uddea Testing in {env_name} environment...\")\n\n            # Initialize versioner for this environment\n            pv = PromptVersioner(\n                project_name=f\"{prompt_name}_{env_name}\",\n                db_path=env_config[\"db_path\"]\n            )\n\n            env_results = []\n\n            for test_case in test_cases:\n                with pv.test_version(prompt_name, version) as test:\n                    # Simulate LLM call with environment-specific settings\n                    result = self.simulate_llm_call(\n                        test_case,\n                        env_config\n                    )\n\n                    test.log(\n                        tokens=result[\"tokens\"],\n                        cost=result[\"cost\"],\n                        latency_ms=result[\"latency\"],\n                        quality_score=result[\"quality\"],\n                        metadata={\n                            \"environment\": env_name,\n                            \"test_case\": test_case[\"name\"]\n                        }\n                    )\n\n                    env_results.append(result)\n\n            self.results[env_name] = env_results\n\n        return self.generate_comparison_report()\n\n    def simulate_llm_call(self, test_case, env_config):\n        \"\"\"Simulate LLM call with environment-specific behavior\"\"\"\n        import random\n\n        # Simulate different performance based on environment\n        base_latency = env_config.get(\"base_latency\", 300)\n        base_quality = env_config.get(\"base_quality\", 0.85)\n\n        return {\n            \"tokens\": random.randint(100, 300),\n            \"cost\": random.uniform(0.001, 0.005),\n            \"latency\": base_latency + random.uniform(-50, 100),\n            \"quality\": base_quality + random.uniform(-0.1, 0.1),\n            \"success\": random.random() &gt; 0.05  # 95% success rate\n        }\n\n    def generate_comparison_report(self):\n        \"\"\"Generate comparison report across environments\"\"\"\n\n        report = {\"summary\": {}, \"details\": self.results}\n\n        for env_name, results in self.results.items():\n            avg_quality = sum(r[\"quality\"] for r in results) / len(results)\n            avg_latency = sum(r[\"latency\"] for r in results) / len(results)\n            avg_cost = sum(r[\"cost\"] for r in results) / len(results)\n\n            report[\"summary\"][env_name] = {\n                \"avg_quality\": avg_quality,\n                \"avg_latency\": avg_latency,\n                \"avg_cost\": avg_cost,\n                \"total_tests\": len(results)\n            }\n\n        return report\n\n# Usage\nenvironments = {\n    \"development\": {\n        \"db_path\": \"dev_prompts.db\",\n        \"base_latency\": 200,\n        \"base_quality\": 0.8\n    },\n    \"staging\": {\n        \"db_path\": \"staging_prompts.db\",\n        \"base_latency\": 350,\n        \"base_quality\": 0.85\n    },\n    \"production\": {\n        \"db_path\": \"prod_prompts.db\",\n        \"base_latency\": 500,\n        \"base_quality\": 0.9\n    }\n}\n\ntest_cases = [\n    {\"name\": \"simple_query\", \"input\": \"What is Python?\"},\n    {\"name\": \"complex_query\", \"input\": \"Explain advanced Python decorators\"},\n    {\"name\": \"edge_case\", \"input\": \"Handle this unusual scenario...\"}\n]\n\ntester = MultiEnvironmentTester(environments)\nreport = tester.test_across_environments(\"code_assistant\", \"1.2.0\", test_cases)\n\nprint(\"\ud83d\udcca Cross-Environment Test Results:\")\nfor env, summary in report[\"summary\"].items():\n    print(f\"{env}: Quality={summary['avg_quality']:.2f}, Latency={summary['avg_latency']:.0f}ms\")\n</code></pre>"},{"location":"examples/advanced-workflows/#ab-testing-with-statistical-significance","title":"A/B Testing with Statistical Significance","text":"<pre><code>import scipy.stats as stats\nfrom prompt_versioner import ABTest\n\nclass StatisticalABTest(ABTest):\n    \"\"\"A/B test with proper statistical analysis\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.alpha = 0.05  # Significance level\n        self.power = 0.8   # Statistical power\n\n    def calculate_sample_size(self, baseline_rate, minimum_detectable_effect):\n        \"\"\"Calculate required sample size for statistical significance\"\"\"\n\n        # Effect size (Cohen's d)\n        effect_size = minimum_detectable_effect / baseline_rate\n\n        # Sample size calculation (simplified)\n        z_alpha = stats.norm.ppf(1 - self.alpha/2)\n        z_beta = stats.norm.ppf(self.power)\n\n        sample_size = ((z_alpha + z_beta) ** 2) * 2 / (effect_size ** 2)\n\n        return int(sample_size)\n\n    def analyze_results_statistical(self):\n        \"\"\"Perform statistical analysis of A/B test results\"\"\"\n\n        if not self.is_ready(min_samples=30):\n            return {\"status\": \"insufficient_data\"}\n\n        # Perform t-test\n        t_stat, p_value = stats.ttest_ind(self.results_b, self.results_a)\n\n        # Calculate effect size (Cohen's d)\n        pooled_std = ((len(self.results_a) - 1) * stats.tstd(self.results_a) ** 2 +\n                     (len(self.results_b) - 1) * stats.tstd(self.results_b) ** 2) / \\\n                     (len(self.results_a) + len(self.results_b) - 2)\n        pooled_std = pooled_std ** 0.5\n\n        cohens_d = (stats.tmean(self.results_b) - stats.tmean(self.results_a)) / pooled_std\n\n        # Confidence interval\n        confidence_interval = stats.t.interval(\n            1 - self.alpha,\n            len(self.results_a) + len(self.results_b) - 2,\n            loc=stats.tmean(self.results_b) - stats.tmean(self.results_a),\n            scale=pooled_std * (1/len(self.results_a) + 1/len(self.results_b)) ** 0.5\n        )\n\n        return {\n            \"status\": \"complete\",\n            \"p_value\": p_value,\n            \"is_significant\": p_value &lt; self.alpha,\n            \"effect_size\": cohens_d,\n            \"confidence_interval\": confidence_interval,\n            \"sample_size_a\": len(self.results_a),\n            \"sample_size_b\": len(self.results_b),\n            \"mean_a\": stats.tmean(self.results_a),\n            \"mean_b\": stats.tmean(self.results_b)\n        }\n\n    def print_statistical_report(self):\n        \"\"\"Print detailed statistical report\"\"\"\n\n        analysis = self.analyze_results_statistical()\n\n        if analysis[\"status\"] == \"insufficient_data\":\n            print(\"\u26a0\ufe0f Insufficient data for statistical analysis\")\n            return\n\n        print(\"\ud83d\udcca STATISTICAL A/B TEST ANALYSIS\")\n        print(\"=\" * 40)\n        print(f\"Version A ({self.version_a}): {analysis['sample_size_a']} samples, mean = {analysis['mean_a']:.3f}\")\n        print(f\"Version B ({self.version_b}): {analysis['sample_size_b']} samples, mean = {analysis['mean_b']:.3f}\")\n        print()\n        print(f\"P-value: {analysis['p_value']:.4f}\")\n        print(f\"Effect size (Cohen's d): {analysis['effect_size']:.3f}\")\n        print(f\"95% CI: [{analysis['confidence_interval'][0]:.3f}, {analysis['confidence_interval'][1]:.3f}]\")\n        print()\n\n        if analysis[\"is_significant\"]:\n            winner = \"B\" if analysis['mean_b'] &gt; analysis['mean_a'] else \"A\"\n            improvement = abs(analysis['mean_b'] - analysis['mean_a']) / analysis['mean_a'] * 100\n            print(f\"\ud83c\udf89 SIGNIFICANT RESULT: Version {winner} wins!\")\n            print(f\"Improvement: {improvement:.1f}%\")\n        else:\n            print(\"\u274c No significant difference detected\")\n\n# Usage\nstatistical_test = StatisticalABTest(\n    versioner=pv,\n    prompt_name=\"customer_service\",\n    version_a=\"2.0.0\",\n    version_b=\"2.1.0\"\n)\n\n# Calculate required sample size\nrequired_samples = statistical_test.calculate_sample_size(\n    baseline_rate=0.85,\n    minimum_detectable_effect=0.05\n)\nprint(f\"Required samples per variant: {required_samples}\")\n\n# ... collect data ...\n\n# Analyze with proper statistics\nstatistical_test.print_statistical_report()\n</code></pre>"},{"location":"examples/advanced-workflows/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>Integrations - Learn about system integrations</li> <li>Best Practices - Comprehensive best practices guide</li> <li>Performance Monitoring - Advanced monitoring techniques</li> </ul>"},{"location":"examples/basic-usage/","title":"Basic Usage","text":"<p>Get started with Prompt Versioner - learn the fundamentals through practical examples.</p>"},{"location":"examples/basic-usage/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\n# Initialize versioner\npv = PromptVersioner(project_name=\"my-first-project\", enable_git=False)\n\n# Save your first prompt version\npv.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are a helpful AI assistant.\",\n    user_prompt=\"Please help me with: {query}\",\n    bump_type=VersionBump.MAJOR,  # Creates v1.0.0\n)\n\nprint(\"\ud83c\udf89 Your first prompt version is saved!\")\n</code></pre>"},{"location":"examples/basic-usage/#creating-versions","title":"\ud83d\udcdd Creating Versions","text":""},{"location":"examples/basic-usage/#your-first-versions","title":"Your First Versions","text":"<pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\n# Initialize\npv = PromptVersioner(project_name=\"learning-prompts\", enable_git=False)\n\n# Create initial version\npv.save_version(\n    name=\"email_writer\",\n    system_prompt=\"You are a professional email writing assistant.\",\n    user_prompt=\"Write a professional email about: {topic}\",\n    bump_type=VersionBump.MAJOR,  # Creates 1.0.0\n    metadata={\"type\": \"email\", \"author\": \"me\"}\n)\n\n# Improve the prompt\npv.save_version(\n    name=\"email_writer\",\n    system_prompt=\"You are a professional email writing assistant. Always be polite and concise.\",\n    user_prompt=\"Write a professional, polite email about: {topic}\\n\\nKeep it concise and friendly.\",\n    bump_type=VersionBump.MINOR,  # Creates 1.1.0\n    metadata={\"improvement\": \"added politeness and conciseness\"}\n)\n\n# Fix a typo\npv.save_version(\n    name=\"email_writer\",\n    system_prompt=\"You are a professional email writing assistant. Always be polite and concise.\",\n    user_prompt=\"Write a professional, polite email about: {topic}\\n\\nKeep it concise and friendly.\",  # Fixed typo\n    bump_type=VersionBump.PATCH,  # Creates 1.1.1\n    metadata={\"fix\": \"typo correction\"}\n)\n\nprint(\"\u2705 Created 3 versions: 1.0.0, 1.1.0, 1.1.1\")\n</code></pre>"},{"location":"examples/basic-usage/#version-types-explained","title":"Version Types Explained","text":"<pre><code># MAJOR version (1.0.0 \u2192 2.0.0) - Breaking changes\npv.save_version(\n    name=\"translator\",\n    system_prompt=\"You are a language translator.\",\n    user_prompt=\"Translate to {language}: {text}\",\n    bump_type=VersionBump.MAJOR\n)\n\n# MINOR version (2.0.0 \u2192 2.1.0) - New features\npv.save_version(\n    name=\"translator\",\n    system_prompt=\"You are a language translator. Provide context when helpful.\",\n    user_prompt=\"Translate to {language}: {text}\\n\\nProvide brief context if the translation might be ambiguous.\",\n    bump_type=VersionBump.MINOR\n)\n\n# PATCH version (2.1.0 \u2192 2.1.1) - Small fixes\npv.save_version(\n    name=\"translator\",\n    system_prompt=\"You are a language translator. Provide context when helpful.\",\n    user_prompt=\"Translate to {language}: {text}\\n\\nProvide brief context if the translation might be ambiguous.\",\n    bump_type=VersionBump.PATCH\n</code></pre>"},{"location":"examples/basic-usage/#working-with-versions","title":"\ud83d\udd0d Working with Versions","text":""},{"location":"examples/basic-usage/#getting-your-versions","title":"Getting Your Versions","text":"<pre><code># Get the latest version\nlatest = pv.get_latest(\"email_writer\")\nprint(f\"Latest version: {latest['version']}\")\nprint(f\"System prompt: {latest['system_prompt']}\")\nprint(f\"User prompt: {latest['user_prompt']}\")\n\n# Get a specific version\nversion_1_0 = pv.get_version(\"email_writer\", \"1.0.0\")\nprint(f\"Version 1.0.0 system prompt: {version_1_0['system_prompt']}\")\n\n# List all versions (newest first)\nversions = pv.list_versions(\"email_writer\")\nprint(f\"All versions:\")\nfor v in versions:\n    print(f\"  v{v['version']} - {v['timestamp']}\")\n\n# List all your prompts\nall_prompts = pv.list_prompts()\nprint(f\"All prompts: {all_prompts}\")\n</code></pre>"},{"location":"examples/basic-usage/#comparing-versions","title":"Comparing Versions","text":"<pre><code># See what changed between versions\ndiff = pv.diff(\"email_writer\", \"1.0.0\", \"1.1.0\", format_output=True)\n# This will print a formatted diff showing the changes\n\n# Get diff details\nprint(f\"Changes summary: {diff.summary}\")\nprint(f\"Number of changes: {len(diff.changes)}\")\n</code></pre>"},{"location":"examples/basic-usage/#tracking-performance","title":"\ud83d\udcca Tracking Performance","text":""},{"location":"examples/basic-usage/#basic-metrics-tracking","title":"Basic Metrics Tracking","text":"<pre><code># Log performance metrics after using a prompt\npv.log_metrics(\n    name=\"email_writer\",\n    version=\"1.1.0\",\n    model_name=\"gpt-4o-mini\",\n    input_tokens=50,\n    output_tokens=120,\n    latency_ms=340,\n    quality_score=0.9,  # Your assessment (0.0 to 1.0)\n    success=True\n)\n\nprint(\"\ud83d\udcca Metrics logged!\")\n\n# Check how many metrics you have\nversion_data = pv.get_version(\"email_writer\", \"1.1.0\")\nmetrics = pv.storage.get_metrics(version_id=version_data[\"id\"])\nprint(f\"Total metrics recorded: {len(metrics)}\")\n</code></pre>"},{"location":"examples/basic-usage/#simple-ab-testing","title":"\ud83e\uddea Simple A/B Testing","text":""},{"location":"examples/basic-usage/#compare-two-versions","title":"Compare Two Versions","text":"<pre><code>from prompt_versioner import ABTest\nimport random\n\n# Create an A/B test\nab_test = ABTest(\n    versioner=pv,\n    prompt_name=\"email_writer\",\n    version_a=\"1.0.0\",  # Original version\n    version_b=\"1.1.0\",  # Improved version\n    metric_name=\"quality_score\"\n)\n\n# Simulate some test results\nprint(\"\ud83e\uddea Running A/B test simulation...\")\n\n# Version A results (original)\nfor i in range(15):\n    score = random.uniform(0.7, 0.85)  # Slightly lower performance\n    ab_test.log_result(\"a\", score)\n\n# Version B results (improved)\nfor i in range(15):\n    score = random.uniform(0.8, 0.95)  # Better performance\n    ab_test.log_result(\"b\", score)\n\n# Get results\nif ab_test.is_ready(min_samples=10):\n    result = ab_test.get_result()\n    print(f\"\ud83c\udfc6 Winner: Version {result.winner}\")\n    print(f\"\ud83d\udcc8 Improvement: {result.improvement:.1f}%\")\n    print(f\"\ud83c\udfaf Confidence: {result.confidence:.1%}\")\n\n    # Print detailed report\n    ab_test.print_result()\nelse:\n    print(\"Need more test data\")\n</code></pre>"},{"location":"examples/basic-usage/#saving-and-sharing","title":"\ud83d\udcc1 Saving and Sharing","text":""},{"location":"examples/basic-usage/#export-your-prompts","title":"Export Your Prompts","text":"<pre><code>from pathlib import Path\n\n# Export a single prompt with all versions\npv.export_prompt(\n    name=\"email_writer\",\n    output_file=Path(\"email_writer_backup.json\"),\n    format=\"json\",\n    include_metrics=True\n)\n\nprint(\"\ud83d\udcbe Exported email_writer to backup file\")\n\n# Export all prompts\npv.export_all(\n    output_dir=Path(\"all_prompts_backup\"),\n    format=\"json\"\n)\n\nprint(\"\ud83d\udcbe Exported all prompts to backup folder\")\n</code></pre>"},{"location":"examples/basic-usage/#adding-notes","title":"\ud83c\udff7\ufe0f Adding Notes","text":""},{"location":"examples/basic-usage/#document-your-changes","title":"Document Your Changes","text":"<pre><code># Add notes to document your changes\npv.add_annotation(\n    name=\"email_writer\",\n    version=\"1.1.0\",\n    text=\"Added politeness instructions. Tested with 20 examples, quality improved by 12%.\",\n    author=\"me\"\n)\n\n# Add another note\npv.add_annotation(\n    name=\"email_writer\",\n    version=\"1.1.0\",\n    text=\"Works especially well for business communications and customer service emails.\",\n    author=\"me\"\n)\n\n# Read your notes\nannotations = pv.get_annotations(\"email_writer\", \"1.1.0\")\nprint(f\"\ud83d\udcdd Notes for email_writer v1.1.0:\")\nfor note in annotations:\n    print(f\"  {note['author']}: {note['text']}\")\n</code></pre>"},{"location":"examples/basic-usage/#clean-up","title":"\ud83e\uddf9 Clean Up","text":""},{"location":"examples/basic-usage/#delete-versions-you-dont-need","title":"Delete Versions You Don't Need","text":"<pre><code># Delete a specific version (be careful!)\nsuccess = pv.delete_version(\"email_writer\", \"1.0.0\")\nif success:\n    print(\"\ud83d\uddd1\ufe0f Deleted version 1.0.0\")\n\n# Delete an entire prompt (use with caution!)\n# success = pv.delete_prompt(\"old_prompt_name\")\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>Ready to learn more advanced features?</p> <ul> <li>Version Management - Advanced version control techniques</li> <li>Metrics Tracking - Comprehensive performance monitoring</li> <li>A/B Testing - Scientific prompt optimization</li> <li>Performance Monitoring - Monitor your prompts in production</li> <li>Advanced Workflows</li> </ul>"},{"location":"examples/best-practices/","title":"Best Practices","text":"<p>Comprehensive best practices guide for using Prompt Versioner effectively in production.</p>"},{"location":"examples/best-practices/#general-principles","title":"\ud83c\udfaf General Principles","text":""},{"location":"examples/best-practices/#1-start-simple-evolve-gradually","title":"1. Start Simple, Evolve Gradually","text":"<pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\n# \u2705 Good: Start with basic setup\npv = PromptVersioner(project_name=\"my-app\", enable_git=False)\n\n# Save your first version\npv.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are a helpful assistant.\",\n    user_prompt=\"Help with: {query}\",\n    bump_type=VersionBump.MAJOR\n)\n\n# \u274c Avoid: Complex setup from the beginning\n# Don't start with advanced features like pre-release versions,\n# complex metadata, or extensive integrations\n</code></pre>"},{"location":"examples/best-practices/#2-consistent-naming-conventions","title":"2. Consistent Naming Conventions","text":"<pre><code># \u2705 Good: Descriptive, consistent names\ngood_names = [\n    \"email_classifier\",      # Clear purpose\n    \"code_reviewer\",         # Descriptive function\n    \"customer_service_bot\",  # Specific domain\n    \"content_summarizer\"     # Clear action\n]\n\n# \u274c Avoid: Vague or inconsistent names\nbad_names = [\n    \"prompt1\",              # Not descriptive\n    \"the_ai_thing\",         # Too vague\n    \"EmailClassifierV2\",    # Inconsistent casing\n    \"temp_prompt\"           # Temporary names in production\n]\n\n# Use consistent naming pattern\ndef create_prompt_name(domain, function, variant=None):\n    \"\"\"Create consistent prompt names\"\"\"\n    name = f\"{domain}_{function}\"\n    if variant:\n        name += f\"_{variant}\"\n    return name.lower()\n\n# Examples\nprint(create_prompt_name(\"customer\", \"service\"))        # \"customer_service\"\nprint(create_prompt_name(\"code\", \"review\", \"python\"))   # \"code_review_python\"\n</code></pre>"},{"location":"examples/best-practices/#version-management-best-practices","title":"\ud83d\udcdd Version Management Best Practices","text":""},{"location":"examples/best-practices/#semantic-versioning-strategy","title":"Semantic Versioning Strategy","text":"<pre><code># \u2705 Good: Clear versioning strategy\nclass VersioningStrategy:\n\n    @staticmethod\n    def patch_for_fixes():\n        \"\"\"Use PATCH for typos, small corrections\"\"\"\n        return VersionBump.PATCH\n        # Examples: Fix typos, grammar corrections, small clarifications\n\n    @staticmethod\n    def minor_for_enhancements():\n        \"\"\"Use MINOR for new features, improvements\"\"\"\n        return VersionBump.MINOR\n        # Examples: Add new instructions, improve clarity, extend functionality\n\n    @staticmethod\n    def major_for_breaking_changes():\n        \"\"\"Use MAJOR for fundamental changes\"\"\"\n        return VersionBump.MAJOR\n        # Examples: Complete rewrites, change in purpose, different output format\n\n# Document your changes\npv.save_version(\n    name=\"code_reviewer\",\n    system_prompt=\"You are an expert Python code reviewer...\",  # Enhanced\n    user_prompt=\"Review this Python code...\",\n    bump_type=VersioningStrategy.minor_for_enhancements(),\n    metadata={\n        \"change_summary\": \"Added Python-specific expertise\",\n        \"author\": \"dev-team\",\n        \"reasoning\": \"Improved accuracy for Python code reviews\"\n    }\n)\n\n# Add meaningful annotations\npv.add_annotation(\n    name=\"code_reviewer\",\n    version=\"1.1.0\",\n    text=\"Tested on 100+ Python files, shows 15% improvement in review quality\",\n    author=\"qa-team\"\n)\n</code></pre>"},{"location":"examples/best-practices/#metadata-best-practices","title":"Metadata Best Practices","text":"<pre><code># \u2705 Good: Comprehensive, structured metadata\ndef create_standard_metadata(author, purpose, testing_info=None):\n    \"\"\"Create standardized metadata\"\"\"\n\n    metadata = {\n        # Who and when\n        \"author\": author,\n        \"created_by_team\": author.split(\"@\")[1] if \"@\" in author else \"unknown\",\n        \"creation_date\": datetime.now().isoformat(),\n\n        # What and why\n        \"purpose\": purpose,\n        \"change_type\": \"enhancement\",  # enhancement, fix, feature, breaking\n\n        # Quality assurance\n        \"tested\": testing_info is not None,\n        \"approval_required\": True,\n        \"production_ready\": False,\n\n        # Context\n        \"target_model\": \"gpt-4o\",\n        \"expected_use_cases\": [],\n        \"known_limitations\": []\n    }\n\n    if testing_info:\n        metadata.update(testing_info)\n\n    return metadata\n\n# Usage\nmetadata = create_standard_metadata(\n    author=\"alice@company.com\",\n    purpose=\"Improve code review accuracy for Python projects\",\n    testing_info={\n        \"test_cases\": 50,\n        \"quality_threshold\": 0.85,\n        \"performance_benchmark\": \"latency &lt; 500ms\"\n    }\n)\n\npv.save_version(\n    name=\"code_reviewer\",\n    system_prompt=\"...\",\n    user_prompt=\"...\",\n    bump_type=VersionBump.MINOR,\n    metadata=metadata\n)\n</code></pre>"},{"location":"examples/best-practices/#metrics-and-monitoring","title":"\ud83d\udcca Metrics and Monitoring","text":""},{"location":"examples/best-practices/#comprehensive-metrics-tracking","title":"Comprehensive Metrics Tracking","text":"<pre><code>class MetricsTracker:\n    \"\"\"Standardized metrics tracking\"\"\"\n\n    def __init__(self, pv):\n        self.pv = pv\n\n    def log_production_metrics(self, prompt_name, version, llm_response,\n                             user_context=None, quality_score=None):\n        \"\"\"Log comprehensive production metrics\"\"\"\n\n        # Calculate quality if not provided\n        if quality_score is None:\n            quality_score = self.evaluate_quality(llm_response)\n\n        # Standard metrics\n        self.pv.log_metrics(\n            name=prompt_name,\n            version=version,\n\n            # LLM performance\n            model_name=llm_response.model,\n            input_tokens=llm_response.usage.prompt_tokens,\n            output_tokens=llm_response.usage.completion_tokens,\n            latency_ms=llm_response.latency_ms,\n\n            # Cost tracking\n            cost_eur=self.calculate_cost(llm_response),\n\n            # Quality assessment\n            quality_score=quality_score,\n            success=llm_response.success,\n            error_message=llm_response.error if not llm_response.success else None,\n\n            # Model parameters\n            temperature=llm_response.temperature,\n            max_tokens=llm_response.max_tokens,\n\n            # Context\n            metadata={\n                \"user_id\": user_context.get(\"user_id\") if user_context else None,\n                \"session_id\": user_context.get(\"session_id\") if user_context else None,\n                \"environment\": \"production\",\n                \"user_feedback\": None,  # To be updated later\n                \"business_context\": user_context.get(\"context\") if user_context else None\n            }\n        )\n\n    def evaluate_quality(self, response):\n        \"\"\"Evaluate response quality (implement your logic)\"\"\"\n        # Implement your quality evaluation logic\n        # This could involve sentiment analysis, coherence checks, etc.\n        return 0.85  # Placeholder\n\n    def calculate_cost(self, response):\n        \"\"\"Calculate cost in EUR\"\"\"\n        # Implement your cost calculation\n        return 0.003  # Placeholder\n\n# Usage\ntracker = MetricsTracker(pv)\n\n# In production code\n# tracker.log_production_metrics(\n#     \"customer_service\", \"2.1.0\",\n#     llm_response, user_context, quality_score\n# )\n</code></pre>"},{"location":"examples/best-practices/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>def setup_performance_monitoring(pv, critical_prompts):\n    \"\"\"Set up automated performance monitoring\"\"\"\n\n    thresholds = {\n        \"min_quality\": 0.8,\n        \"max_latency\": 1000,\n        \"min_success_rate\": 0.95,\n        \"max_cost_per_call\": 0.01\n    }\n\n    def check_prompt_health(prompt_name, version):\n        \"\"\"Check if prompt meets performance thresholds\"\"\"\n\n        version_data = pv.get_version(prompt_name, version)\n        recent_metrics = pv.storage.get_metrics(version_id=version_data[\"id\"], limit=100)\n\n        if len(recent_metrics) &lt; 10:\n            return {\"status\": \"insufficient_data\"}\n\n        # Calculate current performance\n        avg_quality = sum(m.get(\"quality_score\", 0) for m in recent_metrics) / len(recent_metrics)\n        avg_latency = sum(m.get(\"latency_ms\", 0) for m in recent_metrics) / len(recent_metrics)\n        success_rate = sum(1 for m in recent_metrics if m.get(\"success\", True)) / len(recent_metrics)\n        avg_cost = sum(m.get(\"cost_eur\", 0) for m in recent_metrics) / len(recent_metrics)\n\n        # Check thresholds\n        issues = []\n        if avg_quality &lt; thresholds[\"min_quality\"]:\n            issues.append(f\"Quality below threshold: {avg_quality:.2f}\")\n        if avg_latency &gt; thresholds[\"max_latency\"]:\n            issues.append(f\"Latency above threshold: {avg_latency:.0f}ms\")\n        if success_rate &lt; thresholds[\"min_success_rate\"]:\n            issues.append(f\"Success rate below threshold: {success_rate:.1%}\")\n        if avg_cost &gt; thresholds[\"max_cost_per_call\"]:\n            issues.append(f\"Cost above threshold: \u20ac{avg_cost:.4f}\")\n\n        return {\n            \"status\": \"healthy\" if not issues else \"unhealthy\",\n            \"issues\": issues,\n            \"metrics\": {\n                \"quality\": avg_quality,\n                \"latency\": avg_latency,\n                \"success_rate\": success_rate,\n                \"cost\": avg_cost\n            }\n        }\n\n    # Monitor all critical prompts\n    print(\"\ud83d\udd0d Health Check Results:\")\n    for prompt_name, version in critical_prompts:\n        health = check_prompt_health(prompt_name, version)\n        status_emoji = \"\u2705\" if health[\"status\"] == \"healthy\" else \"\u274c\"\n\n        print(f\"{status_emoji} {prompt_name} v{version}: {health['status']}\")\n        for issue in health.get(\"issues\", []):\n            print(f\"    \u26a0\ufe0f {issue}\")\n\n# Monitor critical prompts\ncritical_prompts = [\n    (\"customer_service\", \"2.1.0\"),\n    (\"code_reviewer\", \"1.1.0\"),\n    (\"content_generator\", \"1.3.0\")\n]\n\nsetup_performance_monitoring(pv, critical_prompts)\n</code></pre>"},{"location":"examples/best-practices/#testing-best-practices","title":"\ud83e\uddea Testing Best Practices","text":""},{"location":"examples/best-practices/#systematic-testing-approach","title":"Systematic Testing Approach","text":"<pre><code>class PromptTestSuite:\n    \"\"\"Comprehensive testing for prompt versions\"\"\"\n\n    def __init__(self, pv):\n        self.pv = pv\n        self.test_cases = []\n\n    def add_test_case(self, name, input_data, expected_criteria):\n        \"\"\"Add a test case to the suite\"\"\"\n        self.test_cases.append({\n            \"name\": name,\n            \"input\": input_data,\n            \"criteria\": expected_criteria\n        })\n\n    def run_tests(self, prompt_name, version):\n        \"\"\"Run all test cases for a prompt version\"\"\"\n\n        results = []\n\n        print(f\"\ud83e\uddea Running {len(self.test_cases)} tests for {prompt_name} v{version}\")\n\n        for test_case in self.test_cases:\n            with self.pv.test_version(prompt_name, version) as test:\n                # Simulate LLM call (replace with actual call)\n                result = self.simulate_llm_call(test_case[\"input\"])\n\n                # Evaluate against criteria\n                evaluation = self.evaluate_result(result, test_case[\"criteria\"])\n\n                # Log test metrics\n                test.log(\n                    tokens=result[\"tokens\"],\n                    cost=result[\"cost\"],\n                    latency_ms=result[\"latency\"],\n                    quality_score=evaluation[\"quality\"],\n                    metadata={\n                        \"test_case\": test_case[\"name\"],\n                        \"passed\": evaluation[\"passed\"],\n                        \"criteria_met\": evaluation[\"criteria_met\"]\n                    }\n                )\n\n                results.append({\n                    \"test_case\": test_case[\"name\"],\n                    \"passed\": evaluation[\"passed\"],\n                    \"quality\": evaluation[\"quality\"],\n                    \"details\": evaluation\n                })\n\n        return self.generate_test_report(results)\n\n    def simulate_llm_call(self, input_data):\n        \"\"\"Simulate LLM call (replace with actual implementation)\"\"\"\n        import random\n        return {\n            \"tokens\": random.randint(100, 300),\n            \"cost\": random.uniform(0.001, 0.005),\n            \"latency\": random.uniform(200, 800),\n            \"content\": f\"Response to: {input_data[:50]}\"\n        }\n\n    def evaluate_result(self, result, criteria):\n        \"\"\"Evaluate result against test criteria\"\"\"\n\n        quality = random.uniform(0.7, 0.95)  # Replace with actual evaluation\n\n        criteria_met = {\n            \"quality\": quality &gt;= criteria.get(\"min_quality\", 0.8),\n            \"latency\": result[\"latency\"] &lt;= criteria.get(\"max_latency\", 1000),\n            \"cost\": result[\"cost\"] &lt;= criteria.get(\"max_cost\", 0.01)\n        }\n\n        passed = all(criteria_met.values())\n\n        return {\n            \"passed\": passed,\n            \"quality\": quality,\n            \"criteria_met\": criteria_met\n        }\n\n    def generate_test_report(self, results):\n        \"\"\"Generate comprehensive test report\"\"\"\n\n        total_tests = len(results)\n        passed_tests = sum(1 for r in results if r[\"passed\"])\n        avg_quality = sum(r[\"quality\"] for r in results) / total_tests\n\n        print(f\"\\n\ud83d\udcca Test Results Summary:\")\n        print(f\"Tests passed: {passed_tests}/{total_tests} ({passed_tests/total_tests:.1%})\")\n        print(f\"Average quality: {avg_quality:.2f}\")\n\n        if passed_tests == total_tests:\n            print(\"\u2705 All tests passed! Ready for deployment.\")\n        else:\n            print(\"\u274c Some tests failed. Review before deployment.\")\n            for result in results:\n                if not result[\"passed\"]:\n                    print(f\"  Failed: {result['test_case']}\")\n\n        return {\n            \"total_tests\": total_tests,\n            \"passed_tests\": passed_tests,\n            \"success_rate\": passed_tests / total_tests,\n            \"avg_quality\": avg_quality,\n            \"results\": results\n        }\n\n# Usage\ntest_suite = PromptTestSuite(pv)\n\n# Add test cases\ntest_suite.add_test_case(\n    \"simple_query\",\n    \"What is Python?\",\n    {\"min_quality\": 0.8, \"max_latency\": 500, \"max_cost\": 0.005}\n)\n\ntest_suite.add_test_case(\n    \"complex_query\",\n    \"Explain advanced Python metaclasses with examples\",\n    {\"min_quality\": 0.85, \"max_latency\": 1000, \"max_cost\": 0.01}\n)\n\ntest_suite.add_test_case(\n    \"edge_case\",\n    \"Handle this ambiguous request...\",\n    {\"min_quality\": 0.7, \"max_latency\": 800, \"max_cost\": 0.008}\n)\n\n# Run tests\nreport = test_suite.run_tests(\"code_assistant\", \"1.2.0\")\n</code></pre>"},{"location":"examples/best-practices/#team-collaboration","title":"\ud83c\udfaf Team Collaboration","text":""},{"location":"examples/best-practices/#code-review-process-for-prompts","title":"Code Review Process for Prompts","text":"<pre><code>def create_prompt_review_request(pv, prompt_name, old_version, new_version, reviewers):\n    \"\"\"Create a structured prompt review request\"\"\"\n\n    # Generate diff\n    diff = pv.diff(prompt_name, old_version, new_version, format_output=False)\n\n    # Create review request annotation\n    review_request = f\"\"\"\nPROMPT REVIEW REQUEST\n===================\nPrompt: {prompt_name}\nChange: {old_version} \u2192 {new_version}\nReviewers: {', '.join(reviewers)}\n\nCHANGES SUMMARY:\n{diff.summary}\n\nREVIEW CHECKLIST:\n\u25a1 Prompt clarity and specificity\n\u25a1 Expected behavior alignment\n\u25a1 Performance implications\n\u25a1 Security considerations\n\u25a1 Testing requirements\n\nPlease review and add APPROVED/REJECTED annotation.\n    \"\"\".strip()\n\n    pv.add_annotation(\n        name=prompt_name,\n        version=new_version,\n        text=review_request,\n        author=\"review-system\"\n    )\n\n    print(f\"\ud83d\udcdd Review request created for {prompt_name} v{new_version}\")\n    print(f\"Reviewers: {', '.join(reviewers)}\")\n\ndef approve_prompt_change(pv, prompt_name, version, reviewer, approved=True, comments=\"\"):\n    \"\"\"Approve or reject a prompt change\"\"\"\n\n    status = \"APPROVED\" if approved else \"REJECTED\"\n\n    approval_text = f\"[{status}] {comments}\" if comments else f\"[{status}]\"\n\n    pv.add_annotation(\n        name=prompt_name,\n        version=version,\n        text=approval_text,\n        author=reviewer\n    )\n\n    print(f\"{'\u2705' if approved else '\u274c'} {status} by {reviewer}\")\n\n# Example workflow\ncreate_prompt_review_request(\n    pv, \"customer_service\", \"2.0.0\", \"2.1.0\",\n    [\"senior-dev@company.com\", \"product-owner@company.com\"]\n)\n\napprove_prompt_change(\n    pv, \"customer_service\", \"2.1.0\",\n    \"senior-dev@company.com\", True,\n    \"Looks good, improved clarity and specificity\"\n)\n</code></pre>"},{"location":"examples/best-practices/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>Advanced Workflows - Complex deployment patterns</li> <li>Integrations - Tool and framework integrations</li> <li>Performance Monitoring - Monitor your systems</li> </ul>"},{"location":"examples/integrations/","title":"Integrations","text":"<p>Learn how to integrate Prompt Versioner with popular tools and frameworks.</p>"},{"location":"examples/integrations/#llm-framework-integrations","title":"\ud83d\udd17 LLM Framework Integrations","text":""},{"location":"examples/integrations/#openai-integration","title":"OpenAI Integration","text":"<pre><code>import openai\nfrom prompt_versioner import PromptVersioner\nimport time\n\nclass OpenAIIntegration:\n    \"\"\"Integration with OpenAI API\"\"\"\n\n    def __init__(self, api_key):\n        self.client = openai.OpenAI(api_key=api_key)\n        self.pv = PromptVersioner(project_name=\"openai-integration\")\n\n    def call_with_tracking(self, prompt_name, version, user_input, **kwargs):\n        \"\"\"Call OpenAI API with automatic metrics tracking\"\"\"\n\n        # Get prompt version\n        prompt_data = self.pv.get_version(prompt_name, version)\n        if not prompt_data:\n            raise ValueError(f\"Prompt {prompt_name} v{version} not found\")\n\n        # Prepare messages\n        messages = [\n            {\"role\": \"system\", \"content\": prompt_data[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": prompt_data[\"user_prompt\"].format(input=user_input)}\n        ]\n\n        # Make API call with timing\n        start_time = time.time()\n\n        try:\n            response = self.client.chat.completions.create(\n                model=kwargs.get(\"model\", \"gpt-4o-mini\"),\n                messages=messages,\n                temperature=kwargs.get(\"temperature\", 0.7),\n                max_tokens=kwargs.get(\"max_tokens\", 1000)\n            )\n\n            end_time = time.time()\n\n            # Calculate metrics\n            latency_ms = (end_time - start_time) * 1000\n            input_tokens = response.usage.prompt_tokens\n            output_tokens = response.usage.completion_tokens\n            cost_eur = self.calculate_cost(response.usage, response.model)\n\n            # Log metrics\n            self.pv.log_metrics(\n                name=prompt_name,\n                version=version,\n                model_name=response.model,\n                input_tokens=input_tokens,\n                output_tokens=output_tokens,\n                latency_ms=latency_ms,\n                cost_eur=cost_eur,\n                success=True,\n                temperature=kwargs.get(\"temperature\", 0.7),\n                max_tokens=kwargs.get(\"max_tokens\", 1000),\n                metadata={\n                    \"user_input\": user_input[:100],  # First 100 chars\n                    \"finish_reason\": response.choices[0].finish_reason\n                }\n            )\n\n            return {\n                \"content\": response.choices[0].message.content,\n                \"usage\": response.usage,\n                \"model\": response.model,\n                \"latency_ms\": latency_ms\n            }\n\n        except Exception as e:\n            end_time = time.time()\n\n            # Log error\n            self.pv.log_metrics(\n                name=prompt_name,\n                version=version,\n                latency_ms=(end_time - start_time) * 1000,\n                success=False,\n                error_message=str(e),\n                metadata={\"user_input\": user_input[:100]}\n            )\n\n            raise\n\n    def calculate_cost(self, usage, model):\n        \"\"\"Calculate cost in EUR based on token usage\"\"\"\n\n        # Example pricing (update with current rates)\n        pricing = {\n            \"gpt-4o-mini\": {\"input\": 0.00015, \"output\": 0.0006},  # per 1K tokens\n            \"gpt-4o\": {\"input\": 0.0025, \"output\": 0.01},\n        }\n\n        model_pricing = pricing.get(model, pricing[\"gpt-4o-mini\"])\n\n        input_cost = (usage.prompt_tokens / 1000) * model_pricing[\"input\"]\n        output_cost = (usage.completion_tokens / 1000) * model_pricing[\"output\"]\n\n        return (input_cost + output_cost) * 0.85  # Convert USD to EUR (approximate)\n\n# Usage\nopenai_integration = OpenAIIntegration(api_key=\"your-api-key\")\n\n# Save a prompt version\nopenai_integration.pv.save_version(\n    name=\"code_assistant\",\n    system_prompt=\"You are a helpful Python programming assistant.\",\n    user_prompt=\"Help me with this Python question: {input}\",\n    bump_type=VersionBump.MAJOR\n)\n\n# Use with automatic tracking\nresult = openai_integration.call_with_tracking(\n    prompt_name=\"code_assistant\",\n    version=\"1.0.0\",\n    user_input=\"How do I create a list comprehension?\",\n    model=\"gpt-4o-mini\",\n    temperature=0.3\n)\n\nprint(f\"Response: {result['content']}\")\nprint(f\"Latency: {result['latency_ms']:.1f}ms\")\n</code></pre>"},{"location":"examples/integrations/#langchain-integration","title":"LangChain Integration","text":"<pre><code>from langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.callbacks import BaseCallbackHandler\nfrom prompt_versioner import PromptVersioner\n\nclass PromptVersionerCallback(BaseCallbackHandler):\n    \"\"\"LangChain callback for Prompt Versioner integration\"\"\"\n\n    def __init__(self, pv, prompt_name, version):\n        self.pv = pv\n        self.prompt_name = prompt_name\n        self.version = version\n        self.start_time = None\n        self.tokens_used = {}\n\n    def on_llm_start(self, serialized, prompts, **kwargs):\n        \"\"\"Called when LLM starts running\"\"\"\n        self.start_time = time.time()\n\n    def on_llm_end(self, response, **kwargs):\n        \"\"\"Called when LLM ends running\"\"\"\n\n        if self.start_time:\n            latency_ms = (time.time() - self.start_time) * 1000\n\n            # Extract token usage if available\n            token_usage = response.llm_output.get('token_usage', {}) if response.llm_output else {}\n\n            self.pv.log_metrics(\n                name=self.prompt_name,\n                version=self.version,\n                input_tokens=token_usage.get('prompt_tokens', 0),\n                output_tokens=token_usage.get('completion_tokens', 0),\n                latency_ms=latency_ms,\n                success=True,\n                metadata={\n                    \"langchain_integration\": True,\n                    \"response_generations\": len(response.generations)\n                }\n            )\n\n    def on_llm_error(self, error, **kwargs):\n        \"\"\"Called when LLM encounters an error\"\"\"\n\n        if self.start_time:\n            latency_ms = (time.time() - self.start_time) * 1000\n\n            self.pv.log_metrics(\n                name=self.prompt_name,\n                version=self.version,\n                latency_ms=latency_ms,\n                success=False,\n                error_message=str(error),\n                metadata={\"langchain_integration\": True}\n            )\n\nclass LangChainIntegration:\n    \"\"\"Integration with LangChain framework\"\"\"\n\n    def __init__(self):\n        self.pv = PromptVersioner(project_name=\"langchain-integration\")\n\n    def create_versioned_chain(self, prompt_name, version, llm):\n        \"\"\"Create LangChain chain with versioned prompts\"\"\"\n\n        # Get prompt version\n        prompt_data = self.pv.get_version(prompt_name, version)\n        if not prompt_data:\n            raise ValueError(f\"Prompt {prompt_name} v{version} not found\")\n\n        # Create LangChain prompt template\n        full_prompt = f\"{prompt_data['system_prompt']}\\n\\n{prompt_data['user_prompt']}\"\n        prompt_template = PromptTemplate.from_template(full_prompt)\n\n        # Create callback for tracking\n        callback = PromptVersionerCallback(self.pv, prompt_name, version)\n\n        # Create chain with callback\n        chain = LLMChain(\n            llm=llm,\n            prompt=prompt_template,\n            callbacks=[callback]\n        )\n\n        return chain\n\n# Usage\nfrom langchain.llms import OpenAI\n\nintegration = LangChainIntegration()\n\n# Save versioned prompt\nintegration.pv.save_version(\n    name=\"langchain_assistant\",\n    system_prompt=\"You are a helpful assistant that explains complex topics simply.\",\n    user_prompt=\"Explain {topic} in simple terms that a beginner can understand.\",\n    bump_type=VersionBump.MAJOR\n)\n\n# Create versioned chain\nllm = OpenAI(temperature=0.7)\nchain = integration.create_versioned_chain(\"langchain_assistant\", \"1.0.0\", llm)\n\n# Use chain (automatically tracks metrics)\nresult = chain.run(topic=\"machine learning\")\nprint(result)\n</code></pre>"},{"location":"examples/integrations/#monitoring-and-observability","title":"\ud83d\udcca Monitoring and Observability","text":""},{"location":"examples/integrations/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<pre><code>import wandb\nfrom prompt_versioner import PromptVersioner\n\nclass WandbIntegration:\n    \"\"\"Integration with Weights &amp; Biases for experiment tracking\"\"\"\n\n    def __init__(self, project_name, entity=None):\n        self.pv = PromptVersioner(project_name=f\"wandb_{project_name}\")\n        self.project_name = project_name\n        self.entity = entity\n        self.run = None\n\n    def start_experiment(self, experiment_name, prompt_name, version):\n        \"\"\"Start W&amp;B experiment with prompt version\"\"\"\n\n        prompt_data = self.pv.get_version(prompt_name, version)\n\n        self.run = wandb.init(\n            project=self.project_name,\n            entity=self.entity,\n            name=experiment_name,\n            config={\n                \"prompt_name\": prompt_name,\n                \"prompt_version\": version,\n                \"system_prompt\": prompt_data[\"system_prompt\"],\n                \"user_prompt\": prompt_data[\"user_prompt\"],\n                \"prompt_metadata\": prompt_data.get(\"metadata\", {})\n            }\n        )\n\n        return self.run\n\n    def log_prompt_metrics(self, prompt_name, version, metrics):\n        \"\"\"Log prompt metrics to W&amp;B\"\"\"\n\n        if not self.run:\n            raise ValueError(\"No active W&amp;B run. Call start_experiment first.\")\n\n        # Log to both W&amp;B and Prompt Versioner\n        wandb.log(metrics)\n\n        self.pv.log_metrics(\n            name=prompt_name,\n            version=version,\n            **metrics,\n            metadata={\"wandb_run_id\": self.run.id}\n        )\n\n    def finish_experiment(self):\n        \"\"\"Finish W&amp;B experiment\"\"\"\n        if self.run:\n            wandb.finish()\n            self.run = None\n\n# Usage\nwandb_integration = WandbIntegration(\"prompt-experiments\")\n\n# Start experiment\nrun = wandb_integration.start_experiment(\n    \"customer-service-v2\",\n    \"customer_service\",\n    \"2.1.0\"\n)\n\n# Log metrics during experiment\nfor i in range(10):\n    metrics = {\n        \"quality_score\": 0.85 + random.uniform(-0.05, 0.05),\n        \"latency_ms\": 400 + random.uniform(-50, 50),\n        \"cost_eur\": 0.003 + random.uniform(-0.0005, 0.0005)\n    }\n\n    wandb_integration.log_prompt_metrics(\n        \"customer_service\",\n        \"2.1.0\",\n        metrics\n    )\n\nwandb_integration.finish_experiment()\n</code></pre>"},{"location":"examples/integrations/#mlflow-integration","title":"MLflow Integration","text":"<pre><code>import mlflow\nimport mlflow.sklearn\nfrom prompt_versioner import PromptVersioner\n\nclass MLflowIntegration:\n    \"\"\"Integration with MLflow for model and prompt tracking\"\"\"\n\n    def __init__(self, tracking_uri=None, experiment_name=\"prompt-versioning\"):\n        if tracking_uri:\n            mlflow.set_tracking_uri(tracking_uri)\n\n        mlflow.set_experiment(experiment_name)\n        self.pv = PromptVersioner(project_name=\"mlflow_integration\")\n\n    def log_prompt_as_artifact(self, prompt_name, version):\n        \"\"\"Log prompt version as MLflow artifact\"\"\"\n\n        prompt_data = self.pv.get_version(prompt_name, version)\n\n        with mlflow.start_run():\n            # Log prompt content as parameters\n            mlflow.log_param(\"prompt_name\", prompt_name)\n            mlflow.log_param(\"prompt_version\", version)\n            mlflow.log_param(\"system_prompt_hash\", hash(prompt_data[\"system_prompt\"]))\n            mlflow.log_param(\"user_prompt_hash\", hash(prompt_data[\"user_prompt\"]))\n\n            # Create temporary files for artifacts\n            import tempfile\n            import json\n\n            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n                json.dump(prompt_data, f, indent=2)\n                mlflow.log_artifact(f.name, \"prompts\")\n\n            return mlflow.active_run().info.run_id\n\n    def log_prompt_metrics_to_mlflow(self, prompt_name, version, metrics):\n        \"\"\"Log prompt performance metrics to MLflow\"\"\"\n\n        with mlflow.start_run():\n            # Log metrics to MLflow\n            for key, value in metrics.items():\n                if isinstance(value, (int, float)):\n                    mlflow.log_metric(key, value)\n\n            # Also log to Prompt Versioner\n            self.pv.log_metrics(\n                name=prompt_name,\n                version=version,\n                **metrics,\n                metadata={\"mlflow_run_id\": mlflow.active_run().info.run_id}\n            )\n\n# Usage\nmlflow_integration = MLflowIntegration()\n\n# Log prompt as artifact\nrun_id = mlflow_integration.log_prompt_as_artifact(\"code_assistant\", \"1.2.0\")\nprint(f\"Logged prompt to MLflow run: {run_id}\")\n\n# Log performance metrics\nmlflow_integration.log_prompt_metrics_to_mlflow(\n    \"code_assistant\",\n    \"1.2.0\",\n    {\n        \"accuracy\": 0.92,\n        \"f1_score\": 0.89,\n        \"precision\": 0.91,\n        \"recall\": 0.87\n    }\n)\n</code></pre>"},{"location":"examples/integrations/#database-integrations","title":"\ud83d\uddc4\ufe0f Database Integrations","text":""},{"location":"examples/integrations/#postgresql-integration","title":"PostgreSQL Integration","text":"<pre><code>import psycopg2\nfrom prompt_versioner import PromptVersioner\n\nclass PostgreSQLIntegration:\n    \"\"\"Integration with PostgreSQL for external metrics storage\"\"\"\n\n    def __init__(self, connection_params):\n        self.pv = PromptVersioner(project_name=\"postgres_integration\")\n        self.conn = psycopg2.connect(**connection_params)\n        self.setup_tables()\n\n    def setup_tables(self):\n        \"\"\"Create tables for storing prompt metrics\"\"\"\n\n        with self.conn.cursor() as cur:\n            cur.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS prompt_metrics_external (\n                    id SERIAL PRIMARY KEY,\n                    prompt_name VARCHAR(255) NOT NULL,\n                    version VARCHAR(50) NOT NULL,\n                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    model_name VARCHAR(100),\n                    input_tokens INTEGER,\n                    output_tokens INTEGER,\n                    latency_ms FLOAT,\n                    quality_score FLOAT,\n                    cost_eur FLOAT,\n                    success BOOLEAN,\n                    metadata JSONB\n                )\n            \"\"\")\n            self.conn.commit()\n\n    def log_metrics_dual(self, prompt_name, version, **metrics):\n        \"\"\"Log metrics to both Prompt Versioner and PostgreSQL\"\"\"\n\n        # Log to Prompt Versioner\n        self.pv.log_metrics(\n            name=prompt_name,\n            version=version,\n            **metrics\n        )\n\n        # Log to PostgreSQL\n        with self.conn.cursor() as cur:\n            cur.execute(\"\"\"\n                INSERT INTO prompt_metrics_external\n                (prompt_name, version, model_name, input_tokens, output_tokens,\n                 latency_ms, quality_score, cost_eur, success, metadata)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n            \"\"\", (\n                prompt_name,\n                version,\n                metrics.get(\"model_name\"),\n                metrics.get(\"input_tokens\"),\n                metrics.get(\"output_tokens\"),\n                metrics.get(\"latency_ms\"),\n                metrics.get(\"quality_score\"),\n                metrics.get(\"cost_eur\"),\n                metrics.get(\"success\", True),\n                json.dumps(metrics.get(\"metadata\", {}))\n            ))\n            self.conn.commit()\n\n    def get_aggregated_metrics(self, prompt_name, version, days=7):\n        \"\"\"Get aggregated metrics from PostgreSQL\"\"\"\n\n        with self.conn.cursor() as cur:\n            cur.execute(\"\"\"\n                SELECT\n                    COUNT(*) as total_calls,\n                    AVG(quality_score) as avg_quality,\n                    AVG(latency_ms) as avg_latency,\n                    SUM(cost_eur) as total_cost,\n                    SUM(CASE WHEN success THEN 1 ELSE 0 END)::FLOAT / COUNT(*) as success_rate\n                FROM prompt_metrics_external\n                WHERE prompt_name = %s AND version = %s\n                AND timestamp &gt;= CURRENT_TIMESTAMP - INTERVAL '%s days'\n            \"\"\", (prompt_name, version, days))\n\n            result = cur.fetchone()\n\n            return {\n                \"total_calls\": result[0],\n                \"avg_quality\": float(result[1]) if result[1] else 0,\n                \"avg_latency\": float(result[2]) if result[2] else 0,\n                \"total_cost\": float(result[3]) if result[3] else 0,\n                \"success_rate\": float(result[4]) if result[4] else 0\n            }\n\n# Usage\npostgres_integration = PostgreSQLIntegration({\n    \"host\": \"localhost\",\n    \"database\": \"prompt_metrics\",\n    \"user\": \"your_user\",\n    \"password\": \"your_password\"\n})\n\n# Log metrics to both systems\npostgres_integration.log_metrics_dual(\n    \"customer_service\",\n    \"2.1.0\",\n    model_name=\"gpt-4o\",\n    input_tokens=150,\n    output_tokens=200,\n    latency_ms=420,\n    quality_score=0.91,\n    cost_eur=0.0035,\n    success=True\n)\n\n# Get aggregated metrics\nmetrics = postgres_integration.get_aggregated_metrics(\"customer_service\", \"2.1.0\")\nprint(f\"Total calls: {metrics['total_calls']}\")\nprint(f\"Average quality: {metrics['avg_quality']:.2f}\")\n</code></pre>"},{"location":"examples/integrations/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>Advanced Workflows - Complex deployment patterns</li> <li>Best Practices - Comprehensive guidelines</li> <li>Performance Monitoring - Monitor your integrations</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Learn how to configure Prompt Versioner for your specific needs and environment.</p>"},{"location":"getting-started/configuration/#database-configuration","title":"\ud83d\udd27 Database Configuration","text":"<p>By default, Prompt Versioner uses SQLite with a local database file:</p> <pre><code>from prompt_versioner import PromptVersioner\n\n# Default configuration (creates database in project folder)\npv = PromptVersioner(project_name=\"my-project\")\n\n# Custom database path\npv = PromptVersioner(\n    project_name=\"my-project\",\n    db_path=\"/path/to/your/database.db\"\n)\n\n# Disable Git integration\npv = PromptVersioner(\n    project_name=\"my-project\",\n    enable_git=False\n)\n</code></pre>"},{"location":"getting-started/configuration/#git-integration","title":"\ud83d\udd17 Git Integration","text":"<p>Configure Git integration for version control:</p> <pre><code># Initialize with Git tracking enabled\npv = PromptVersioner(\n    project_name=\"my-project\",\n    enable_git=True,\n    git_repo=\"/path/to/git/repo\"\n)\n\n# Install Git hooks for automatic tracking\npv.install_git_hooks()\n\n# Later, uninstall if needed\npv.uninstall_git_hooks()\n</code></pre>"},{"location":"getting-started/configuration/#performance-monitoring","title":"\ud83d\udcca Performance Monitoring","text":"<p>Set up custom monitoring for your prompts:</p> <pre><code>def monitor_performance(pv, prompt_name):\n    \"\"\"Simple performance monitoring\"\"\"\n    latest = pv.get_latest(prompt_name)\n    version_data = pv.get_version(prompt_name, latest[\"version\"])\n    metrics = pv.storage.get_metrics(version_id=version_data[\"id\"])\n\n    if metrics:\n        recent_metrics = metrics[-10:]  # Last 10 calls\n        avg_quality = sum(m.get(\"quality_score\", 0) for m in recent_metrics) / len(recent_metrics)\n        avg_latency = sum(m.get(\"latency_ms\", 0) for m in recent_metrics) / len(recent_metrics)\n\n        # Check thresholds\n        if avg_quality &lt; 0.7:\n            print(f\"\u26a0\ufe0f Low quality: {avg_quality:.2f}\")\n        if avg_latency &gt; 5000:\n            print(f\"\u26a0\ufe0f High latency: {avg_latency:.0f}ms\")\n        else:\n            print(f\"\u2705 Performance OK: Quality {avg_quality:.2f}, Latency {avg_latency:.0f}ms\")\n\n# Monitor your prompts\nmonitor_performance(pv, \"my_prompt\")\n</code></pre>"},{"location":"getting-started/configuration/#best-practices","title":"\ud83d\udcdd Best Practices","text":"<ol> <li>Environment Separation: Use different projects for dev/staging/production</li> <li>Secure Paths: Use secure database locations for sensitive data</li> <li>Git Integration: Enable Git for version control and team collaboration</li> <li>Regular Monitoring: Set up performance monitoring with thresholds</li> <li>Documentation: Use annotations to document changes and decisions</li> </ol>"},{"location":"getting-started/configuration/#quick-validation","title":"\ud83d\udd0d Quick Validation","text":"<p>Test your configuration:</p> <pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\ndef validate_setup(project_name, db_path):\n    \"\"\"Quick setup validation\"\"\"\n    try:\n        pv = PromptVersioner(project_name=project_name, db_path=db_path)\n\n        # Test basic operations\n        pv.save_version(\n            name=\"test_prompt\",\n            system_prompt=\"Test system prompt\",\n            user_prompt=\"Test user prompt\",\n            bump_type=VersionBump.MAJOR\n        )\n\n        # Clean up\n        pv.delete_prompt(\"test_prompt\")\n\n        print(\"\u2705 Configuration valid!\")\n        return True\n\n    except Exception as e:\n        print(f\"\u274c Configuration error: {e}\")\n        return False\n\n# Validate\nvalidate_setup(\"my-project\", \"./test.db\")\n</code></pre> <p>Ready to get started? Check out the Quick Start Guide or learn about Basic Usage.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install and set up Prompt Versioner in your environment.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.11 or higher</li> <li>Operating System: Windows, macOS, or Linux</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#poetry-recommended","title":"\ud83d\udce6 Poetry (Recommended)","text":"<p>Install from the GitHub repository:</p> <pre><code># Install from GitHub repository\npoetry add git+https://github.com/pepes97/prompt-versioner.git\n\n# Or clone and install locally for development\ngit clone https://github.com/pepes97/prompt-versioner.git\ncd prompt-versioner\npoetry install\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"\ud83d\ude80 Development Installation","text":"<p>For development:</p> <pre><code># Clone the repository\ngit clone https://github.com/pepes97/prompt-versioner.git\ncd prompt-versioner\n\n# Install with Poetry\npoetry install\n\n# Or install with pip in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code># Test the Python import\nfrom prompt_versioner import PromptVersioner, VersionBump\npv = PromptVersioner(project_name=\"test\", enable_git=False)\nprint(\"\u2705 Prompt Versioner is working!\")\n</code></pre>"},{"location":"getting-started/installation/#database-setup","title":"Database Setup","text":"<p>Prompt Versioner uses SQLite by default - no additional setup required. The database file will be created automatically.</p>"},{"location":"getting-started/installation/#custom-database-location","title":"Custom Database Location","text":"<pre><code>from prompt_versioner import PromptVersioner\n\n# Custom database path\npv = PromptVersioner(\n    project_name=\"my-project\",\n    db_path=\"/path/to/your/database.db\"\n)\n</code></pre>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the GitHub Issues</li> <li>Include your Python version, OS, and error messages</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have Prompt Versioner installed:</p> <ul> <li>Quick Start Guide - Get up and running in minutes</li> <li>Configuration - Customize your setup</li> <li>Basic Usage - Learn with practical examples</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get up and running with Prompt Versioner in just a few minutes!</p>"},{"location":"getting-started/quick-start/#your-first-prompt","title":"\ud83d\ude80 Your First Prompt","text":"<p>Let's start by creating and versioning your first prompt:</p> <pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\n# Initialize the versioner\npv = PromptVersioner(project_name=\"my-first-project\", enable_git=False)\n\n# Create your first prompt version\npv.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are a helpful assistant.\",\n    user_prompt=\"Please answer the following question: {question}\",\n    bump_type=VersionBump.MAJOR\n)\n\nprint(\"\u2705 Created first prompt version 1.0.0!\")\n</code></pre>"},{"location":"getting-started/quick-start/#creating-versions","title":"\ud83d\udcdd Creating Versions","text":"<p>Improve the prompt and create a new version:</p> <pre><code># Create an improved version\npv.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are an expert AI tutor with deep knowledge.\",\n    user_prompt=\"Please provide a comprehensive answer to: {question}\",\n    bump_type=VersionBump.MINOR  # Creates 1.1.0\n)\n\nprint(\"\u2705 Created version 1.1.0!\")\n</code></pre>"},{"location":"getting-started/quick-start/#tracking-metrics","title":"\ud83d\udcca Tracking Metrics","text":"<p>Track the performance of your prompts:</p> <pre><code># Log performance metrics after using a prompt\npv.log_metrics(\n    name=\"assistant\",\n    version=\"1.1.0\",\n    model_name=\"gpt-4o-mini\",\n    input_tokens=25,\n    output_tokens=150,\n    latency_ms=2300,\n    quality_score=0.85,\n    success=True\n)\n\nprint(\"\ud83d\udcca Metrics tracked successfully!\")\n</code></pre>"},{"location":"getting-started/quick-start/#using-your-prompts","title":"\ud83d\udd0d Using Your Prompts","text":"<p>Get prompts and their versions:</p> <pre><code># Get the latest version\nlatest = pv.get_latest(\"assistant\")\nprint(f\"Latest version: {latest['version']}\")\nprint(f\"System prompt: {latest['system_prompt']}\")\n\n# Get a specific version\nv1_prompt = pv.get_version(\"assistant\", \"1.0.0\")\nprint(f\"V1.0.0 system prompt: {v1_prompt['system_prompt']}\")\n\n# List all versions\nversions = pv.list_versions(\"assistant\")\nfor v in versions:\n    print(f\"  v{v['version']} - {v['timestamp']}\")\n</code></pre>"},{"location":"getting-started/quick-start/#basic-ab-testing","title":"\ud83e\uddea Basic A/B Testing","text":"<p>Compare two prompt versions:</p> <pre><code>from prompt_versioner import ABTest\n\n# Create an A/B test\nab_test = ABTest(\n    versioner=pv,\n    prompt_name=\"assistant\",\n    version_a=\"1.0.0\",\n    version_b=\"1.1.0\",\n    metric_name=\"quality_score\"\n)\n\n# Simulate test results\nfor i in range(15):\n    ab_test.log_result(\"a\", 0.7 + (i * 0.01))  # Version A\n    ab_test.log_result(\"b\", 0.8 + (i * 0.01))  # Version B\n\n# Get results\nif ab_test.is_ready(min_samples=10):\n    result = ab_test.get_result()\n    print(f\"\ud83c\udfc6 Winner: Version {result.winner}\")\n    print(f\"\ud83d\udcc8 Improvement: {result.improvement:.1f}%\")\n</code></pre>"},{"location":"getting-started/quick-start/#whats-next","title":"\ud83c\udfaf What's Next?","text":"<p>Now that you've got the basics down, explore more:</p> <ul> <li>Configuration: Customize your setup</li> <li>Basic Usage: Learn with practical examples</li> <li>Core Concepts: Understand the architecture</li> <li>Version Management: Advanced versioning strategies</li> </ul>"},{"location":"user-guide/ab-testing/","title":"A/B Testing","text":"<p>A concise guide to running A/B tests with Prompt Versioner. Keep this page focused on the common, practical workflows: Quick Start, setting up versions, collecting results, basic analysis and a short example.</p>"},{"location":"user-guide/ab-testing/#quick-start","title":"\ud83e\uddea Quick Start","text":"<pre><code>from prompt_versioner import PromptVersioner, ABTest\nimport random\n\npv = PromptVersioner(project_name=\"my-project\", enable_git=False)\n\n# Prepare versions (assumes 1.0.0 and 1.1.0 exist)\nab_test = ABTest(\n    versioner=pv,\n    prompt_name=\"code_reviewer\",\n    version_a=\"1.0.0\",\n    version_b=\"1.1.0\",\n    metric_name=\"quality_score\",\n)\n\n# Simulate logging\nfor _ in range(30):\n    ab_test.log_result(\"a\", random.uniform(0.70, 0.85))\n    ab_test.log_result(\"b\", random.uniform(0.75, 0.90))\n\nif ab_test.is_ready(min_samples=20):\n    result = ab_test.get_result()\n    print(f\"Winner: {result.winner} \u2014 improvement: {result.improvement:.2%}, confidence: {result.confidence:.1%}\")\n</code></pre>"},{"location":"user-guide/ab-testing/#setting-up-tests","title":"\ud83d\udd27 Setting Up Tests","text":"<ol> <li> <p>Create the versions you want to compare. Use <code>pv.save_version(...)</code> with <code>VersionBump.MAJOR/MINOR/PATCH</code> depending on the change.</p> </li> <li> <p>Initialize the <code>ABTest</code> with the prompt name and the two versions to compare. Choose a primary metric (e.g., <code>quality_score</code>) to evaluate.</p> </li> </ol> <pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\npv = PromptVersioner(project_name=\"my-project\")\n\n# Baseline\npv.save_version(\n    name=\"summarizer\",\n    system_prompt=\"You are a summarization assistant.\",\n    user_prompt=\"Summarize: {text}\",\n    bump_type=VersionBump.MAJOR,\n)\n\n# Improved\npv.save_version(\n    name=\"summarizer\",\n    system_prompt=\"You are an expert summarizer that produces concise, accurate summaries.\",\n    user_prompt=\"Please summarize: {text}\",\n    bump_type=VersionBump.MINOR,\n)\n\nab_test = ABTest(\n    versioner=pv,\n    prompt_name=\"summarizer\",\n    version_a=\"1.0.0\",\n    version_b=\"1.1.0\",\n    metric_name=\"quality_score\",\n)\n</code></pre>"},{"location":"user-guide/ab-testing/#collecting-results","title":"\ud83d\udcca Collecting Results","text":"<ul> <li>Use <code>ab_test.log_result(variant, value)</code> to add single observations (variant is \"a\" or \"b\").</li> <li>For batch uploads, use <code>ab_test.log_batch_results(variant, list_of_values)</code> where available.</li> <li>Integrate logging into your production path: after an LLM call, evaluate the response and log the metric.</li> </ul> <pre><code># Example after calling the LLM and computing a quality_score\nab_test.log_result(\"a\", quality_score)\npv.log_metrics(name=\"summarizer\", version=\"1.1.0\", model_name=\"gpt-4o\", quality_score=quality_score, success=True)\n</code></pre>"},{"location":"user-guide/ab-testing/#analyzing-results","title":"\ud83d\udcc8 Analyzing Results","text":"<ul> <li>Check sample counts with <code>ab_test.get_sample_counts()</code>.</li> <li>When <code>ab_test.is_ready(min_samples)</code> is True, call <code>ab_test.get_result()</code> to get summary stats (means, samples, winner, confidence).</li> <li>Use <code>ab_test.print_result()</code> for a human-readable report.</li> </ul> <pre><code>count_a, count_b = ab_test.get_sample_counts()\nprint(f\"Samples: A={count_a}, B={count_b}\")\n\nif ab_test.is_ready(min_samples=30):\n    result = ab_test.get_result()\n    print(result)\n    ab_test.print_result()\n</code></pre>"},{"location":"user-guide/ab-testing/#best-practices-short","title":"\u2696\ufe0f Best Practices (Short)","text":"<ul> <li>Test one change at a time. Keep tests focused.</li> <li>Define the primary metric and a minimum sample size before starting.</li> <li>Randomize assignment to avoid bias.</li> <li>Monitor test health (latency, errors) alongside quality metrics.</li> </ul>"},{"location":"user-guide/ab-testing/#short-example","title":"\ud83e\udde9 Short Example","text":"<p>A short, complete example showing the minimal end-to-end flow.</p> <pre><code>from prompt_versioner import PromptVersioner, VersionBump, ABTest\nimport random\n\npv = PromptVersioner(project_name=\"quick-ab\")\n\npv.save_version(name=\"g\", system_prompt=\"You are helpful.\", user_prompt=\"Q:{q}\", bump_type=VersionBump.MAJOR)\npv.save_version(name=\"g\", system_prompt=\"You are concise and helpful.\", user_prompt=\"Q:{q}\", bump_type=VersionBump.MINOR)\n\nab = ABTest(versioner=pv, prompt_name=\"g\", version_a=\"1.0.0\", version_b=\"1.1.0\", metric_name=\"quality_score\")\nfor i in range(40):\n    ab.log_result(\"a\", random.uniform(0.6, 0.8))\n    ab.log_result(\"b\", random.uniform(0.65, 0.9))\n\nif ab.is_ready(min_samples=20):\n    r = ab.get_result()\n    ab.print_result()\n</code></pre>"},{"location":"user-guide/ab-testing/#next-steps","title":"\ud83d\udcda Next steps","text":"<ul> <li>Metrics &amp; Tracking</li> <li>Version Management</li> </ul>"},{"location":"user-guide/collaboration/","title":"Collaboration","text":"<p>Work effectively with your team using Prompt Versioner's collaboration features.</p>"},{"location":"user-guide/collaboration/#quick-start","title":"\ud83d\udc65 Quick Start","text":"<pre><code>from prompt_versioner import PromptVersioner\n\n# Initialize for team use\npv = PromptVersioner(\n    project_name=\"team-project\",\n    enable_git=True,  # Enable Git for team collaboration\n)\n\n# Add annotation for team communication\npv.add_annotation(\n    name=\"customer_service\",\n    version=\"2.1.0\",\n    text=\"Updated for better sentiment handling. Ready for production.\",\n    author=\"alice@company.com\"\n)\n\n# Export for sharing\nfrom pathlib import Path\npv.export_prompt(\n    name=\"customer_service\",\n    output_file=Path(\"customer_service_v2.1.0.json\"),\n    include_metrics=True\n)\n</code></pre>"},{"location":"user-guide/collaboration/#team-workflows","title":"\ud83e\udd1d Team Workflows","text":""},{"location":"user-guide/collaboration/#version-control-integration","title":"Version Control Integration","text":"<pre><code># Work with Git branches\npv = PromptVersioner(project_name=\"team-ai\", enable_git=True)\n\n# Install Git hooks for automatic tracking\npv.install_git_hooks()\n\n# Create version with Git context\npv.save_version(\n    name=\"code_reviewer\",\n    system_prompt=\"You are an expert code reviewer for Python.\",\n    user_prompt=\"Review this Python code:\\n{code}\\n\\nFocus on: {focus_areas}\",\n    bump_type=VersionBump.MINOR,\n    metadata={\n        \"author\": \"dev-team@company.com\",\n        \"branch\": \"feature/python-focus\",\n        \"reviewer\": \"senior-dev@company.com\",\n        \"approved\": True\n    }\n)\n</code></pre>"},{"location":"user-guide/collaboration/#team-annotations","title":"Team Annotations","text":"<pre><code># Document changes for team\ndef add_team_annotation(pv, prompt_name, version, change_summary, author):\n    \"\"\"Standard team annotation format\"\"\"\n\n    pv.add_annotation(\n        name=prompt_name,\n        version=version,\n        text=f\"[CHANGE] {change_summary}\",\n        author=author\n    )\n\n# Usage examples\nadd_team_annotation(\n    pv, \"email_classifier\", \"1.2.0\",\n    \"Improved spam detection accuracy by 15%\",\n    \"ml-team@company.com\"\n)\n\nadd_team_annotation(\n    pv, \"customer_service\", \"2.1.0\",\n    \"Added support for multiple languages\",\n    \"product@company.com\"\n)\n\n# Review team annotations\nannotations = pv.get_annotations(\"customer_service\", \"2.1.0\")\nfor note in annotations:\n    print(f\"[{note['author']}] {note['text']}\")\n    print(f\"Added: {note['timestamp']}\\n\")\n</code></pre>"},{"location":"user-guide/collaboration/#sharing-and-distribution","title":"\ud83d\udce4 Sharing and Distribution","text":""},{"location":"user-guide/collaboration/#export-for-team-sharing","title":"Export for Team Sharing","text":"<pre><code># Export prompts for team distribution\ndef export_for_team(pv, prompt_name, output_dir):\n    \"\"\"Export prompt with full context for team sharing\"\"\"\n\n    from pathlib import Path\n    from datetime import datetime\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_file = Path(output_dir) / f\"{prompt_name}_{timestamp}.json\"\n\n    pv.export_prompt(\n        name=prompt_name,\n        output_file=output_file,\n        format=\"json\",\n        include_metrics=True\n    )\n\n    print(f\"\ud83d\udce4 Exported {prompt_name} to {output_file}\")\n    print(f\"Share this file with your team!\")\n\n    return output_file\n\n# Export latest version\nexport_for_team(pv, \"customer_service\", \"team_exports\")\n</code></pre>"},{"location":"user-guide/collaboration/#review-processes","title":"\ud83c\udfaf Review Processes","text":""},{"location":"user-guide/collaboration/#code-review-for-prompts","title":"Code Review for Prompts","text":"<pre><code>def create_review_request(pv, prompt_name, old_version, new_version, reviewers):\n    \"\"\"Create a prompt review request\"\"\"\n\n    # Generate diff for review\n    diff = pv.diff(prompt_name, old_version, new_version, format_output=False)\n\n    # Create review annotation\n    review_text = f\"\"\"\nREVIEW REQUEST\n--------------\nChanges: {old_version} \u2192 {new_version}\nReviewers: {', '.join(reviewers)}\nSummary: {diff.summary}\n\nPlease review and approve/reject this change.\n    \"\"\".strip()\n\n    pv.add_annotation(\n        name=prompt_name,\n        version=new_version,\n        text=review_text,\n        author=\"review-system\"\n    )\n\n    print(f\"\ud83d\udccb Review request created for {prompt_name} v{new_version}\")\n    print(f\"Reviewers: {', '.join(reviewers)}\")\n\ndef approve_change(pv, prompt_name, version, reviewer, approved=True):\n    \"\"\"Approve or reject a prompt change\"\"\"\n\n    status = \"APPROVED\" if approved else \"REJECTED\"\n\n    pv.add_annotation(\n        name=prompt_name,\n        version=version,\n        text=f\"[{status}] Reviewed by {reviewer}\",\n        author=reviewer\n    )\n\n    print(f\"\u2705 Change {status.lower()} by {reviewer}\")\n\n# Example workflow\ncreate_review_request(\n    pv, \"code_reviewer\", \"1.0.0\", \"1.1.0\",\n    [\"senior-dev@company.com\", \"team-lead@company.com\"]\n)\n\napprove_change(pv, \"code_reviewer\", \"1.1.0\", \"senior-dev@company.com\", approved=True)\n</code></pre>"},{"location":"user-guide/collaboration/#team-setup-checklist","title":"\ud83d\ude80 Team Setup Checklist","text":"<ol> <li>\u2705 Initialize Git: Enable Git integration for version control</li> <li>\u2705 Set Team Standards: Define annotation formats and review processes</li> <li>\u2705 Install Hooks: Set up Git hooks for automatic tracking</li> <li>\u2705 Define Roles: Establish team roles and permissions</li> <li>\u2705 Create Workflows: Document review and approval processes</li> <li>\u2705 Setup Monitoring: Track team performance and activity</li> <li>\u2705 Train Team: Ensure all members understand the tools and processes</li> </ol>"},{"location":"user-guide/collaboration/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>Version Management - Learn advanced version control</li> <li>Web Dashboard - Use the visual interface for team collaboration</li> <li>Performance Monitoring - Monitor team prompt performance</li> </ul>"},{"location":"user-guide/core-concepts/","title":"Core Concepts","text":"<p>Understanding the fundamental concepts behind Prompt Versioner will help you effectively manage and optimize your AI prompts.</p>"},{"location":"user-guide/core-concepts/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":"<p>Prompt Versioner provides a comprehensive framework for prompt lifecycle management:</p> <pre><code>graph TB\n    subgraph \"Core Components\"\n        A[Prompt Storage] --&gt; B[Version Manager]\n        B --&gt; C[Metrics Tracker]\n        C --&gt; D[Performance Monitor]\n        B --&gt; E[A/B Testing]\n        E --&gt; C\n    end\n\n    subgraph \"Interfaces\"\n        F[CLI Interface]\n        G[Web Dashboard]\n        H[Python API]\n    end\n\n    subgraph \"Integrations\"\n        I[Git Integration]\n        J[Database Storage]\n    end\n\n    A --&gt; F\n    A --&gt; G\n    A --&gt; H\n    B --&gt; I\n    A --&gt; J\n    C --&gt; G\n    D --&gt; G\n    E --&gt; G</code></pre>"},{"location":"user-guide/core-concepts/#component-overview","title":"Component Overview","text":"<ul> <li>Prompt Storage: Centralized repository for all prompt versions</li> <li>Version Manager: Semantic versioning and change tracking</li> <li>Metrics Tracker: Performance monitoring and analytics</li> <li>A/B Testing: Statistical comparison framework</li> <li>Git Integration: Version control synchronization</li> <li>Multi-Interface Access: CLI, Web dashboard, and Python API</li> </ul>"},{"location":"user-guide/core-concepts/#prompt-structure","title":"\ud83d\udcdd Prompt Structure","text":""},{"location":"user-guide/core-concepts/#core-components","title":"Core Components","text":"<p>Every prompt in Prompt Versioner contains:</p> <ul> <li>System Prompt: AI role and behavior instructions</li> <li>User Prompt: Input template with variable placeholders</li> <li>Metadata: Versioning info, timestamps, custom attributes</li> <li>Performance Data: Usage metrics and quality scores</li> </ul> <pre><code># Example prompt structure\n{\n    \"id\": \"uuid-string\",\n    \"name\": \"assistant_prompt\",\n    \"version\": \"1.2.0\",\n    \"system_prompt\": \"You are a helpful assistant specializing in {domain}.\",\n    \"user_prompt\": \"Please help with: {user_input}\",\n    \"metadata\": {\"created_for\": \"customer support\", \"author\": \"team_lead\"},\n    \"timestamp\": \"2025-01-15T10:30:00Z\"\n}\n</code></pre>"},{"location":"user-guide/core-concepts/#variable-templates","title":"Variable Templates","text":"<p>Use variables for dynamic, reusable prompts:</p> <pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\npv = PromptVersioner(project_name=\"customer-service\", enable_git=False)\n\npv.save_version(\n    name=\"support_assistant\",\n    system_prompt=\"You are a {role} specializing in {domain}.\",\n    user_prompt=\"Issue: {issue}\\nProvide {response_type} assistance.\",\n    bump_type=VersionBump.MAJOR,\n    metadata={\"variables\": [\"role\", \"domain\", \"issue\", \"response_type\"]}\n)\n</code></pre>"},{"location":"user-guide/core-concepts/#version-management","title":"\ud83d\udd04 Version Management","text":""},{"location":"user-guide/core-concepts/#semantic-versioning-system","title":"Semantic Versioning System","text":"<p>Prompt Versioner uses semantic versioning (MAJOR.MINOR.PATCH):</p> Type Format Use Case Example MAJOR x.0.0 Breaking changes, complete redesigns New prompt structure MINOR x.y.0 Feature additions, improvements Added context variables PATCH x.y.z Bug fixes, minor tweaks Grammar corrections"},{"location":"user-guide/core-concepts/#version-operations","title":"Version Operations","text":"<pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\npv = PromptVersioner(project_name=\"my-project\", enable_git=False)\n\n# Create versions with different bump types\npv.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are a helpful assistant.\",\n    user_prompt=\"Question: {question}\",\n    bump_type=VersionBump.MAJOR\n)\n\n# Get version information\nlatest = pv.get_latest(\"assistant\")\nspecific = pv.get_version(\"assistant\", \"1.0.0\")\nall_versions = pv.list_versions(\"assistant\")\n\n# Compare versions\ndiff = pv.diff(\"assistant\", \"1.0.0\", \"2.0.0\", format_output=True)\n</code></pre>"},{"location":"user-guide/core-concepts/#metrics-and-performance","title":"\ud83d\udcca Metrics and Performance","text":""},{"location":"user-guide/core-concepts/#key-performance-indicators","title":"Key Performance Indicators","text":"<p>Track essential metrics for prompt optimization:</p> Metric Description Optimal Range Quality Score Human or AI assessment (0-1) &gt; 0.8 Latency Response time in milliseconds &lt; 3000ms Cost Token usage cost in euros Varies by model Success Rate Successful request percentage &gt; 95%"},{"location":"user-guide/core-concepts/#logging-metrics","title":"Logging Metrics","text":"<pre><code># Log performance data\npv.log_metrics(\n    name=\"assistant\",\n    version=\"1.1.0\",\n    model_name=\"gpt-4o-mini\",\n    input_tokens=150,\n    output_tokens=200,\n    latency_ms=1500,\n    cost_eur=0.004,\n    quality_score=0.85,\n    success=True,\n    metadata={\"domain\": \"customer_service\"}\n)\n\n# Analyze performance\nversion_data = pv.get_version(\"assistant\", \"1.1.0\")\nmetrics = pv.storage.get_metrics(version_id=version_data[\"id\"])\n\nif metrics:\n    avg_quality = sum(m.get(\"quality_score\", 0) for m in metrics) / len(metrics)\n    avg_latency = sum(m.get(\"latency_ms\", 0) for m in metrics) / len(metrics)\n    print(f\"Average quality: {avg_quality:.2f}\")\n    print(f\"Average latency: {avg_latency:.0f}ms\")\n</code></pre>"},{"location":"user-guide/core-concepts/#ab-testing-framework","title":"\ud83e\uddea A/B Testing Framework","text":""},{"location":"user-guide/core-concepts/#statistical-testing-structure","title":"Statistical Testing Structure","text":"<p>A/B testing framework with built-in statistical analysis:</p> <pre><code>graph LR\n    A[Control Group&lt;br/&gt;Version A] --&gt; C[Statistical Analysis]\n    B[Treatment Group&lt;br/&gt;Version B] --&gt; C\n    C --&gt; D[Results&lt;br/&gt;Winner + Confidence]\n\n    subgraph \"Metrics Collected\"\n        E[Quality Scores]\n        F[Performance Data]\n        G[Success Rates]\n    end\n\n    A --&gt; E\n    B --&gt; E\n    A --&gt; F\n    B --&gt; F\n    A --&gt; G\n    B --&gt; G</code></pre>"},{"location":"user-guide/core-concepts/#implementation","title":"Implementation","text":"<pre><code>from prompt_versioner import ABTest\n\n# Create A/B test\nab_test = ABTest(\n    versioner=pv,\n    prompt_name=\"customer_service\",\n    version_a=\"1.0.0\",  # Control\n    version_b=\"1.1.0\",  # Treatment\n    metric_name=\"quality_score\"\n)\n\n# Log test results for both groups\nfor i in range(30):\n    ab_test.log_result(\"a\", 0.75 + (i * 0.005))  # Control baseline\n    ab_test.log_result(\"b\", 0.80 + (i * 0.005))  # Treatment improvement\n\n# Get statistical results\nif ab_test.is_ready(min_samples=25):\n    result = ab_test.get_result()\n    print(f\"Winner: Version {result.winner}\")\n    print(f\"Improvement: {result.improvement:.1%}\")\n    print(f\"Confidence: {result.confidence:.1%}\")\n</code></pre>"},{"location":"user-guide/core-concepts/#key-features","title":"Key Features","text":"<ul> <li>Statistical Significance: Automatic p-value calculation</li> <li>Sample Size Validation: Ensures reliable results</li> <li>Confidence Intervals: Quantifies uncertainty</li> <li>Clear Recommendations: Winner determination with confidence levels</li> </ul>"},{"location":"user-guide/core-concepts/#monitoring-and-alerts","title":"\u26a0\ufe0f Monitoring and Alerts","text":""},{"location":"user-guide/core-concepts/#performance-thresholds","title":"Performance Thresholds","text":"<p>Set up monitoring with configurable thresholds:</p> Metric Warning Critical Quality Score &lt; 0.8 &lt; 0.6 Latency &gt; 3000ms &gt; 5000ms Success Rate &lt; 98% &lt; 95% Cost per Request &gt; \u20ac0.01 &gt; \u20ac0.02"},{"location":"user-guide/core-concepts/#basic-monitoring","title":"Basic Monitoring","text":"<pre><code>def monitor_performance(pv, prompt_name, version, min_samples=20):\n    \"\"\"Monitor prompt performance with threshold alerts\"\"\"\n\n    version_data = pv.get_version(prompt_name, version)\n    metrics = pv.storage.get_metrics(version_id=version_data[\"id\"])\n\n    if len(metrics) &lt; min_samples:\n        print(f\"\u23f3 Need {min_samples - len(metrics)} more samples\")\n        return\n\n    # Calculate averages from recent metrics\n    recent = metrics[-min_samples:]\n    avg_quality = sum(m.get(\"quality_score\", 0) for m in recent) / len(recent)\n    avg_latency = sum(m.get(\"latency_ms\", 0) for m in recent) / len(recent)\n    success_rate = sum(1 for m in recent if m.get(\"success\", True)) / len(recent)\n\n    # Check thresholds and alert\n    alerts = []\n    if avg_quality &lt; 0.7: alerts.append(f\"Low quality: {avg_quality:.2f}\")\n    if avg_latency &gt; 3000: alerts.append(f\"High latency: {avg_latency:.0f}ms\")\n    if success_rate &lt; 0.95: alerts.append(f\"Low success: {success_rate:.1%}\")\n\n    if alerts:\n        print(f\"\ud83d\udea8 {prompt_name} v{version} alerts: {', '.join(alerts)}\")\n    else:\n        print(f\"\u2705 {prompt_name} v{version} healthy\")\n\n# Usage\nmonitor_performance(pv, \"customer_service\", \"1.1.0\")\n</code></pre>"},{"location":"user-guide/core-concepts/#collaboration-and-organization","title":"\ud83e\udd1d Collaboration and Organization","text":""},{"location":"user-guide/core-concepts/#team-annotations","title":"Team Annotations","text":"<p>Enable team collaboration with structured annotations:</p> <pre><code># Add team annotations for reviews and approvals\npv.add_annotation(\n    name=\"customer_service\",\n    version=\"1.1.0\",\n    text=\"Approved for production deployment after A/B testing\",\n    author=\"team_lead\"\n)\n\n# Read all team annotations\nannotations = pv.get_annotations(\"customer_service\", \"1.1.0\")\nfor note in annotations:\n    print(f\"{note['author']}: {note['text']}\")\n</code></pre>"},{"location":"user-guide/core-concepts/#prompt-organization","title":"Prompt Organization","text":"<p>Structure prompts using naming conventions and metadata:</p> <pre><code># Organized naming convention: domain_subdomain_type\nprompt_types = [\n    \"customer_support_general\",\n    \"customer_support_technical\",\n    \"marketing_email_campaigns\",\n    \"marketing_social_media\",\n    \"internal_qa_automation\"\n]\n\n# Find prompts by domain\ndef find_prompts_by_domain(pv, domain):\n    all_prompts = pv.list_prompts()\n    return [name for name in all_prompts if name.startswith(f\"{domain}_\")]\n\ncustomer_prompts = find_prompts_by_domain(pv, \"customer\")\nmarketing_prompts = find_prompts_by_domain(pv, \"marketing\")\n</code></pre>"},{"location":"user-guide/core-concepts/#version-history-tracking","title":"Version History Tracking","text":"<pre><code># Get comprehensive version history\nversions = pv.list_versions(\"customer_service\")\nfor v in versions:\n    print(f\"v{v['version']} - {v['timestamp']}\")\n\n# Compare version changes\ndiff = pv.diff(\"customer_service\", \"1.0.0\", \"1.1.0\", format_output=True)\nprint(f\"Changes: {diff.summary}\")\n</code></pre>"},{"location":"user-guide/core-concepts/#git-integration","title":"Git Integration","text":"<p>Synchronize with version control:</p> <pre><code># Enable Git tracking\npv_git = PromptVersioner(\n    project_name=\"git-tracked\",\n    enable_git=True,\n    git_repo=\"/path/to/repo\"\n)\n\n# Install hooks for automatic tracking\npv_git.install_git_hooks()\n\n# Prompt changes are now tracked in Git\npv_git.save_version(\n    name=\"tracked_prompt\",\n    system_prompt=\"You are a helpful assistant.\",\n    user_prompt=\"Help with: {request}\",\n    bump_type=VersionBump.MAJOR\n)\n</code></pre>"},{"location":"user-guide/core-concepts/#backup-and-export","title":"Backup and Export","text":"<pre><code>from pathlib import Path\n\n# Export prompts for backup\npv.export_prompt(\n    name=\"customer_service\",\n    output_file=Path(\"backups/customer_service.json\"),\n    format=\"json\",\n    include_metrics=True\n)\n\n# Import from backup\nresult = pv.import_prompt(\n    input_file=Path(\"backups/customer_service.json\"),\n    overwrite=False\n)\n</code></pre>"},{"location":"user-guide/core-concepts/#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>Now that you understand the core concepts, explore these advanced topics:</p> <ul> <li>Version Management: Deep dive into semantic versioning strategies</li> <li>A/B Testing Guide: Advanced statistical testing techniques</li> <li>Performance Monitoring: Monitoring prompts</li> <li>Team Workflows: Set up collaborative prompt development</li> <li>API Reference: Complete method documentation</li> </ul>"},{"location":"user-guide/metrics-tracking/","title":"Metrics Tracking","text":"<p>Track and analyze prompt performance with Prompt Versioner's metrics system.</p>"},{"location":"user-guide/metrics-tracking/#quick-start","title":"\ud83d\udcca Quick Start","text":"<pre><code>from prompt_versioner import PromptVersioner\n\n# Initialize versioner\npv = PromptVersioner(project_name=\"my-project\", enable_git=False)\n\n# Log metrics after using a prompt\npv.log_metrics(\n    name=\"code_reviewer\",\n    version=\"1.0.0\",\n    model_name=\"gpt-4o\",\n    input_tokens=150,\n    output_tokens=250,\n    latency_ms=420.5,\n    quality_score=0.92,\n    cost_eur=0.003,\n    temperature=0.7,\n    max_tokens=1000,\n    success=True,\n    metadata={\"user_feedback\": \"excellent\", \"domain\": \"backend\"}\n)\n\nprint(\"\ud83d\udcca Metrics logged successfully!\")\n</code></pre>"},{"location":"user-guide/metrics-tracking/#core-metrics","title":"\ud83c\udfaf Core Metrics","text":""},{"location":"user-guide/metrics-tracking/#basic-performance-metrics","title":"Basic Performance Metrics","text":"<pre><code># Essential metrics for every prompt call\npv.log_metrics(\n    name=\"summarizer\",\n    version=\"1.1.0\",\n\n    # Model info\n    model_name=\"gpt-4o-mini\",\n\n    # Token usage\n    input_tokens=200,\n    output_tokens=100,\n\n    # Performance\n    latency_ms=350.0,\n    success=True,\n\n    # Optional quality assessment\n    quality_score=0.88,  # Your evaluation (0.0-1.0)\n)\n</code></pre>"},{"location":"user-guide/metrics-tracking/#advanced-metrics","title":"Advanced Metrics","text":"<pre><code># Comprehensive metrics tracking\npv.log_metrics(\n    name=\"customer_service\",\n    version=\"2.1.0\",\n    model_name=\"gpt-4o\",\n\n    # Usage\n    input_tokens=180,\n    output_tokens=220,\n\n    # Performance\n    latency_ms=580.2,\n    cost_eur=0.0045,\n\n    # Quality metrics\n    quality_score=0.94,\n    accuracy=0.91,\n\n    # Model parameters used\n    temperature=0.3,\n    top_p=0.9,\n    max_tokens=500,\n\n    # Result\n    success=True,\n    error_message=None,\n\n    # Context\n    metadata={\n        \"user_id\": \"user123\",\n        \"session_id\": \"sess456\",\n        \"issue_type\": \"billing\",\n        \"satisfaction_score\": 4.5,\n        \"resolved\": True\n    }\n)\n</code></pre>"},{"location":"user-guide/metrics-tracking/#analyzing-performance","title":"\ud83d\udcc8 Analyzing Performance","text":""},{"location":"user-guide/metrics-tracking/#get-metrics-for-analysis","title":"Get Metrics for Analysis","text":"<pre><code># Get version details\nversion = pv.get_version(\"customer_service\", \"2.1.0\")\nversion_id = version[\"id\"]\n\n# Retrieve all metrics for this version\nmetrics = pv.storage.get_metrics(version_id=version_id, limit=100)\n\nprint(f\"Found {len(metrics)} metric records\")\n\n# Calculate averages\nif metrics:\n    avg_quality = sum(m[\"quality_score\"] for m in metrics if m[\"quality_score\"]) / len(metrics)\n    avg_latency = sum(m[\"latency_ms\"] for m in metrics if m[\"latency_ms\"]) / len(metrics)\n    success_rate = sum(1 for m in metrics if m[\"success\"]) / len(metrics)\n\n    print(f\"\ud83d\udcca Performance Summary:\")\n    print(f\"  Average Quality: {avg_quality:.2f}\")\n    print(f\"  Average Latency: {avg_latency:.1f}ms\")\n    print(f\"  Success Rate: {success_rate:.1%}\")\n</code></pre>"},{"location":"user-guide/metrics-tracking/#compare-version-performance","title":"Compare Version Performance","text":"<pre><code># Compare multiple versions\ndef compare_version_metrics(pv, prompt_name, versions):\n    \"\"\"Compare performance across versions\"\"\"\n\n    results = {}\n\n    for version_str in versions:\n        version = pv.get_version(prompt_name, version_str)\n        if not version:\n            continue\n\n        metrics = pv.storage.get_metrics(version_id=version[\"id\"], limit=1000)\n\n        if metrics:\n            results[version_str] = {\n                \"samples\": len(metrics),\n                \"avg_quality\": sum(m.get(\"quality_score\", 0) for m in metrics) / len(metrics),\n                \"avg_latency\": sum(m.get(\"latency_ms\", 0) for m in metrics) / len(metrics),\n                \"success_rate\": sum(1 for m in metrics if m.get(\"success\", False)) / len(metrics),\n                \"avg_cost\": sum(m.get(\"cost_eur\", 0) for m in metrics) / len(metrics)\n            }\n\n    return results\n\n# Usage\ncomparison = compare_version_metrics(pv, \"customer_service\", [\"2.0.0\", \"2.1.0\", \"2.2.0\"])\n\nfor version, stats in comparison.items():\n    print(f\"\\nVersion {version}:\")\n    print(f\"  Samples: {stats['samples']}\")\n    print(f\"  Quality: {stats['avg_quality']:.2f}\")\n    print(f\"  Latency: {stats['avg_latency']:.1f}ms\")\n    print(f\"  Success: {stats['success_rate']:.1%}\")\n    print(f\"  Cost: \u20ac{stats['avg_cost']:.4f}\")\n</code></pre>"},{"location":"user-guide/metrics-tracking/#testing-with-metrics","title":"\ud83e\uddea Testing with Metrics","text":""},{"location":"user-guide/metrics-tracking/#test-context-for-controlled-testing","title":"Test Context for Controlled Testing","text":"<pre><code># Use test context for structured testing\nwith pv.test_version(\"summarizer\", \"1.2.0\") as test:\n    # Your LLM call would go here\n    # result = call_llm(prompt, text)\n\n    # Log test metrics\n    test.log(\n        tokens=180,\n        cost=0.002,\n        latency_ms=290,\n        quality_score=0.89,\n        metadata={\"test_case\": \"technical_doc\", \"length\": \"medium\"}\n    )\n\nprint(\"\u2705 Test metrics logged\")\n</code></pre>"},{"location":"user-guide/metrics-tracking/#batch-metrics-collection","title":"Batch Metrics Collection","text":"<pre><code># Collect metrics for multiple test cases\ntest_cases = [\n    {\"text\": \"Simple text\", \"expected_quality\": 0.8},\n    {\"text\": \"Complex technical document\", \"expected_quality\": 0.85},\n    {\"text\": \"Creative content\", \"expected_quality\": 0.9}\n]\n\nfor i, case in enumerate(test_cases):\n    with pv.test_version(\"summarizer\", \"1.2.0\") as test:\n        # Simulate processing\n        simulated_quality = case[\"expected_quality\"] + random.uniform(-0.05, 0.05)\n        simulated_latency = 300 + random.uniform(-50, 100)\n\n        test.log(\n            tokens=150 + i * 20,\n            cost=0.002 + i * 0.0005,\n            latency_ms=simulated_latency,\n            quality_score=simulated_quality,\n            metadata={\"test_case\": f\"case_{i+1}\", \"type\": case[\"text\"][:10]}\n        )\n\nprint(f\"\u2705 Logged metrics for {len(test_cases)} test cases\")\n</code></pre>"},{"location":"user-guide/metrics-tracking/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>A/B Testing - Compare versions systematically</li> <li>Version Management - Manage your prompt versions</li> <li>Basic Usage - More examples and patterns</li> </ul>"},{"location":"user-guide/performance-monitoring/","title":"Performance Monitoring","text":"<p>Monitor prompt performance and system health with Prompt Versioner's monitoring capabilities.</p>"},{"location":"user-guide/performance-monitoring/#quick-overview","title":"\ud83d\udd0d Quick Overview","text":"<pre><code>from prompt_versioner import PromptVersioner\n\npv = PromptVersioner(project_name=\"my-project\", enable_git=False)\n\n# Check recent performance\ndef check_prompt_health(prompt_name, version):\n    version_data = pv.get_version(prompt_name, version)\n    metrics = pv.storage.get_metrics(version_id=version_data[\"id\"], limit=50)\n\n    if metrics:\n        avg_quality = sum(m.get(\"quality_score\", 0) for m in metrics) / len(metrics)\n        avg_latency = sum(m.get(\"latency_ms\", 0) for m in metrics) / len(metrics)\n        success_rate = sum(1 for m in metrics if m.get(\"success\", True)) / len(metrics)\n\n        print(f\"\ud83d\udcca {prompt_name} v{version} Health:\")\n        print(f\"  Quality: {avg_quality:.2f}\")\n        print(f\"  Latency: {avg_latency:.1f}ms\")\n        print(f\"  Success: {success_rate:.1%}\")\n        print(f\"  Samples: {len(metrics)}\")\n\ncheck_prompt_health(\"code_reviewer\", \"1.1.0\")\n</code></pre>"},{"location":"user-guide/performance-monitoring/#performance-metrics","title":"\ud83d\udcc8 Performance Metrics","text":""},{"location":"user-guide/performance-monitoring/#key-performance-indicators","title":"Key Performance Indicators","text":"<p>Monitor these essential metrics:</p> <ul> <li>Quality Score: Your custom evaluation (0.0-1.0)</li> <li>Latency: Response time in milliseconds</li> <li>Success Rate: Percentage of successful calls</li> <li>Cost: Per-call cost tracking</li> <li>Token Usage: Input/output token consumption</li> </ul>"},{"location":"user-guide/performance-monitoring/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"user-guide/performance-monitoring/#1-set-appropriate-thresholds","title":"1. Set Appropriate Thresholds","text":"<ul> <li>Define quality thresholds based on your use case</li> <li>Set latency limits based on user experience requirements</li> <li>Monitor success rates to catch errors early</li> </ul>"},{"location":"user-guide/performance-monitoring/#2-regular-health-checks","title":"2. Regular Health Checks","text":"<ul> <li>Check performance at least daily for production prompts</li> <li>Set up automated alerts for critical issues</li> <li>Review trends weekly to identify patterns</li> </ul>"},{"location":"user-guide/performance-monitoring/#3-baseline-comparisons","title":"3. Baseline Comparisons","text":"<ul> <li>Compare current performance to historical baselines</li> <li>Look for gradual degradation over time</li> <li>Track performance after prompt updates</li> </ul>"},{"location":"user-guide/performance-monitoring/#4-cost-monitoring","title":"4. Cost Monitoring","text":"<ul> <li>Monitor cost per call and daily/monthly totals</li> <li>Set budget alerts to prevent unexpected costs</li> <li>Track cost efficiency alongside quality metrics</li> </ul>"},{"location":"user-guide/performance-monitoring/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>Metrics Tracking - Learn about detailed metrics collection</li> <li>A/B Testing - Compare performance between versions</li> <li>Version Management - Manage your prompt versions</li> </ul>"},{"location":"user-guide/version-management/","title":"Version Management","text":"<p>Manage prompt versions effectively with Prompt Versioner's semantic versioning system.</p>"},{"location":"user-guide/version-management/#quick-start","title":"\ud83d\udd04 Quick Start","text":"<pre><code>from prompt_versioner import PromptVersioner, VersionBump\n\n# Initialize versioner\npv = PromptVersioner(project_name=\"my-project\", enable_git=False)\n\n# Save your first version\npv.save_version(\n    name=\"code_reviewer\",\n    system_prompt=\"You are an expert code reviewer.\",\n    user_prompt=\"Review this code:\\n{code}\",\n    bump_type=VersionBump.MAJOR,  # Creates 1.0.0\n)\n\n# Create an improved version\npv.save_version(\n    name=\"code_reviewer\",\n    system_prompt=\"You are an expert code reviewer with deep knowledge of software engineering.\",\n    user_prompt=\"Review this code thoroughly:\\n{code}\\n\\nProvide detailed feedback.\",\n    bump_type=VersionBump.MINOR,  # Creates 1.1.0\n)\n\n# Get latest version\nlatest = pv.get_latest(\"code_reviewer\")\nprint(f\"Latest: {latest['version']}\")\n\n# List all versions\nversions = pv.list_versions(\"code_reviewer\")\nfor v in versions:\n    print(f\"Version {v['version']}: {v['timestamp']}\")\n</code></pre>"},{"location":"user-guide/version-management/#semantic-versioning","title":"\ud83d\udcdd Semantic Versioning","text":"<p>Prompt Versioner uses semantic versioning (SemVer):</p> <pre><code>MAJOR.MINOR.PATCH\n  \u2502     \u2502     \u2502\n  \u2502     \u2502     \u2514\u2500\u2500 Small fixes, typos\n  \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New features, improvements\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Breaking changes\n</code></pre>"},{"location":"user-guide/version-management/#version-types","title":"Version Types","text":"<p>PATCH (1.0.1) - Small corrections: <pre><code># Fix typo in prompt\npv.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are a helpful assistant.\",  # Fixed \"helpfull\" -&gt; \"helpful\"\n    user_prompt=\"How can I help you?\",\n    bump_type=VersionBump.PATCH,\n)\n</code></pre></p> <p>MINOR (1.1.0) - New features: <pre><code># Add more detailed instructions\npv.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are a helpful assistant. Always be polite and detailed.\",\n    user_prompt=\"How can I help you today? Please be specific.\",\n    bump_type=VersionBump.MINOR,\n)\n</code></pre></p> <p>MAJOR (2.0.0) - Breaking changes: <pre><code># Complete redesign\npv.save_version(\n    name=\"assistant\",\n    system_prompt=\"You are an AI assistant specialized in technical support.\",\n    user_prompt=\"What technical issue can I help you resolve?\",\n    bump_type=VersionBump.MAJOR,\n)\n</code></pre></p>"},{"location":"user-guide/version-management/#working-with-versions","title":"\ud83d\udd0d Working with Versions","text":""},{"location":"user-guide/version-management/#getting-versions","title":"Getting Versions","text":"<pre><code># Get specific version\nversion = pv.get_version(\"code_reviewer\", \"1.0.0\")\nprint(f\"System: {version['system_prompt']}\")\nprint(f\"User: {version['user_prompt']}\")\n\n# Get latest version\nlatest = pv.get_latest(\"code_reviewer\")\nprint(f\"Latest: {latest['version']}\")\n\n# List all versions for a prompt\nversions = pv.list_versions(\"code_reviewer\")\nfor v in versions:\n    print(f\"v{v['version']}: {v['timestamp']}\")\n\n# List all prompts\nprompts = pv.list_prompts()\nprint(f\"All prompts: {prompts}\")\n</code></pre>"},{"location":"user-guide/version-management/#comparing-versions","title":"Comparing Versions","text":"<pre><code># Compare two versions\ndiff = pv.diff(\"code_reviewer\", \"1.0.0\", \"1.1.0\", format_output=True)\nprint(f\"Changes: {diff.summary}\")\n\n# Compare multiple versions\ncomparison = pv.compare_versions(\"code_reviewer\", [\"1.0.0\", \"1.1.0\", \"1.2.0\"])\nprint(f\"Comparison data: {comparison}\")\n</code></pre>"},{"location":"user-guide/version-management/#rolling-back","title":"Rolling Back","text":"<pre><code># Rollback to previous version (creates new version)\nnew_version_id = pv.rollback(\"code_reviewer\", to_version=\"1.0.0\")\nprint(f\"Rolled back to create new version: {new_version_id}\")\n\n# Check what version we're now at\nlatest = pv.get_latest(\"code_reviewer\")\nprint(f\"Current version: {latest['version']}\")\n</code></pre>"},{"location":"user-guide/version-management/#tracking-metrics","title":"\ud83d\udcca Tracking Metrics","text":"<p>Track performance for each version:</p> <pre><code># Log metrics after using a prompt\npv.log_metrics(\n    name=\"code_reviewer\",\n    version=\"1.1.0\",\n    model_name=\"gpt-4o\",\n    input_tokens=150,\n    output_tokens=250,\n    latency_ms=420,\n    quality_score=0.92,\n    success=True,\n)\n\n# Get metrics for a version\nversion = pv.get_version(\"code_reviewer\", \"1.1.0\")\nmetrics = pv.storage.get_metrics(version_id=version[\"id\"])\nprint(f\"Metrics: {len(metrics)} recorded\")\n</code></pre>"},{"location":"user-guide/version-management/#export","title":"\ud83d\udcc1 Export","text":""},{"location":"user-guide/version-management/#export-prompts","title":"Export Prompts","text":"<pre><code>from pathlib import Path\n\n# Export single prompt\npv.export_prompt(\n    name=\"code_reviewer\",\n    output_file=Path(\"code_reviewer.json\"),\n    format=\"json\",\n    include_metrics=True,\n)\n\n# Export all prompts\npv.export_all(\n    output_dir=Path(\"exports\"),\n    format=\"json\"\n)\n</code></pre>"},{"location":"user-guide/version-management/#annotations","title":"\ud83c\udff7\ufe0f Annotations","text":"<p>Add notes to specific versions:</p> <pre><code># Add annotation\npv.add_annotation(\n    name=\"code_reviewer\",\n    version=\"1.1.0\",\n    text=\"Improved handling of complex code patterns\",\n    author=\"team-lead\"\n)\n\n# Get annotations\nannotations = pv.get_annotations(\"code_reviewer\", \"1.1.0\")\nfor note in annotations:\n    print(f\"{note['author']}: {note['text']}\")\n</code></pre>"},{"location":"user-guide/version-management/#cleanup","title":"\ud83d\uddd1\ufe0f Cleanup","text":"<pre><code># Delete specific version\nsuccess = pv.delete_version(\"code_reviewer\", \"1.0.0\")\nprint(f\"Deleted: {success}\")\n\n# Delete entire prompt (all versions)\nsuccess = pv.delete_prompt(\"old_prompt\")\nprint(f\"Deleted prompt: {success}\")\n</code></pre>"},{"location":"user-guide/version-management/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>Metrics Tracking - Track prompt performance</li> <li>A/B Testing - Compare versions scientifically</li> <li>Basic Usage - More examples</li> </ul>"},{"location":"user-guide/web-dashboard/","title":"Web Dashboard","text":"<p>Access Prompt Versioner's web interface for visual prompt management and monitoring.</p>"},{"location":"user-guide/web-dashboard/#quick-start","title":"\ud83c\udf10 Quick Start","text":"<pre><code>from prompt_versioner import PromptVersioner\n\n# Initialize with web dashboard support\npv = PromptVersioner(project_name=\"my-project\", enable_git=False)\n\n# Start the web dashboard\npv.start_dashboard(host=\"127.0.0.1\", port=8080)\n</code></pre> <p>Then open your browser to: <code>http://localhost:8080</code></p>"},{"location":"user-guide/web-dashboard/#dashboard-features","title":"\ud83c\udf9b\ufe0f Dashboard Features","text":""},{"location":"user-guide/web-dashboard/#main-dashboard","title":"Main Dashboard","text":"<ul> <li>Overview: See all your prompts and their latest versions</li> <li>Metrics: Performance statistics</li> <li>Version History: Browse version changes over time</li> <li>Comparisons: Side-by-side version comparisons</li> </ul>"},{"location":"user-guide/web-dashboard/#prompt-management","title":"Prompt Management","text":"<ul> <li>View Prompts: Browse all system and user prompts</li> <li>Export: Download prompt files</li> <li>Annotations: Add notes and documentation</li> </ul>"},{"location":"user-guide/web-dashboard/#performance-monitoring","title":"Performance Monitoring","text":"<ul> <li>Real-time Metrics: Live performance data</li> <li>Quality Trends: Track quality scores over time</li> <li>Cost Analysis: Monitor usage costs</li> </ul>"},{"location":"user-guide/web-dashboard/#starting-the-dashboard","title":"\ud83d\ude80 Starting the Dashboard","text":""},{"location":"user-guide/web-dashboard/#basic-setup","title":"Basic Setup","text":"<pre><code>from prompt_versioner import PromptVersioner\n\n# Initialize versioner\npv = PromptVersioner(project_name=\"my-project\")\n\n# Start dashboard on default port (5000)\npv.start_dashboard()\nprint(\"Dashboard running at http://localhost:5000\")\n</code></pre>"},{"location":"user-guide/web-dashboard/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Custom host and port\npv.start_dashboard(\n    host=\"0.0.0.0\",  # Allow external access\n    port=8080,\n    debug=False,      # Set True for development\n    auto_open=True    # Automatically open browser\n)\n</code></pre>"},{"location":"user-guide/web-dashboard/#dashboard-views","title":"\ud83d\udcca Dashboard Views","text":""},{"location":"user-guide/web-dashboard/#prompts-overview","title":"Prompts Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83d\udcdd PROMPTS OVERVIEW                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_service  v2.1.0  \u2197\ufe0f 0.92 quality      \u2502\n\u2502 code_reviewer     v1.1.0  \u2197\ufe0f 0.89 quality      \u2502\n\u2502 summarizer        v1.2.0  \u2198\ufe0f 0.85 quality      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/web-dashboard/#version-details","title":"Version Details","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83d\udd0d customer_service v2.1.0                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Created: 2024-01-15 14:30                      \u2502\n\u2502 Calls: 1,247                                   \u2502\n\u2502 Avg Quality: 0.92                              \u2502\n\u2502 Avg Latency: 425ms                             \u2502\n\u2502 Success Rate: 98.4%                            \u2502\n\u2502                                                 \u2502\n\u2502 [View Code] [Edit] [Export] [Compare]          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/web-dashboard/#api-integration","title":"\ud83d\udd17 API Integration","text":""},{"location":"user-guide/web-dashboard/#rest-api-endpoints","title":"REST API Endpoints","text":"<p>The dashboard exposes REST API endpoints:</p> <pre><code># Get all prompts\nGET /api/prompts\n\n# Get specific prompt versions\nGET /api/prompts/{name}/versions\n\n# Get metrics\nGET /api/prompts/{name}/versions/{version}/metrics\n\n# Create new version\nPOST /api/prompts/{name}/versions\n\n# Update version\nPUT /api/prompts/{name}/versions/{version}\n</code></pre>"},{"location":"user-guide/web-dashboard/#getting-started-checklist","title":"\ud83d\ude80 Getting Started Checklist","text":"<ol> <li>\u2705 Start Dashboard: <code>pv.start_dashboard()</code></li> <li>\u2705 Browse Prompts: View your existing prompts</li> <li>\u2705 Check Metrics: Review performance data</li> <li>\u2705 Create Version: Make changes through web UI</li> <li>\u2705 Set Monitoring: Configure alerts and thresholds</li> <li>\u2705 Share Access: Invite team members if needed</li> </ol>"},{"location":"user-guide/web-dashboard/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>Version Management - Learn about version control</li> <li>Metrics Tracking - Understand performance metrics</li> <li>A/B Testing - Compare versions systematically</li> </ul>"}]}